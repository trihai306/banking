{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Train Model v√† Upload l√™n Hugging Face\n",
        "\n",
        "Notebook n√†y gi√∫p b·∫°n:\n",
        "1. ‚úÖ Ki·ªÉm tra dataset\n",
        "2. üîÑ Generate assistant responses (n·∫øu c·∫ßn)\n",
        "3. üéØ Train model v·ªõi LoRA\n",
        "4. üì§ Upload model l√™n Hugging Face\n",
        "\n",
        "**L∆∞u √Ω:** Dataset c·∫ßn c√≥ format Qwen3 v·ªõi `messages` ch·ª©a `system`, `user`, v√† `assistant` roles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. C√†i ƒë·∫∑t Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "%pip install -q transformers>=4.41.0,<5.0.0\n",
        "%pip install -q accelerate>=0.20.0\n",
        "%pip install -q bitsandbytes>=0.41.0\n",
        "%pip install -q peft>=0.6.0\n",
        "%pip install -q datasets>=2.14.0\n",
        "%pip install -q torch>=2.0.0\n",
        "%pip install -q tensorboard>=2.13.0\n",
        "%pip install -q huggingface_hub>=0.20.0\n",
        "%pip install -q tqdm\n",
        "\n",
        "print(\"‚úÖ ƒê√£ c√†i ƒë·∫∑t t·∫•t c·∫£ dependencies!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from transformers import (\n",
        "    Qwen3VLForConditionalGeneration,\n",
        "    AutoProcessor,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import Dataset, load_dataset\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import login, HfApi, create_repo\n",
        "\n",
        "print(\"‚úÖ ƒê√£ import t·∫•t c·∫£ libraries!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. C·∫•u H√¨nh\n",
        "\n",
        "Thay ƒë·ªïi c√°c tham s·ªë d∆∞·ªõi ƒë√¢y theo nhu c·∫ßu c·ªßa b·∫°n:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C·∫§U H√åNH MODEL\n",
        "# ============================================================================\n",
        "MODEL_NAME = \"hainguyen306201/bank-model-2b\"  # Model base ƒë·ªÉ fine-tune\n",
        "\n",
        "# ============================================================================\n",
        "# C·∫§U H√åNH DATASET\n",
        "# ============================================================================\n",
        "# C√°ch 1: Upload file l√™n Colab (ch·∫°y cell upload b√™n d∆∞·ªõi)\n",
        "DATASET_PATH = \"/content/dataset.jsonl\"\n",
        "\n",
        "# C√°ch 2: Load t·ª´ GitHub ho·∫∑c URL\n",
        "# DATASET_URL = \"https://raw.githubusercontent.com/username/repo/main/dataset.jsonl\"\n",
        "\n",
        "# ============================================================================\n",
        "# C·∫§U H√åNH TRAINING\n",
        "# ============================================================================\n",
        "OUTPUT_DIR = \"./checkpoints\"\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 4\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "MAX_LENGTH = 2048\n",
        "WARMUP_STEPS = 100\n",
        "SAVE_STEPS = 500\n",
        "LOGGING_STEPS = 50\n",
        "\n",
        "# ============================================================================\n",
        "# C·∫§U H√åNH LoRA\n",
        "# ============================================================================\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# ============================================================================\n",
        "# C·∫§U H√åNH HUGGING FACE\n",
        "# ============================================================================\n",
        "# ƒê·∫∑t token c·ªßa b·∫°n (ho·∫∑c login b·∫±ng huggingface-cli)\n",
        "HF_TOKEN = \"\"  # ƒê·ªÉ tr·ªëng n·∫øu ƒë√£ login\n",
        "HUB_MODEL_ID = \"\"  # V√≠ d·ª•: \"username/bank-model-finetuned\"\n",
        "HUB_PRIVATE = False  # True n·∫øu mu·ªën private repo\n",
        "PUSH_TO_HUB = False  # True ƒë·ªÉ upload sau khi train xong\n",
        "\n",
        "# ============================================================================\n",
        "# C·∫§U H√åNH KH√ÅC\n",
        "# ============================================================================\n",
        "USE_QUANTIZATION = True  # D√πng 4-bit quantization ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ\n",
        "USE_FLASH_ATTENTION = False  # C·∫ßn c√†i flash-attn ri√™ng\n",
        "\n",
        "print(\"‚úÖ ƒê√£ c·∫•u h√¨nh xong!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Upload Dataset (N·∫øu c·∫ßn)\n",
        "\n",
        "N·∫øu dataset c·ªßa b·∫°n ·ªü local, upload l√™n Colab:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload file dataset\n",
        "uploaded = files.upload()\n",
        "\n",
        "# T√¨m file v·ª´a upload\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"‚úÖ ƒê√£ upload: {filename}\")\n",
        "    DATASET_PATH = f\"/content/{filename}\"\n",
        "    break\n",
        "\n",
        "print(f\"\\nüìÇ Dataset path: {DATASET_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Ki·ªÉm Tra Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_dataset(file_path: str):\n",
        "    \"\"\"Ki·ªÉm tra dataset v√† hi·ªÉn th·ªã th·ªëng k√™\"\"\"\n",
        "    print(f\"üìÇ ƒêang ki·ªÉm tra dataset: {file_path}\\n\")\n",
        "    \n",
        "    if not Path(file_path).exists():\n",
        "        print(f\"‚ùå File kh√¥ng t·ªìn t·∫°i: {file_path}\")\n",
        "        return False\n",
        "    \n",
        "    # ƒê·ªçc dataset\n",
        "    samples = []\n",
        "    issues = []\n",
        "    \n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            \n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                samples.append((line_num, data))\n",
        "            except json.JSONDecodeError as e:\n",
        "                issues.append(f\"D√≤ng {line_num}: JSON kh√¥ng h·ª£p l·ªá - {e}\")\n",
        "    \n",
        "    if not samples:\n",
        "        print(\"‚ùå Dataset tr·ªëng!\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"‚úÖ ƒê√£ ƒë·ªçc {len(samples)} samples\\n\")\n",
        "    \n",
        "    # Ki·ªÉm tra format\n",
        "    role_counts = Counter()\n",
        "    has_system = 0\n",
        "    has_user = 0\n",
        "    has_assistant = 0\n",
        "    missing_assistant = []\n",
        "    \n",
        "    for line_num, data in samples:\n",
        "        if \"messages\" not in data:\n",
        "            issues.append(f\"D√≤ng {line_num}: Thi·∫øu field 'messages'\")\n",
        "            continue\n",
        "        \n",
        "        messages = data.get(\"messages\", [])\n",
        "        if not isinstance(messages, list) or len(messages) == 0:\n",
        "            issues.append(f\"D√≤ng {line_num}: Messages r·ªóng ho·∫∑c kh√¥ng ph·∫£i list\")\n",
        "            continue\n",
        "        \n",
        "        # ƒê·∫øm roles\n",
        "        roles = [msg.get(\"role\") for msg in messages if isinstance(msg, dict)]\n",
        "        role_counts.update(roles)\n",
        "        \n",
        "        # Ki·ªÉm tra c√≥ system/user/assistant kh√¥ng\n",
        "        if any(msg.get(\"role\") == \"system\" for msg in messages):\n",
        "            has_system += 1\n",
        "        if any(msg.get(\"role\") == \"user\" for msg in messages):\n",
        "            has_user += 1\n",
        "        if any(msg.get(\"role\") == \"assistant\" for msg in messages):\n",
        "            has_assistant += 1\n",
        "        else:\n",
        "            missing_assistant.append(line_num)\n",
        "    \n",
        "    # Hi·ªÉn th·ªã th·ªëng k√™\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üìä TH·ªêNG K√ä DATASET\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"T·ªïng s·ªë samples: {len(samples)}\")\n",
        "    print(f\"\\nPh√¢n b·ªë roles:\")\n",
        "    for role, count in role_counts.most_common():\n",
        "        print(f\"  - {role}: {count} messages\")\n",
        "    \n",
        "    print(f\"\\nSamples c√≥:\")\n",
        "    print(f\"  - System message: {has_system}/{len(samples)} ({has_system/len(samples)*100:.1f}%)\")\n",
        "    print(f\"  - User message: {has_user}/{len(samples)} ({has_user/len(samples)*100:.1f}%)\")\n",
        "    print(f\"  - Assistant message: {has_assistant}/{len(samples)} ({has_assistant/len(samples)*100:.1f}%)\")\n",
        "    \n",
        "    # C·∫£nh b√°o\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚ö†Ô∏è  C·∫¢NH B√ÅO\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    if missing_assistant:\n",
        "        print(f\"\\n‚ùå QUAN TR·ªåNG: {len(missing_assistant)} samples THI·∫æU assistant responses!\")\n",
        "        print(\"   Dataset c·∫ßn c√≥ c·∫£ user v√† assistant messages ƒë·ªÉ training hi·ªáu qu·∫£.\")\n",
        "        if len(missing_assistant) <= 10:\n",
        "            print(f\"   C√°c d√≤ng thi·∫øu: {missing_assistant[:10]}\")\n",
        "        else:\n",
        "            print(f\"   C√°c d√≤ng thi·∫øu: {missing_assistant[:10]} ... v√† {len(missing_assistant) - 10} d√≤ng kh√°c\")\n",
        "        print(\"\\n   üí° Gi·∫£i ph√°p: Ch·∫°y cell 'Generate Responses' b√™n d∆∞·ªõi\")\n",
        "    \n",
        "    if has_assistant == 0:\n",
        "        print(\"\\n‚ùå Dataset KH√îNG C√ì assistant responses n√†o!\")\n",
        "        print(\"   C·∫ßn generate responses tr∆∞·ªõc khi training.\")\n",
        "        return False\n",
        "    \n",
        "    if has_assistant < len(samples) * 0.9:\n",
        "        print(f\"\\n‚ö†Ô∏è  Ch·ªâ {has_assistant}/{len(samples)} samples c√≥ assistant responses\")\n",
        "        print(\"   N√™n c√≥ √≠t nh·∫•t 90% samples c√≥ assistant responses\")\n",
        "    \n",
        "    if issues:\n",
        "        print(f\"\\n‚ö†Ô∏è  C√≥ {len(issues)} v·∫•n ƒë·ªÅ trong dataset:\")\n",
        "        for issue in issues[:10]:\n",
        "            print(f\"   - {issue}\")\n",
        "        if len(issues) > 10:\n",
        "            print(f\"   ... v√† {len(issues) - 10} v·∫•n ƒë·ªÅ kh√°c\")\n",
        "    \n",
        "    # K·∫øt lu·∫≠n\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ K·∫æT LU·∫¨N\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    if has_assistant == len(samples) and has_user == len(samples):\n",
        "        print(\"‚úÖ Dataset h·ª£p l·ªá v√† s·∫µn s√†ng cho training!\")\n",
        "        return True\n",
        "    elif has_assistant > 0:\n",
        "        print(\"‚ö†Ô∏è  Dataset c√≥ th·ªÉ training nh∆∞ng kh√¥ng t·ªëi ∆∞u\")\n",
        "        print(\"   N√™n b·ªï sung assistant responses cho t·∫•t c·∫£ samples\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"‚ùå Dataset kh√¥ng th·ªÉ training (thi·∫øu assistant responses)\")\n",
        "        return False\n",
        "\n",
        "# Ki·ªÉm tra dataset\n",
        "dataset_valid = check_dataset(DATASET_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Generate Assistant Responses (N·∫øu c·∫ßn)\n",
        "\n",
        "**Ch·ªâ ch·∫°y cell n√†y n·∫øu dataset thi·∫øu assistant responses!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ch·ªâ ch·∫°y n·∫øu dataset thi·∫øu assistant responses\n",
        "GENERATE_RESPONSES = False  # ƒê·∫∑t True n·∫øu c·∫ßn generate\n",
        "\n",
        "if GENERATE_RESPONSES:\n",
        "    print(\"üì• ƒêang load model ƒë·ªÉ generate responses...\")\n",
        "    \n",
        "    # Load model\n",
        "    model_gen = Qwen3VLForConditionalGeneration.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    processor_gen = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "    \n",
        "    print(\"‚úÖ Model ƒë√£ ƒë∆∞·ª£c load!\\n\")\n",
        "    \n",
        "    # ƒê·ªçc dataset\n",
        "    print(f\"üìÇ ƒêang ƒë·ªçc dataset: {DATASET_PATH}...\")\n",
        "    samples = []\n",
        "    with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                samples.append(json.loads(line))\n",
        "    \n",
        "    print(f\"‚úÖ ƒê√£ ƒë·ªçc {len(samples)} samples\\n\")\n",
        "    \n",
        "    # Process t·ª´ng sample\n",
        "    print(\"üîÑ ƒêang generate responses...\")\n",
        "    processed_samples = []\n",
        "    \n",
        "    def generate_response(messages, max_new_tokens=512):\n",
        "        \"\"\"Generate response t·ª´ model\"\"\"\n",
        "        inputs = processor_gen.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_dict=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        inputs = {k: v.to(model_gen.device) for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            generated_ids = model_gen.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=processor_gen.tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        input_length = inputs[\"input_ids\"].shape[1]\n",
        "        generated_ids_trimmed = generated_ids[:, input_length:]\n",
        "        output_text = processor_gen.batch_decode(\n",
        "            generated_ids_trimmed,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=False\n",
        "        )\n",
        "        \n",
        "        return output_text[0].strip()\n",
        "    \n",
        "    for i, sample in enumerate(tqdm(samples, desc=\"Generating\")):\n",
        "        if \"messages\" not in sample:\n",
        "            continue\n",
        "        \n",
        "        messages = sample[\"messages\"]\n",
        "        \n",
        "        # Ki·ªÉm tra xem ƒë√£ c√≥ assistant response ch∆∞a\n",
        "        has_assistant = any(msg.get(\"role\") == \"assistant\" for msg in messages)\n",
        "        if has_assistant:\n",
        "            processed_samples.append(sample)\n",
        "            continue\n",
        "        \n",
        "        # Generate response\n",
        "        try:\n",
        "            response = generate_response(messages, max_new_tokens=512)\n",
        "            \n",
        "            # Th√™m assistant response\n",
        "            new_messages = messages.copy()\n",
        "            new_messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": response\n",
        "            })\n",
        "            \n",
        "            new_sample = {\"messages\": new_messages}\n",
        "            if \"metadata\" in sample:\n",
        "                new_sample[\"metadata\"] = sample[\"metadata\"]\n",
        "            \n",
        "            processed_samples.append(new_sample)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö†Ô∏è  L·ªói ·ªü sample {i+1}: {e}\")\n",
        "            processed_samples.append(sample)\n",
        "    \n",
        "    # L∆∞u dataset m·ªõi\n",
        "    output_path = DATASET_PATH.replace(\".jsonl\", \"_with_responses.jsonl\")\n",
        "    print(f\"\\nüíæ ƒêang l∆∞u dataset m·ªõi: {output_path}...\")\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for sample in processed_samples:\n",
        "            f.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
        "    \n",
        "    DATASET_PATH = output_path\n",
        "    print(f\"‚úÖ ƒê√£ l∆∞u {len(processed_samples)} samples v√†o {output_path}\")\n",
        "    print(f\"‚úÖ Dataset path m·ªõi: {DATASET_PATH}\")\n",
        "    \n",
        "    # Ki·ªÉm tra l·∫°i\n",
        "    check_dataset(DATASET_PATH)\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  B·ªè qua generate responses (GENERATE_RESPONSES = False)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if PUSH_TO_HUB:\n",
        "    if HF_TOKEN:\n",
        "        login(token=HF_TOKEN)\n",
        "        print(\"‚úÖ ƒê√£ login b·∫±ng token\")\n",
        "    else:\n",
        "        print(\"üí° Ch·∫°y l·ªánh sau ƒë·ªÉ login:\")\n",
        "        print(\"   !huggingface-cli login\")\n",
        "        print(\"\\nHo·∫∑c set HF_TOKEN trong cell c·∫•u h√¨nh\")\n",
        "        # Uncomment d√≤ng d∆∞·ªõi n·∫øu mu·ªën login th·ªß c√¥ng\n",
        "        # login()\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Kh√¥ng c·∫ßn login (PUSH_TO_HUB = False)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Load Model v√† Processor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"üì• ƒêang load model: {MODEL_NAME}...\")\n",
        "\n",
        "# C·∫•u h√¨nh quantization\n",
        "bnb_config = None\n",
        "if USE_QUANTIZATION:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    print(\"‚úÖ S·ª≠ d·ª•ng 4-bit quantization\")\n",
        "\n",
        "# Load model\n",
        "model_kwargs = {\n",
        "    \"device_map\": \"auto\",\n",
        "    \"torch_dtype\": torch.bfloat16,\n",
        "    \"trust_remote_code\": True,\n",
        "}\n",
        "\n",
        "if bnb_config:\n",
        "    model_kwargs[\"quantization_config\"] = bnb_config\n",
        "\n",
        "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    **model_kwargs\n",
        ")\n",
        "\n",
        "# Load processor\n",
        "processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "print(\"‚úÖ Model v√† processor ƒë√£ ƒë∆∞·ª£c load!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Setup LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîß ƒêang setup LoRA...\")\n",
        "\n",
        "# Chu·∫©n b·ªã model cho training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# C·∫•u h√¨nh LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# √Åp d·ª•ng LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# In th√¥ng tin\n",
        "model.print_trainable_parameters()\n",
        "print(\"‚úÖ LoRA ƒë√£ ƒë∆∞·ª£c setup!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Load v√† Preprocess Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"üìÇ ƒêang load dataset t·ª´ {DATASET_PATH}...\")\n",
        "\n",
        "# Load t·ª´ JSONL\n",
        "data = []\n",
        "with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            data.append(json.loads(line))\n",
        "\n",
        "dataset = Dataset.from_list(data)\n",
        "print(f\"‚úÖ ƒê√£ load {len(dataset)} samples\")\n",
        "\n",
        "# Preview m·ªôt sample\n",
        "print(\"\\nüìã Preview sample ƒë·∫ßu ti√™n:\")\n",
        "print(json.dumps(data[0], ensure_ascii=False, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    \"\"\"Preprocess function cho dataset\"\"\"\n",
        "    messages_list = examples[\"messages\"]\n",
        "    \n",
        "    input_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    labels_list = []\n",
        "    \n",
        "    for messages in messages_list:\n",
        "        # Apply chat template\n",
        "        inputs = processor.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=False,\n",
        "            return_dict=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        input_ids = inputs[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = inputs.get(\"attention_mask\", None)\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.squeeze(0)\n",
        "        else:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        \n",
        "        # Labels: copy input_ids\n",
        "        labels = input_ids.clone()\n",
        "        \n",
        "        input_ids_list.append(input_ids)\n",
        "        attention_mask_list.append(attention_mask)\n",
        "        labels_list.append(labels)\n",
        "    \n",
        "    # Pad sequences\n",
        "    max_length = min(MAX_LENGTH, max(len(ids) for ids in input_ids_list))\n",
        "    \n",
        "    def pad_sequence(seq, max_len, pad_value=processor.tokenizer.pad_token_id):\n",
        "        if len(seq) >= max_len:\n",
        "            return seq[:max_len]\n",
        "        padded = torch.cat([seq, torch.full((max_len - len(seq),), pad_value, dtype=seq.dtype)])\n",
        "        return padded\n",
        "    \n",
        "    pad_token_id = processor.tokenizer.pad_token_id or processor.tokenizer.eos_token_id\n",
        "    \n",
        "    input_ids_padded = torch.stack([pad_sequence(ids, max_length, pad_token_id) for ids in input_ids_list])\n",
        "    attention_mask_padded = torch.stack([pad_sequence(mask, max_length, 0) for mask in attention_mask_list])\n",
        "    labels_padded = torch.stack([pad_sequence(labels, max_length, -100) for labels in labels_list])\n",
        "    \n",
        "    return {\n",
        "        \"input_ids\": input_ids_padded,\n",
        "        \"attention_mask\": attention_mask_padded,\n",
        "        \"labels\": labels_padded,\n",
        "    }\n",
        "\n",
        "# Preprocess dataset\n",
        "print(\"üîÑ ƒêang preprocess dataset...\")\n",
        "processed_dataset = dataset.map(\n",
        "    lambda x: preprocess_function({\"messages\": [x[\"messages\"]]}),\n",
        "    batched=False,\n",
        "    remove_columns=dataset.column_names,\n",
        ")\n",
        "print(\"‚úÖ ƒê√£ preprocess xong!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Training Arguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=3,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
        "    optim=\"adamw_torch\",\n",
        "    report_to=\"tensorboard\",\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training arguments ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh!\")\n",
        "print(f\"\\nüìä Th√¥ng tin training:\")\n",
        "print(f\"   - Epochs: {EPOCHS}\")\n",
        "print(f\"   - Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   - Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"   - Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"   - Output dir: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. B·∫Øt ƒê·∫ßu Training üöÄ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=processor.tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=processed_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Training\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üöÄ B·∫ÆT ƒê·∫¶U TRAINING\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Dataset: {DATASET_PATH} ({len(dataset)} samples)\")\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Output: {OUTPUT_DIR}\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save final model\n",
        "print(\"\\nüíæ ƒêang l∆∞u model cu·ªëi c√πng...\")\n",
        "trainer.save_model()\n",
        "processor.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"\\n‚úÖ Training ho√†n th√†nh! Model ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Upload l√™n Hugging Face üì§\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if PUSH_TO_HUB and HUB_MODEL_ID:\n",
        "    print(f\"üì§ ƒêang upload model l√™n Hugging Face: {HUB_MODEL_ID}...\")\n",
        "    \n",
        "    # T·∫°o repository n·∫øu ch∆∞a t·ªìn t·∫°i\n",
        "    api = HfApi()\n",
        "    try:\n",
        "        create_repo(\n",
        "            repo_id=HUB_MODEL_ID,\n",
        "            repo_type=\"model\",\n",
        "            private=HUB_PRIVATE,\n",
        "            exist_ok=True\n",
        "        )\n",
        "        print(f\"‚úÖ Repository {HUB_MODEL_ID} ƒë√£ s·∫µn s√†ng\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Repository c√≥ th·ªÉ ƒë√£ t·ªìn t·∫°i ho·∫∑c c√≥ l·ªói: {e}\")\n",
        "    \n",
        "    # Upload model\n",
        "    try:\n",
        "        api.upload_folder(\n",
        "            folder_path=OUTPUT_DIR,\n",
        "            repo_id=HUB_MODEL_ID,\n",
        "            repo_type=\"model\",\n",
        "            commit_message=f\"Upload fine-tuned model after {EPOCHS} epochs training\"\n",
        "        )\n",
        "        print(f\"\\n‚úÖ ƒê√£ upload model th√†nh c√¥ng!\")\n",
        "        print(f\"üîó Link: https://huggingface.co/{HUB_MODEL_ID}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå L·ªói khi upload model: {e}\")\n",
        "        print(f\"üí° B·∫°n c√≥ th·ªÉ upload th·ªß c√¥ng b·∫±ng l·ªánh:\")\n",
        "        print(f\"   !huggingface-cli upload {HUB_MODEL_ID} {OUTPUT_DIR} --repo-type model\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Kh√¥ng upload (PUSH_TO_HUB = False ho·∫∑c HUB_MODEL_ID ch∆∞a ƒë∆∞·ª£c set)\")\n",
        "    if not HUB_MODEL_ID:\n",
        "        print(\"üí° ƒê·∫∑t HUB_MODEL_ID trong cell c·∫•u h√¨nh ƒë·ªÉ upload\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Test Model (T√πy ch·ªçn)\n",
        "\n",
        "Test model sau khi training:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model ƒë√£ fine-tune ƒë·ªÉ test\n",
        "TEST_MODEL = False  # ƒê·∫∑t True ƒë·ªÉ test\n",
        "\n",
        "if TEST_MODEL:\n",
        "    print(\"üì• ƒêang load model ƒë√£ fine-tune...\")\n",
        "    \n",
        "    test_model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
        "        OUTPUT_DIR,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "    test_processor = AutoProcessor.from_pretrained(OUTPUT_DIR)\n",
        "    \n",
        "    # Test v·ªõi m·ªôt c√¢u h·ªèi m·∫´u\n",
        "    test_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"B·∫°n l√† t∆∞ v·∫•n vi√™n ng√¢n h√†ng ch√¢u √Å, n√≥i chuy·ªán trang tr·ªçng, x∆∞ng h√¥ anh/ch·ªã, gi·∫£i th√≠ch r√µ r√†ng v·ªÅ l√£i su·∫•t v√† c√°c g√≥i vay.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"L√£i su·∫•t vay hi·ªán t·∫°i l√† bao nhi√™u?\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    inputs = test_processor.apply_chat_template(\n",
        "        test_messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    inputs = {k: v.to(test_model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    generated_ids = test_model.generate(**inputs, max_new_tokens=256)\n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs[\"input_ids\"], generated_ids)\n",
        "    ]\n",
        "    output_text = test_processor.batch_decode(\n",
        "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üìù TEST RESULT\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Input:\", test_messages[1][\"content\"])\n",
        "    print(\"\\nOutput:\", output_text[0])\n",
        "    print(\"=\"*50)\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  B·ªè qua test (TEST_MODEL = False)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Ho√†n Th√†nh!\n",
        "\n",
        "Model ƒë√£ ƒë∆∞·ª£c train v√† upload (n·∫øu c√≥ c·∫•u h√¨nh). B·∫°n c√≥ th·ªÉ:\n",
        "- S·ª≠ d·ª•ng model t·ª´ `OUTPUT_DIR` ƒë·ªÉ inference local\n",
        "- S·ª≠ d·ª•ng model t·ª´ Hugging Face Hub n·∫øu ƒë√£ upload\n",
        "- Ti·∫øp t·ª•c fine-tune v·ªõi dataset kh√°c\n",
        "\n",
        "**L∆∞u √Ω:** N·∫øu training tr√™n Colab, nh·ªõ download model v·ªÅ local tr∆∞·ªõc khi session h·∫øt h·∫°n!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
