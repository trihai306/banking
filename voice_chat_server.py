#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Voice Chat Server v·ªõi Bank Model
Server realtime ƒë·ªÉ chat b·∫±ng gi·ªçng n√≥i v·ªõi model bank-model-2b.
T√≠nh nƒÉng:
- üéôÔ∏è Nh·∫≠n gi·ªçng n√≥i (Speech-to-Text)
- ü§ñ X·ª≠ l√Ω v·ªõi model Qwen3-VL-2B
- üîä Tr·∫£ l·ªùi b·∫±ng gi·ªçng n√≥i (Text-to-Speech) realtime
- üìä Hi·ªÉn th·ªã t√†i nguy√™n h·ªá th·ªëng (CPU, RAM, GPU) realtime
"""

import torch
import whisper
from transformers import (
    Qwen3VLForConditionalGeneration,
    AutoProcessor,
    BitsAndBytesConfig,
    TextIteratorStreamer
)
import gradio as gr
import io
import base64
import os
import sys
import subprocess
import time
import re
import tempfile
import traceback
from typing import Optional, Tuple
from functools import lru_cache
from threading import Thread
import queue
import psutil

# Import TTS v·ªõi fallback
TTS_AVAILABLE = False
pyttsx3_available = False
tts = None
tts_type = None

try:
    from TTS.api import TTS
    TTS_AVAILABLE = True
    print("‚úÖ Coqui TTS ƒë√£ s·∫µn s√†ng!")
except ImportError:
    print("‚ö†Ô∏è  Coqui TTS kh√¥ng kh·∫£ d·ª•ng, s·∫Ω th·ª≠ pyttsx3...")
    try:
        import pyttsx3
        pyttsx3_available = True
        print("‚úÖ pyttsx3 ƒë√£ s·∫µn s√†ng!")
    except ImportError:
        print("‚ö†Ô∏è  C·∫£ TTS v√† pyttsx3 ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng. C·∫ßn c√†i m·ªôt trong hai.")

# Global variables
model = None
processor = None
whisper_model = None


def install_tts():
    """C√†i ƒë·∫∑t TTS v·ªõi nhi·ªÅu fallback options"""
    global TTS_AVAILABLE, TTS_INSTALLED
    
    print("="*60)
    print("üîä ƒêang c√†i TTS (Text-to-Speech) cho AI Voice Reply...")
    print("="*60)
    
    TTS_INSTALLED = False
    TTS_ERRORS = []
    
    # Ki·ªÉm tra xem TTS ƒë√£ ƒë∆∞·ª£c c√†i ch∆∞a
    try:
        from TTS.api import TTS
        TTS_INSTALLED = True
        TTS_AVAILABLE = True
        print("‚úÖ TTS ƒë√£ ƒë∆∞·ª£c c√†i s·∫µn!")
        return True
    except ImportError:
        print("‚ö†Ô∏è  TTS ch∆∞a ƒë∆∞·ª£c c√†i, ƒëang th·ª≠ c√†i...")
        
        # Option 1: Th·ª≠ c√†i TTS t·ª´ PyPI (version c·ª• th·ªÉ - ·ªïn ƒë·ªãnh h∆°n)
        print("\nüì¶ Option 1: C√†i TTS t·ª´ PyPI (version ·ªïn ƒë·ªãnh)...")
        try:
            result = subprocess.run(
                [sys.executable, "-m", "pip", "install", "-q", "TTS==0.22.0"], 
                capture_output=True, 
                text=True,
                timeout=300
            )
            if result.returncode == 0:
                try:
                    from TTS.api import TTS
                    TTS_INSTALLED = True
                    TTS_AVAILABLE = True
                    print("‚úÖ TTS ƒë√£ ƒë∆∞·ª£c c√†i th√†nh c√¥ng t·ª´ PyPI!")
                    return True
                except ImportError:
                    TTS_ERRORS.append("TTS c√†i nh∆∞ng kh√¥ng import ƒë∆∞·ª£c")
        except subprocess.TimeoutExpired:
            TTS_ERRORS.append("PyPI install timeout")
        except Exception as e:
            TTS_ERRORS.append(f"PyPI install error: {str(e)[:200]}")
        
        # Option 2: Th·ª≠ c√†i TTS t·ª´ PyPI (latest)
        if not TTS_INSTALLED:
            print("\nüì¶ Option 2: C√†i TTS t·ª´ PyPI (latest version)...")
            try:
                result = subprocess.run(
                    [sys.executable, "-m", "pip", "install", "-q", "TTS", "--no-deps"], 
                    capture_output=True, 
                    text=True,
                    timeout=300
                )
                if result.returncode == 0:
                    # C√†i dependencies ri√™ng
                    deps = ["numpy", "scipy", "librosa", "soundfile", "torch", "torchaudio"]
                    for dep in deps:
                        subprocess.run(
                            [sys.executable, "-m", "pip", "install", "-q", dep],
                            capture_output=True,
                            timeout=60
                        )
                    try:
                        from TTS.api import TTS
                        TTS_INSTALLED = True
                        TTS_AVAILABLE = True
                        print("‚úÖ TTS ƒë√£ ƒë∆∞·ª£c c√†i th√†nh c√¥ng t·ª´ PyPI (latest)!")
                        return True
                    except ImportError:
                        TTS_ERRORS.append("TTS c√†i nh∆∞ng kh√¥ng import ƒë∆∞·ª£c sau khi c√†i deps")
            except Exception as e:
                TTS_ERRORS.append(f"PyPI latest install error: {str(e)[:200]}")
        
        # Option 3: Th·ª≠ c√†i t·ª´ GitHub (source)
        if not TTS_INSTALLED:
            print("\nüì¶ Option 3: C√†i TTS t·ª´ GitHub source...")
            try:
                result = subprocess.run(
                    [sys.executable, "-m", "pip", "install", "-q", 
                     "git+https://github.com/coqui-ai/TTS.git@v0.22.0"], 
                    capture_output=True,
                    text=True,
                    timeout=600
                )
                if result.returncode == 0:
                    try:
                        from TTS.api import TTS
                        TTS_INSTALLED = True
                        TTS_AVAILABLE = True
                        print("‚úÖ TTS ƒë√£ ƒë∆∞·ª£c c√†i th√†nh c√¥ng t·ª´ GitHub!")
                        return True
                    except ImportError:
                        TTS_ERRORS.append("TTS c√†i t·ª´ GitHub nh∆∞ng kh√¥ng import ƒë∆∞·ª£c")
            except subprocess.TimeoutExpired:
                TTS_ERRORS.append("GitHub install timeout")
            except Exception as e:
                TTS_ERRORS.append(f"GitHub install error: {str(e)[:200]}")
        
        # Option 4: Fallback - pyttsx3 (nh·∫π h∆°n, kh√¥ng c·∫ßn model l·ªõn)
        if not TTS_INSTALLED:
            print("\nüì¶ Option 4: C√†i pyttsx3 l√†m fallback (nh·∫π h∆°n)...")
            try:
                result = subprocess.run(
                    [sys.executable, "-m", "pip", "install", "-q", "pyttsx3"], 
                    capture_output=True,
                    text=True,
                    timeout=60
                )
                if result.returncode == 0:
                    try:
                        import pyttsx3
                        print("‚úÖ pyttsx3 ƒë√£ ƒë∆∞·ª£c c√†i (fallback TTS)")
                        print("   ‚ö†Ô∏è  L∆∞u √Ω: pyttsx3 c√≥ th·ªÉ c·∫ßn eSpeak tr√™n Linux")
                        return True
                    except ImportError:
                        TTS_ERRORS.append("pyttsx3 c√†i nh∆∞ng kh√¥ng import ƒë∆∞·ª£c")
            except Exception as e:
                TTS_ERRORS.append(f"pyttsx3 install error: {str(e)[:200]}")
    
    # T√≥m t·∫Øt k·∫øt qu·∫£
    print("\n" + "="*60)
    if TTS_INSTALLED:
        print("‚úÖ TTS ƒë√£ s·∫µn s√†ng ƒë·ªÉ s·ª≠ d·ª•ng!")
        print("   AI s·∫Ω c√≥ th·ªÉ tr·∫£ l·ªùi b·∫±ng gi·ªçng n√≥i")
        return True
    else:
        print("‚ö†Ô∏è  TTS ch∆∞a ƒë∆∞·ª£c c√†i th√†nh c√¥ng")
        print("   Server v·∫´n ho·∫°t ƒë·ªông nh∆∞ng AI s·∫Ω kh√¥ng tr·∫£ l·ªùi b·∫±ng gi·ªçng n√≥i")
        print("   (ch·ªâ hi·ªÉn th·ªã text)")
        if TTS_ERRORS:
            print("\n   C√°c l·ªói ƒë√£ g·∫∑p:")
            for i, error in enumerate(TTS_ERRORS, 1):
                print(f"   {i}. {error}")
        print("\n   üí° C√≥ th·ªÉ th·ª≠ c√†i th·ªß c√¥ng:")
        print("      pip install TTS")
        print("      ho·∫∑c")
        print("      pip install git+https://github.com/coqui-ai/TTS.git")
        print("="*60)
        return False


def get_system_resources():
    """
    L·∫•y th√¥ng tin t√†i nguy√™n h·ªá th·ªëng (CPU, RAM, GPU)
    """
    # CPU
    cpu_percent = psutil.cpu_percent(interval=0.1)
    cpu_count = psutil.cpu_count()
    
    # RAM
    memory = psutil.virtual_memory()
    ram_total_gb = memory.total / (1024**3)
    ram_used_gb = memory.used / (1024**3)
    ram_available_gb = memory.available / (1024**3)
    ram_percent = memory.percent
    
    # GPU (n·∫øu c√≥)
    gpu_info = ""
    gpu_memory_used = 0
    gpu_memory_total = 0
    gpu_memory_percent = 0
    
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        gpu_info = f"GPU: {torch.cuda.get_device_name(0)}"
        gpu_memory_used = torch.cuda.memory_allocated(0) / (1024**3)
        gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        gpu_memory_percent = (gpu_memory_used / gpu_memory_total) * 100
    else:
        gpu_info = "GPU: Kh√¥ng c√≥"
    
    return {
        "cpu_percent": cpu_percent,
        "cpu_count": cpu_count,
        "ram_total_gb": ram_total_gb,
        "ram_used_gb": ram_used_gb,
        "ram_available_gb": ram_available_gb,
        "ram_percent": ram_percent,
        "gpu_info": gpu_info,
        "gpu_memory_used": gpu_memory_used,
        "gpu_memory_total": gpu_memory_total,
        "gpu_memory_percent": gpu_memory_percent,
    }


def format_resources_info():
    """
    Format th√¥ng tin t√†i nguy√™n th√†nh string ƒë·ªÉ hi·ªÉn th·ªã
    """
    res = get_system_resources()
    
    info = f"""
### üìä T√†i nguy√™n h·ªá th·ªëng:

**CPU:**
- S·ª≠ d·ª•ng: {res['cpu_percent']:.1f}% / {res['cpu_count']} cores
- C√≤n l·∫°i: {100 - res['cpu_percent']:.1f}%

**RAM:**
- T·ªïng: {res['ram_total_gb']:.2f} GB
- ƒê√£ d√πng: {res['ram_used_gb']:.2f} GB ({res['ram_percent']:.1f}%)
- C√≤n l·∫°i: {res['ram_available_gb']:.2f} GB ({100 - res['ram_percent']:.1f}%)

**{res['gpu_info']}**
"""
    
    if torch.cuda.is_available():
        info += f"""
- T·ªïng: {res['gpu_memory_total']:.2f} GB
- ƒê√£ d√πng: {res['gpu_memory_used']:.2f} GB ({res['gpu_memory_percent']:.1f}%)
- C√≤n l·∫°i: {res['gpu_memory_total'] - res['gpu_memory_used']:.2f} GB ({100 - res['gpu_memory_percent']:.1f}%)
"""
    
    return info


def load_models(model_name: str = "hainguyen306201/bank-model-2b", install_tts_on_load: bool = False):
    """
    Load Whisper model v√† Bank Model
    """
    global model, processor, whisper_model, tts, tts_type, TTS_AVAILABLE
    
    # Load Whisper model cho Speech-to-Text
    print("ƒêang load Whisper model (tiny - nhanh nh·∫•t)...")
    if torch.cuda.is_available():
        print(f"üöÄ GPU c√≥ s·∫µn: {torch.cuda.get_device_name(0)}")
        print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB")
        whisper_model = whisper.load_model("tiny", device="cuda")
        print("‚úÖ Whisper model ƒë√£ load tr√™n GPU!")
    else:
        whisper_model = whisper.load_model("tiny", device="cpu")
        print("‚ö†Ô∏è  Whisper model ƒë√£ load tr√™n CPU (kh√¥ng c√≥ GPU)")
    
    # Load Bank Model t·ª´ Hugging Face
    print("\n" + "="*50)
    print("ƒêang t·∫£i Bank Model t·ª´ Hugging Face...")
    print("="*50)
    
    print(f"Model: {model_name}")
    
    # Ki·ªÉm tra xem model c√≥ t·ªìn t·∫°i kh√¥ng
    try:
        from huggingface_hub import model_info
        info = model_info(model_name)
        print(f"‚úÖ Model t√¨m th·∫•y tr√™n Hugging Face!")
        print(f"   - Model ID: {info.modelId}")
        print(f"   - Files: {len(info.siblings)} files")
    except Exception as e:
        print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ki·ªÉm tra model info: {e}")
        print("   Ti·∫øp t·ª•c t·∫£i model...")
    
    # C·∫•u h√¨nh quantization 4-bit ƒë·ªÉ ti·∫øt ki·ªám memory v√† tƒÉng t·ªëc
    print("\nƒêang c·∫•u h√¨nh quantization (4-bit)...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )
    
    # Load model v·ªõi c√°c t√πy ch·ªçn ƒë·ªÉ ƒë·∫£m b·∫£o t·∫£i ƒë√∫ng v√† ch·∫°y tr√™n GPU
    print("\nƒêang t·∫£i model (c√≥ th·ªÉ m·∫•t v√†i ph√∫t l·∫ßn ƒë·∫ßu)...")
    
    # Ki·ªÉm tra GPU v√† quy·∫øt ƒë·ªãnh quantization
    if torch.cuda.is_available():
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"üöÄ GPU: {torch.cuda.get_device_name(0)}")
        print(f"   GPU Memory: {gpu_memory_gb:.2f} GB")
        
        # N·∫øu GPU >= 20GB, c√≥ th·ªÉ load kh√¥ng quantization ƒë·ªÉ nhanh h∆°n
        if gpu_memory_gb >= 20:
            print("‚úÖ GPU ƒë·ªß l·ªõn, s·∫Ω load model kh√¥ng quantization (full precision) ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô!")
            use_quantization = False
        else:
            print("‚ö†Ô∏è  GPU nh·ªè, s·∫Ω d√πng quantization 4-bit ƒë·ªÉ ti·∫øt ki·ªám memory")
            use_quantization = True
    else:
        print("‚ö†Ô∏è  Kh√¥ng c√≥ GPU, s·∫Ω d√πng quantization 4-bit")
        use_quantization = True
    
    try:
        if use_quantization:
            # D√πng quantization cho GPU nh·ªè ho·∫∑c CPU
            model = Qwen3VLForConditionalGeneration.from_pretrained(
                model_name,
                quantization_config=bnb_config,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                trust_remote_code=True,
                resume_download=True,
                force_download=False,
            )
        else:
            # Load full precision tr√™n GPU l·ªõn (nhanh h∆°n)
            # Ki·ªÉm tra xem c√≥ flash-attn kh√¥ng
            try:
                import flash_attn
                use_flash_attention = True
                print("‚úÖ Flash Attention 2 ƒë∆∞·ª£c ph√°t hi·ªán, s·∫Ω s·ª≠ d·ª•ng ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô")
            except ImportError:
                use_flash_attention = False
                print("‚ö†Ô∏è  Flash Attention 2 ch∆∞a ƒë∆∞·ª£c c√†i, s·ª≠ d·ª•ng attention m·∫∑c ƒë·ªãnh")
                print("   C√≥ th·ªÉ c√†i: pip install flash-attn (t√πy ch·ªçn, ƒë·ªÉ tƒÉng t·ªëc)")
            
            model = Qwen3VLForConditionalGeneration.from_pretrained(
                model_name,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                trust_remote_code=True,
                resume_download=True,
                force_download=False,
                attn_implementation="flash_attention_2" if use_flash_attention else "sdpa",
            )
        
        # ƒê·∫£m b·∫£o model tr√™n GPU v√† t·ªëi ∆∞u
        if torch.cuda.is_available():
            model_device = next(model.parameters()).device
            print(f"‚úÖ Model ƒë√£ ƒë∆∞·ª£c t·∫£i v√† load th√†nh c√¥ng!")
            print(f"   Model device: {model_device}")
            if model_device.type == "cuda":
                print(f"   ‚úÖ Model ƒëang ch·∫°y tr√™n GPU: {torch.cuda.get_device_name(model_device.index)}")
                # T·ªëi ∆∞u: Compile model n·∫øu PyTorch >= 2.0
                try:
                    if hasattr(torch, 'compile') and torch.__version__ >= "2.0.0":
                        print("   üîß ƒêang compile model ƒë·ªÉ tƒÉng t·ªëc (PyTorch 2.0+)...")
                        model = torch.compile(model, mode="reduce-overhead")
                        print("   ‚úÖ Model ƒë√£ ƒë∆∞·ª£c compile!")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Kh√¥ng th·ªÉ compile model: {e} (kh√¥ng ·∫£nh h∆∞·ªüng ch·ª©c nƒÉng)")
            else:
                print(f"   ‚ö†Ô∏è  Model ƒëang ch·∫°y tr√™n {model_device.type}, ƒëang chuy·ªÉn l√™n GPU...")
                model = model.to("cuda")
                try:
                    if hasattr(torch, 'compile') and torch.__version__ >= "2.0.0":
                        model = torch.compile(model, mode="reduce-overhead")
                        print("   ‚úÖ Model ƒë√£ ƒë∆∞·ª£c compile!")
                except:
                    pass
        else:
            print("‚úÖ Model ƒë√£ ƒë∆∞·ª£c t·∫£i v√† load th√†nh c√¥ng tr√™n CPU!")
            
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫£i model: {e}")
        print("\nTh·ª≠ t·∫£i l·∫°i v·ªõi force_download=True...")
        if use_quantization:
            model = Qwen3VLForConditionalGeneration.from_pretrained(
                model_name,
                quantization_config=bnb_config,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                trust_remote_code=True,
                force_download=True,
            )
        else:
            try:
                import flash_attn
                use_flash_attention = True
            except ImportError:
                use_flash_attention = False
            
            model = Qwen3VLForConditionalGeneration.from_pretrained(
                model_name,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                trust_remote_code=True,
                force_download=True,
                attn_implementation="flash_attention_2" if use_flash_attention else "sdpa",
            )
        print("‚úÖ Model ƒë√£ ƒë∆∞·ª£c t·∫£i l·∫°i th√†nh c√¥ng!")
        
        # ƒê·∫£m b·∫£o model tr√™n GPU
        if torch.cuda.is_available():
            model = model.to("cuda")
            print(f"‚úÖ Model ƒë√£ ƒë∆∞·ª£c chuy·ªÉn l√™n GPU!")
    
    # Load processor
    print("\nƒêang t·∫£i processor...")
    processor = AutoProcessor.from_pretrained(
        model_name, 
        trust_remote_code=True,
        resume_download=True,
    )
    print("‚úÖ Processor ƒë√£ load!")
    
    # Ki·ªÉm tra processor c√≥ ƒë√∫ng kh√¥ng
    if not hasattr(processor, 'apply_chat_template'):
        print("‚ö†Ô∏è  Processor kh√¥ng c√≥ apply_chat_template, c√≥ th·ªÉ c√≥ v·∫•n ƒë·ªÅ")
    else:
        print("‚úÖ Processor c√≥ apply_chat_template - OK")
    
    # Ki·ªÉm tra tokenizer
    if hasattr(processor, 'tokenizer'):
        print(f"‚úÖ Tokenizer: {type(processor.tokenizer).__name__}")
        if hasattr(processor.tokenizer, 'eos_token_id') and processor.tokenizer.eos_token_id:
            print(f"   EOS token ID: {processor.tokenizer.eos_token_id}")
        else:
            print("   ‚ö†Ô∏è  EOS token ID kh√¥ng ƒë∆∞·ª£c set, s·∫Ω d√πng pad_token_id")
    
    # Ki·ªÉm tra v√† hi·ªÉn th·ªã th√¥ng tin GPU
    print("\n" + "="*50)
    print("üìä Th√¥ng tin GPU v√† Model:")
    print("="*50)
    if torch.cuda.is_available():
        print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")
        print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB")
        
        # Ki·ªÉm tra model ƒë√£ ƒë∆∞·ª£c load ch∆∞a
        try:
            if model is not None:
                model_device = next(model.parameters()).device
                if model_device.type == "cuda":
                    print(f"   ‚úÖ Model ƒëang ch·∫°y tr√™n GPU: {torch.cuda.get_device_name(model_device.index)}")
                    print(f"   GPU Memory ƒë√£ d√πng: {torch.cuda.memory_allocated(0) / (1024**3):.2f} GB")
                else:
                    print(f"   ‚ö†Ô∏è  Model ƒëang ch·∫°y tr√™n {model_device.type}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Kh√¥ng th·ªÉ ki·ªÉm tra model device: {e}")
        
        # Ki·ªÉm tra Whisper model
        try:
            if whisper_model is not None:
                if hasattr(whisper_model, 'encoder') and hasattr(whisper_model.encoder, 'parameters'):
                    whisper_device = next(whisper_model.encoder.parameters()).device
                    if whisper_device.type == "cuda":
                        print(f"   ‚úÖ Whisper ƒëang ch·∫°y tr√™n GPU")
                    else:
                        print(f"   ‚ö†Ô∏è  Whisper ƒëang ch·∫°y tr√™n CPU")
                else:
                    print(f"   ‚úÖ Whisper ƒë√£ ƒë∆∞·ª£c load v·ªõi device ph√π h·ª£p")
        except Exception as e:
            print(f"   ‚ÑπÔ∏è  Kh√¥ng th·ªÉ ki·ªÉm tra Whisper device: {e}")
    else:
        print("‚ö†Ô∏è  Kh√¥ng c√≥ GPU, t·∫•t c·∫£ ƒëang ch·∫°y tr√™n CPU")
    
    print("\n" + "="*50)
    print("‚úÖ Bank Model ƒë√£ s·∫µn s√†ng ƒë·ªÉ s·ª≠ d·ª•ng!")
    print("="*50)
    
    # T·ªëi ∆∞u: Warmup model ƒë·ªÉ tƒÉng t·ªëc l·∫ßn ƒë·∫ßu generate
    print("\nüî• ƒêang warmup model (l·∫ßn ƒë·∫ßu c√≥ th·ªÉ ch·∫≠m)...")
    try:
        warmup_messages = [{"role": "user", "content": [{"type": "text", "text": "Hello"}]}]
        warmup_inputs = processor.apply_chat_template(
            warmup_messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        if torch.cuda.is_available():
            warmup_inputs = {k: v.to("cuda") if isinstance(v, torch.Tensor) else v for k, v in warmup_inputs.items()}
        
        model.eval()
        with torch.inference_mode():
            _ = model.generate(
                **warmup_inputs,
                max_new_tokens=10,
                do_sample=False,
                use_cache=True,
            )
        print("‚úÖ Model ƒë√£ ƒë∆∞·ª£c warmup - s·∫µn s√†ng generate nhanh!")
    except Exception as e:
        print(f"‚ö†Ô∏è  Warmup kh√¥ng th√†nh c√¥ng (kh√¥ng ·∫£nh h∆∞·ªüng ch·ª©c nƒÉng): {e}")
    
    # Load TTS offline
    print("\n" + "="*50)
    print("ƒêang load TTS model (offline)...")
    print("="*50)
    
    # ƒê·∫£m b·∫£o TTS ch·∫°y tr√™n GPU n·∫øu c√≥
    use_gpu_tts = torch.cuda.is_available()
    if use_gpu_tts:
        print(f"üöÄ TTS s·∫Ω ch·∫°y tr√™n GPU: {torch.cuda.get_device_name(0)}")
    else:
        print("‚ö†Ô∏è  TTS s·∫Ω ch·∫°y tr√™n CPU (kh√¥ng c√≥ GPU)")
    
    # Ki·ªÉm tra l·∫°i TTS
    if install_tts_on_load:
        install_tts()
    
    if not TTS_AVAILABLE:
        print("‚ö†Ô∏è  Coqui TTS ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t, ƒëang th·ª≠ c√†i l·∫°i...")
        try:
            print("Th·ª≠ c√†i TTS==0.22.0...")
            result = subprocess.run(
                [sys.executable, "-m", "pip", "install", "-q", "TTS==0.22.0"],
                capture_output=True,
                text=True,
                timeout=300
            )
            if result.returncode == 0:
                print("‚úÖ ƒê√£ c√†i TTS t·ª´ PyPI, ƒëang import l·∫°i...")
                try:
                    from TTS.api import TTS
                    TTS_AVAILABLE = True
                    print("‚úÖ Coqui TTS ƒë√£ s·∫µn s√†ng!")
                except ImportError:
                    print("‚ö†Ô∏è  V·∫´n kh√¥ng th·ªÉ import TTS sau khi c√†i")
        except Exception as e:
            print(f"‚ö†Ô∏è  L·ªói khi c√†i TTS: {e}")
    
    # Load TTS model v·ªõi nhi·ªÅu fallback options
    if TTS_AVAILABLE:
        print("\nüîä ƒêang th·ª≠ load TTS models (theo th·ª© t·ª± ∆∞u ti√™n)...")
        
        # Option 1: XTTS v2
        if tts is None:
            try:
                print("üì¶ Option 1: Th·ª≠ load XTTS v2 (ch·∫•t l∆∞·ª£ng cao, multilingual)...")
                tts = TTS(model_name="tts_models/multilingual/multi-dataset/xtts_v2", gpu=use_gpu_tts)
                tts_type = "coqui_xtts"
                if use_gpu_tts:
                    print("‚úÖ TTS model (XTTS v2) ƒë√£ load tr√™n GPU!")
                else:
                    print("‚úÖ TTS model (XTTS v2) ƒë√£ load tr√™n CPU!")
            except Exception as e:
                print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ load XTTS v2: {str(e)[:200]}")
                tts = None
        
        # Option 2: Model ti·∫øng Vi·ªát
        if tts is None:
            try:
                print("üì¶ Option 2: Th·ª≠ load TTS model ti·∫øng Vi·ªát...")
                tts = TTS(model_name="tts_models/vi/vietnamese", gpu=use_gpu_tts)
                tts_type = "coqui_vi"
                if use_gpu_tts:
                    print("‚úÖ TTS model ti·∫øng Vi·ªát ƒë√£ load tr√™n GPU!")
                else:
                    print("‚úÖ TTS model ti·∫øng Vi·ªát ƒë√£ load tr√™n CPU!")
            except Exception as e:
                print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ load TTS ti·∫øng Vi·ªát: {str(e)[:200]}")
                tts = None
        
        # Option 3: Model ƒë∆°n gi·∫£n
        if tts is None:
            try:
                print("üì¶ Option 3: Th·ª≠ load TTS model ƒë∆°n gi·∫£n (English)...")
                tts = TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC", gpu=use_gpu_tts)
                tts_type = "coqui_en"
                if use_gpu_tts:
                    print("‚úÖ TTS model ƒë∆°n gi·∫£n ƒë√£ load tr√™n GPU!")
                else:
                    print("‚úÖ TTS model ƒë∆°n gi·∫£n ƒë√£ load tr√™n CPU!")
            except Exception as e:
                print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ load TTS ƒë∆°n gi·∫£n: {str(e)[:200]}")
                tts = None
        
        # Option 4: Model m·∫∑c ƒë·ªãnh
        if tts is None:
            try:
                print("üì¶ Option 4: Th·ª≠ load TTS model m·∫∑c ƒë·ªãnh...")
                tts = TTS(gpu=use_gpu_tts)
                tts_type = "coqui_default"
                if use_gpu_tts:
                    print("‚úÖ TTS model m·∫∑c ƒë·ªãnh ƒë√£ load tr√™n GPU!")
                else:
                    print("‚úÖ TTS model m·∫∑c ƒë·ªãnh ƒë√£ load tr√™n CPU!")
            except Exception as e:
                print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ load TTS m·∫∑c ƒë·ªãnh: {str(e)[:200]}")
                tts = None
    
    # Fallback: d√πng pyttsx3
    if tts is None:
        print("\nüì¶ Fallback: Th·ª≠ d√πng pyttsx3...")
        try:
            import pyttsx3
            tts = pyttsx3.init()
            tts_type = "pyttsx3"
            try:
                voices = tts.getProperty('voices')
                for voice in voices:
                    if 'vietnamese' in voice.name.lower() or 'vi' in voice.id.lower():
                        tts.setProperty('voice', voice.id)
                        print(f"‚úÖ ƒê√£ ch·ªçn gi·ªçng: {voice.name}")
                        break
            except:
                pass
            print("‚úÖ TTS model (pyttsx3) ƒë√£ load!")
            print("   ‚ö†Ô∏è  L∆∞u √Ω: pyttsx3 c√≥ th·ªÉ c·∫ßn eSpeak tr√™n Linux")
        except Exception as e:
            print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ load pyttsx3: {str(e)[:200]}")
            tts = None
    
    # T√≥m t·∫Øt k·∫øt qu·∫£
    print("\n" + "="*50)
    if tts is not None:
        print(f"‚úÖ TTS ƒë√£ s·∫µn s√†ng! Type: {tts_type}")
        print("   AI s·∫Ω c√≥ th·ªÉ tr·∫£ l·ªùi b·∫±ng gi·ªçng n√≥i")
    else:
        print("‚ö†Ô∏è  C·∫¢NH B√ÅO: Kh√¥ng c√≥ TTS n√†o kh·∫£ d·ª•ng!")
        print("="*50)
        print("Server v·∫´n ho·∫°t ƒë·ªông nh∆∞ng AI s·∫Ω KH√îNG tr·∫£ l·ªùi b·∫±ng gi·ªçng n√≥i")
        print("(ch·ªâ hi·ªÉn th·ªã text response)")
        print("\nüí° C√≥ th·ªÉ th·ª≠ c√°c c√°ch sau ƒë·ªÉ c√†i TTS:")
        print("   1. Ch·∫°y l·∫°i v·ªõi install_tts_on_load=True")
        print("   2. C√†i th·ªß c√¥ng: pip install TTS==0.22.0")
        print("   3. Ho·∫∑c c√†i t·ª´ source: pip install git+https://github.com/coqui-ai/TTS.git@v0.22.0")
        print("   4. Ho·∫∑c c√†i pyttsx3 (nh·∫π h∆°n): pip install pyttsx3")
        print("="*50)


def speech_to_text(audio_path: Optional[str]) -> str:
    """
    Chuy·ªÉn ƒë·ªïi file audio th√†nh text s·ª≠ d·ª•ng Whisper (t·ªëi ∆∞u t·ªëc ƒë·ªô)
    """
    global whisper_model
    
    if whisper_model is None:
        return "[L·ªói: Whisper model ch∆∞a ƒë∆∞·ª£c load]"
    
    if audio_path is None:
        return ""
    
    try:
        use_fp16 = torch.cuda.is_available()
        result = whisper_model.transcribe(
            audio_path,
            language="vi",
            fp16=use_fp16,
            verbose=False,
            condition_on_previous_text=False,
            initial_prompt="ƒê√¢y l√† m·ªôt cu·ªôc tr√≤ chuy·ªán b·∫±ng ti·∫øng Vi·ªát.",
            compression_ratio_threshold=2.4,
            logprob_threshold=-1.0,
            no_speech_threshold=0.6,
            beam_size=1,
            best_of=1,
            temperature=0.0,
        )
        text = result["text"].strip()
        return text
    except Exception as e:
        print(f"L·ªói trong speech-to-text: {e}")
        try:
            result = whisper_model.transcribe(
                audio_path,
                language="vi",
                fp16=torch.cuda.is_available(),
                verbose=False,
                beam_size=1,
            )
            return result["text"].strip()
        except:
            return ""


def process_with_model_stream(text: str):
    """
    X·ª≠ l√Ω text v·ªõi Bank Model v·ªõi streaming (yield t·ª´ng ph·∫ßn text)
    """
    global model, processor
    
    if model is None:
        yield "Xin l·ªói, model ch∆∞a ƒë∆∞·ª£c load. Vui l√≤ng ch·∫°y load_models() tr∆∞·ªõc."
        return
    
    if processor is None:
        yield "Xin l·ªói, processor ch∆∞a ƒë∆∞·ª£c load. Vui l√≤ng ch·∫°y load_models() tr∆∞·ªõc."
        return
    
    if not text.strip():
        yield "Xin l·ªói, t√¥i kh√¥ng hi·ªÉu. B·∫°n c√≥ th·ªÉ vi·∫øt l·∫°i ƒë∆∞·ª£c kh√¥ng?"
        return
    
    try:
        messages = [
            {
                "role": "user",
                "content": [{"type": "text", "text": text}]
            }
        ]
        
        inputs = processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        
        if not isinstance(inputs, dict):
            raise ValueError("Inputs ph·∫£i l√† dict sau apply_chat_template")
        if "input_ids" not in inputs:
            raise ValueError("Inputs ph·∫£i c√≥ 'input_ids'")
        
        if len(inputs["input_ids"].shape) != 2:
            inputs["input_ids"] = inputs["input_ids"].unsqueeze(0)
        
        if torch.cuda.is_available():
            try:
                model_device = next(model.parameters()).device
                if model_device.type != "cuda":
                    model = model.to("cuda")
                    model_device = next(model.parameters()).device
            except Exception as e:
                print(f"‚ö†Ô∏è  L·ªói khi ki·ªÉm tra model device: {e}")
                model_device = torch.device("cuda")
                try:
                    model = model.to("cuda")
                except:
                    model_device = torch.device("cpu")
            
            device = model_device
            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
        else:
            device = torch.device("cpu")
            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
        
        tokenizer = processor.tokenizer
        eos_token_id = getattr(tokenizer, 'eos_token_id', None) or getattr(tokenizer, 'pad_token_id', None)
        
        streamer = TextIteratorStreamer(
            tokenizer,
            skip_prompt=True,
            skip_special_tokens=True,
            timeout=60.0,
            clean_up_tokenization_spaces=True,
        )
        
        model.eval()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        generation_kwargs = {
            **inputs,
            "max_new_tokens": 512,
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 50,
            "do_sample": True,
            "pad_token_id": eos_token_id,
            "eos_token_id": eos_token_id,
            "use_cache": True,
            "num_beams": 1,
            "repetition_penalty": 1.1,
            "streamer": streamer,
        }
        
        generation_error = [None]
        
        def generate_with_error_handling():
            try:
                with torch.inference_mode():
                    model.generate(**generation_kwargs)
            except Exception as e:
                generation_error[0] = e
                print(f"‚ùå L·ªói trong generation thread: {e}")
        
        thread = Thread(target=generate_with_error_handling)
        thread.daemon = True
        thread.start()
        
        generated_text = ""
        try:
            for new_text in streamer:
                if generation_error[0]:
                    raise generation_error[0]
                generated_text += new_text
                yield generated_text
        except Exception as e:
            print(f"‚ö†Ô∏è  L·ªói trong streaming: {e}")
            if generated_text:
                yield generated_text
            else:
                yield f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi generate: {str(e)}"
        finally:
            thread.join(timeout=5.0)
            if thread.is_alive():
                print("‚ö†Ô∏è  Generation thread v·∫´n ch·∫°y sau timeout")
    
    except Exception as e:
        print(f"L·ªói trong model processing: {e}")
        traceback.print_exc()
        yield f"Xin l·ªói, c√≥ l·ªói x·∫£y ra: {str(e)}"


def process_with_model(text: str) -> str:
    """
    X·ª≠ l√Ω text v·ªõi Bank Model (t·ªëi ∆∞u t·ªëc ƒë·ªô) - ch·ªâ text, kh√¥ng c√≥ ·∫£nh
    """
    global model, processor
    
    if model is None:
        return "Xin l·ªói, model ch∆∞a ƒë∆∞·ª£c load. Vui l√≤ng ch·∫°y load_models() tr∆∞·ªõc."
    
    if processor is None:
        return "Xin l·ªói, processor ch∆∞a ƒë∆∞·ª£c load. Vui l√≤ng ch·∫°y load_models() tr∆∞·ªõc."
    
    if not text.strip():
        return "Xin l·ªói, t√¥i kh√¥ng nghe r√µ. B·∫°n c√≥ th·ªÉ n√≥i l·∫°i ƒë∆∞·ª£c kh√¥ng?"
    
    try:
        messages = [
            {
                "role": "user",
                "content": [{"type": "text", "text": text}]
            }
        ]
        
        inputs = processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        
        if not isinstance(inputs, dict) or "input_ids" not in inputs:
            raise ValueError("Inputs kh√¥ng ƒë√∫ng format")
        
        if len(inputs["input_ids"].shape) != 2:
            inputs["input_ids"] = inputs["input_ids"].unsqueeze(0)
        
        tokenizer = processor.tokenizer
        eos_token_id = getattr(tokenizer, 'eos_token_id', None) or getattr(tokenizer, 'pad_token_id', None)
        if eos_token_id is None:
            raise ValueError("Tokenizer ph·∫£i c√≥ eos_token_id ho·∫∑c pad_token_id")
        
        if torch.cuda.is_available():
            try:
                model_device = next(model.parameters()).device
                if model_device.type != "cuda":
                    print("‚ö†Ô∏è  Model kh√¥ng tr√™n GPU, ƒëang chuy·ªÉn l√™n GPU...")
                    model = model.to("cuda")
                    model_device = next(model.parameters()).device
                    print(f"‚úÖ Model ƒë√£ chuy·ªÉn l√™n GPU: {torch.cuda.get_device_name(model_device.index)}")
            except Exception as e:
                print(f"‚ö†Ô∏è  L·ªói khi ki·ªÉm tra model device: {e}")
                try:
                    model = model.to("cuda")
                    model_device = torch.device("cuda")
                    print("‚úÖ Model ƒë√£ ƒë∆∞·ª£c chuy·ªÉn l√™n GPU")
                except:
                    model_device = torch.device("cpu")
                    print("‚ö†Ô∏è  Kh√¥ng th·ªÉ chuy·ªÉn model l√™n GPU, s·ª≠ d·ª•ng CPU")
            
            device = model_device
            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
        else:
            device = torch.device("cpu")
            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
            print("‚ö†Ô∏è  Kh√¥ng c√≥ GPU, model ƒëang ch·∫°y tr√™n CPU")
        
        model.eval()
        with torch.inference_mode():
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.9,
                top_k=50,
                do_sample=True,
                pad_token_id=eos_token_id,
                eos_token_id=eos_token_id,
                use_cache=True,
                num_beams=2,
                repetition_penalty=1.1,
                length_penalty=1.0,
                early_stopping=True,
                output_scores=False,
                return_dict_in_generate=False,
            )
        
        input_length = inputs["input_ids"].shape[1]
        
        if len(generated_ids.shape) == 1:
            generated_ids = generated_ids.unsqueeze(0)
        
        generated_ids_trimmed = [
            out_ids[input_length:].cpu()
            for out_ids in generated_ids
        ]
        
        output_text = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True,
        )[0]
        
        output_text = output_text.strip()
        output_text = re.sub(r'\s+', ' ', output_text)
        output_text = output_text.strip()
        
        if not output_text:
            return "Xin l·ªói, t√¥i kh√¥ng th·ªÉ t·∫°o response. Vui l√≤ng th·ª≠ l·∫°i."
        
        return output_text
    
    except Exception as e:
        print(f"L·ªói trong model processing: {e}")
        return f"Xin l·ªói, c√≥ l·ªói x·∫£y ra: {str(e)}"


def text_to_speech(text: str, lang: str = "vi") -> Optional[str]:
    """
    Chuy·ªÉn ƒë·ªïi text th√†nh file audio s·ª≠ d·ª•ng TTS (offline, t·ªëi ∆∞u t·ªëc ƒë·ªô)
    """
    global tts, tts_type
    
    if tts is None:
        print("‚ö†Ô∏è  TTS ch∆∞a ƒë∆∞·ª£c load")
        return None
    
    if not text.strip():
        return None
    
    try:
        max_chars = 500
        if len(text) > max_chars:
            text = text[:max_chars] + "..."
        
        fd, audio_path = tempfile.mkstemp(suffix=".wav", prefix="tts_")
        os.close(fd)
        
        try:
            if tts_type == "pyttsx3":
                tts.save_to_file(text, audio_path)
                tts.runAndWait()
            elif tts_type and tts_type.startswith("coqui"):
                if tts_type == "coqui_xtts":
                    tts.tts_to_file(
                        text=text,
                        file_path=audio_path,
                        language=lang,
                        speaker_wav=None,
                        speed=1.3,
                    )
                else:
                    tts.tts_to_file(text=text, file_path=audio_path)
            else:
                if hasattr(tts, 'tts_to_file'):
                    tts.tts_to_file(text=text, file_path=audio_path)
                else:
                    print("TTS kh√¥ng h·ªó tr·ª£ tts_to_file")
                    return None
            
            return audio_path
        except Exception as e:
            if os.path.exists(audio_path):
                try:
                    os.remove(audio_path)
                except:
                    pass
            raise e
        
    except Exception as e:
        print(f"L·ªói trong text-to-speech: {e}")
        return None


def create_gradio_interface():
    """T·∫°o Gradio interface"""
    
    # H√†m x·ª≠ l√Ω text input v·ªõi streaming (kh√¥ng c√≥ TTS)
    def chat_text_stream(user_text, history):
        if not user_text or not user_text.strip():
            return history, "", None
        
        if history is None:
            history = []
        history.append((user_text.strip(), None))
        
        response_text = ""
        try:
            for partial_response in process_with_model_stream(user_text.strip()):
                response_text = partial_response
                history[-1] = (user_text.strip(), response_text)
                yield history, "", None
        except Exception as e:
            print(f"L·ªói trong chat_stream: {e}")
            response_text = f"Xin l·ªói, c√≥ l·ªói x·∫£y ra: {str(e)}"
            history[-1] = (user_text.strip(), response_text)
        
        yield history, "", None
    
    # H√†m x·ª≠ l√Ω text input v·ªõi streaming + TTS
    def chat_text_stream_with_tts(user_text, history):
        if not user_text or not user_text.strip():
            return history, "", None
        
        if history is None:
            history = []
        history.append((user_text.strip(), None))
        
        response_text = ""
        try:
            for partial_response in process_with_model_stream(user_text.strip()):
                response_text = partial_response
                history[-1] = (user_text.strip(), response_text)
                yield history, "", None
        except Exception as e:
            print(f"L·ªói trong chat_stream: {e}")
            response_text = f"Xin l·ªói, c√≥ l·ªói x·∫£y ra: {str(e)}"
            history[-1] = (user_text.strip(), response_text)
        
        audio_output_path = None
        if response_text and response_text.strip():
            try:
                print("üîä ƒêang t·∫°o AI voice reply...")
                audio_output_path = text_to_speech(response_text, lang="vi")
                if audio_output_path:
                    print(f"‚úÖ AI voice reply ƒë√£ ƒë∆∞·ª£c t·∫°o: {audio_output_path}")
            except Exception as e:
                print(f"‚ö†Ô∏è  L·ªói khi t·∫°o TTS: {e}")
        
        yield history, "", audio_output_path
    
    # H√†m x·ª≠ l√Ω voice input v·ªõi streaming + TTS
    def chat_voice_stream_with_tts(audio_input, history):
        if audio_input is None:
            return history, None
        
        if whisper_model is None:
            error_msg = "Whisper model ch∆∞a ƒë∆∞·ª£c load"
            if history is None:
                history = []
            history.append(("[L·ªói]", error_msg))
            return history, None
        
        user_text = speech_to_text(audio_input)
        
        if not user_text or not user_text.strip():
            error_msg = "Xin l·ªói, t√¥i kh√¥ng nghe r√µ. B·∫°n c√≥ th·ªÉ n√≥i l·∫°i ƒë∆∞·ª£c kh√¥ng?"
            if history is None:
                history = []
            history.append(("[Kh√¥ng nghe r√µ]", error_msg))
            audio_output_path = None
            try:
                audio_output_path = text_to_speech(error_msg, lang="vi")
            except:
                pass
            return history, audio_output_path
        
        if history is None:
            history = []
        history.append((user_text.strip(), None))
        
        response_text = ""
        try:
            for partial_response in process_with_model_stream(user_text.strip()):
                response_text = partial_response
                history[-1] = (user_text.strip(), response_text)
                yield history, None
        except Exception as e:
            print(f"L·ªói trong chat_voice_stream: {e}")
            response_text = f"Xin l·ªói, c√≥ l·ªói x·∫£y ra: {str(e)}"
            history[-1] = (user_text.strip(), response_text)
        
        audio_output_path = None
        if response_text and response_text.strip():
            try:
                print("üîä ƒêang t·∫°o AI voice reply...")
                audio_output_path = text_to_speech(response_text, lang="vi")
                if audio_output_path:
                    print(f"‚úÖ AI voice reply ƒë√£ ƒë∆∞·ª£c t·∫°o: {audio_output_path}")
            except Exception as e:
                print(f"‚ö†Ô∏è  L·ªói khi t·∫°o TTS: {e}")
        
        yield history, audio_output_path
    
    # T·∫°o Gradio interface
    with gr.Blocks(title="Chat v·ªõi Bank Model - Text & Voice Input + AI Voice Reply") as demo:
        gr.Markdown("""
        # üí¨ Chat v·ªõi Bank Model - Text & Voice Input + AI Voice Reply
        
        Chat b·∫±ng text HO·∫∂C gi·ªçng n√≥i v·ªõi AI model, AI s·∫Ω tr·∫£ l·ªùi b·∫±ng gi·ªçng n√≥i!
        - ‚úçÔ∏è Nh·∫≠p text v√† nh·∫•n Enter ho·∫∑c n√∫t G·ª≠i
        - üéôÔ∏è Ho·∫∑c n√≥i v√†o microphone
        - ‚ö° Response ƒë∆∞·ª£c stream t·ª´ng ph·∫ßn (kh√¥ng c·∫ßn ch·ªù)
        - üîä AI tr·∫£ l·ªùi b·∫±ng gi·ªçng n√≥i (TTS)
        """)
        
        with gr.Row():
            with gr.Column(scale=2):
                chatbot = gr.Chatbot(
                    label="üí¨ L·ªãch s·ª≠ chat",
                    height=400,
                    show_label=True,
                    type="tuples",
                    allow_tags=False
                )
                
                audio_output = gr.Audio(
                    label="üîä AI Voice Reply",
                    type="filepath",
                    show_label=True,
                    visible=True
                )
            
            with gr.Column(scale=1):
                with gr.Tabs():
                    with gr.Tab("‚úçÔ∏è Text Input"):
                        text_input = gr.Textbox(
                            label="Nh·∫≠p c√¢u h·ªèi",
                            placeholder="Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n ·ªü ƒë√¢y...",
                            lines=3,
                            show_label=True
                        )
                        text_submit_btn = gr.Button("G·ª≠i Text", variant="primary", size="lg")
                        text_with_voice_btn = gr.Button("G·ª≠i Text + AI Voice", variant="secondary", size="lg")
                    
                    with gr.Tab("üéôÔ∏è Voice Input"):
                        audio_input = gr.Audio(
                            sources=["microphone"],
                            type="filepath",
                            label="N√≥i v√†o ƒë√¢y",
                            show_label=True
                        )
                        audio_submit_btn = gr.Button("G·ª≠i Voice + AI Voice", variant="primary", size="lg")
                
                clear_btn = gr.Button("X√≥a l·ªãch s·ª≠", variant="secondary")
                
                resources_display = gr.Markdown(
                    value=format_resources_info(),
                    label="üìä T√†i nguy√™n h·ªá th·ªëng"
                )
        
        # Event handlers
        text_submit_btn.click(
            fn=chat_text_stream,
            inputs=[text_input, chatbot],
            outputs=[chatbot, text_input, audio_output],
            show_progress=False
        )
        
        text_input.submit(
            fn=chat_text_stream,
            inputs=[text_input, chatbot],
            outputs=[chatbot, text_input, audio_output],
            show_progress=False
        )
        
        text_with_voice_btn.click(
            fn=chat_text_stream_with_tts,
            inputs=[text_input, chatbot],
            outputs=[chatbot, text_input, audio_output],
            show_progress=False
        )
        
        audio_submit_btn.click(
            fn=chat_voice_stream_with_tts,
            inputs=[audio_input, chatbot],
            outputs=[chatbot, audio_output],
            show_progress=False
        )
        
        audio_input.stop_recording(
            fn=chat_voice_stream_with_tts,
            inputs=[audio_input, chatbot],
            outputs=[chatbot, audio_output],
            show_progress=False
        )
        
        clear_btn.click(
            fn=lambda: [],
            outputs=[chatbot]
        )
        
        demo.load(
            fn=lambda: format_resources_info(),
            inputs=None,
            outputs=resources_display
        )
        
        gr.Markdown("""
        ### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:
        1. **Text Input**: 
           - ‚úçÔ∏è Nh·∫≠p c√¢u h·ªèi v√†o √¥ text
           - ‚èé Nh·∫•n Enter ho·∫∑c n√∫t "G·ª≠i Text" (ch·ªâ text, kh√¥ng c√≥ voice)
           - üîä Ho·∫∑c nh·∫•n "G·ª≠i Text + AI Voice" (AI s·∫Ω n√≥i tr·∫£ l·ªùi)
        
        2. **Voice Input**:
           - üéôÔ∏è Nh·∫•n n√∫t microphone v√† b·∫Øt ƒë·∫ßu n√≥i
           - ‚èπÔ∏è D·ª´ng recording ho·∫∑c nh·∫•n "G·ª≠i Voice + AI Voice"
           - üîä AI s·∫Ω t·ª± ƒë·ªông tr·∫£ l·ªùi b·∫±ng gi·ªçng n√≥i
        
        3. ‚ö° Xem response ƒë∆∞·ª£c stream t·ª´ng ph·∫ßn (kh√¥ng c·∫ßn ch·ªù)
        4. üîä Nghe AI voice reply ·ªü ph·∫ßn "AI Voice Reply" b√™n d∆∞·ªõi
        5. üìä Xem t√†i nguy√™n h·ªá th·ªëng ·ªü b√™n ph·∫£i
        
        ### ‚ö° T√≠nh nƒÉng:
        - Response ƒë∆∞·ª£c stream realtime, kh√¥ng c·∫ßn ch·ªù to√†n b·ªô
        - H·ªó tr·ª£ c·∫£ text v√† voice input
        - Model ch·∫°y tr√™n GPU ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô
        - üîä **AI tr·∫£ l·ªùi b·∫±ng gi·ªçng n√≥i (TTS)** - T√≠nh nƒÉng m·ªõi!
        - TTS offline (Coqui TTS ho·∫∑c pyttsx3)
        
        ### L∆∞u √Ω:
        - C√≥ th·ªÉ d√πng text HO·∫∂C voice ƒë·ªÉ input
        - Response hi·ªÉn th·ªã text V√Ä c√≥ audio (AI n√≥i)
        - Response ƒë∆∞·ª£c stream t·ª´ng token
        - TTS s·∫Ω ƒë∆∞·ª£c t·∫°o sau khi c√≥ full response
        - N·∫øu TTS kh√¥ng kh·∫£ d·ª•ng, ch·ªâ hi·ªÉn th·ªã text
        """)
    
    return demo


def main():
    """H√†m main ƒë·ªÉ ch·∫°y server"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Voice Chat Server v·ªõi Bank Model")
    parser.add_argument("--model-name", type=str, default="hainguyen306201/bank-model-2b",
                       help="T√™n model tr√™n Hugging Face")
    parser.add_argument("--install-tts", action="store_true",
                       help="C√†i ƒë·∫∑t TTS khi load model")
    parser.add_argument("--host", type=str, default="0.0.0.0",
                       help="Host ƒë·ªÉ ch·∫°y server")
    parser.add_argument("--port", type=int, default=7860,
                       help="Port ƒë·ªÉ ch·∫°y server")
    parser.add_argument("--share", action="store_true",
                       help="T·∫°o public link (Gradio share)")
    parser.add_argument("--debug", action="store_true",
                       help="Ch·∫°y ·ªü ch·∫ø ƒë·ªô debug")
    
    args = parser.parse_args()
    
    print("="*60)
    print("üöÄ Kh·ªüi ƒë·ªông Voice Chat Server v·ªõi Bank Model")
    print("="*60)
    
    # Load models
    print("\nüì¶ ƒêang load models...")
    load_models(model_name=args.model_name, install_tts_on_load=args.install_tts)
    
    # T·∫°o Gradio interface
    print("\nüé® ƒêang t·∫°o Gradio interface...")
    demo = create_gradio_interface()
    print("‚úÖ Gradio interface ƒë√£ ƒë∆∞·ª£c t·∫°o!")
    
    # Kh·ªüi ƒë·ªông server
    print("\nüåê ƒêang kh·ªüi ƒë·ªông server...")
    print(f"   Host: {args.host}")
    print(f"   Port: {args.port}")
    print(f"   Share: {args.share}")
    print(f"   Debug: {args.debug}")
    
    demo.launch(
        server_name=args.host,
        server_port=args.port,
        share=args.share,
        debug=args.debug
    )


if __name__ == "__main__":
    main()

