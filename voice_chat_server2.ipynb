{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Voice Chat Server v·ªõi Bank Model\n",
        "\n",
        "Notebook n√†y t·∫°o m·ªôt server realtime ƒë·ªÉ chat b·∫±ng gi·ªçng n√≥i v·ªõi model bank-model-2b.\n",
        "T√≠nh nƒÉng:\n",
        "- üéôÔ∏è Nh·∫≠n gi·ªçng n√≥i (Speech-to-Text) - ch·ªâ voice, kh√¥ng c√≥ ·∫£nh\n",
        "- ü§ñ X·ª≠ l√Ω v·ªõi model Qwen3-VL-2B\n",
        "- üîä Tr·∫£ l·ªùi b·∫±ng gi·ªçng n√≥i (Text-to-Speech) realtime\n",
        "- üìä Hi·ªÉn th·ªã t√†i nguy√™n h·ªá th·ªëng (CPU, RAM, GPU) realtime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra v√† upgrade transformers n·∫øu c·∫ßn (cho Qwen3VL)\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Ki·ªÉm tra version transformers hi·ªán t·∫°i\n",
        "try:\n",
        "    import transformers\n",
        "    print(f\"üì¶ Transformers version hi·ªán t·∫°i: {transformers.__version__}\")\n",
        "    \n",
        "    # Ki·ªÉm tra xem c√≥ Qwen3VL kh√¥ng\n",
        "    try:\n",
        "        from transformers import Qwen3VLForConditionalGeneration\n",
        "        print(\"‚úÖ Qwen3VLForConditionalGeneration ƒë√£ c√≥ s·∫µn trong transformers\")\n",
        "    except ImportError:\n",
        "        print(\"‚ùå Qwen3VLForConditionalGeneration ch∆∞a c√≥ trong transformers\")\n",
        "        print(\"üí° ƒêang upgrade transformers l√™n b·∫£n m·ªõi nh·∫•t...\")\n",
        "        # Th·ª≠ c√†i b·∫£n m·ªõi nh·∫•t tr∆∞·ªõc\n",
        "        result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\", \"--upgrade\", \"--force-reinstall\"], check=False)\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ ƒê√£ upgrade transformers l√™n b·∫£n m·ªõi nh·∫•t\")\n",
        "            print(\"üí° Vui l√≤ng restart runtime v√† ch·∫°y l·∫°i cell n√†y\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Kh√¥ng th·ªÉ c√†i b·∫£n m·ªõi nh·∫•t, th·ª≠ version t∆∞∆°ng th√≠ch v·ªõi coqui-tts...\")\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers>=4.52.1,<4.56\", \"--upgrade\"], check=False)\n",
        "            print(\"‚úÖ ƒê√£ upgrade transformers (version t∆∞∆°ng th√≠ch v·ªõi coqui-tts)\")\n",
        "            print(\"üí° L∆∞u √Ω: Version transformers ƒë∆∞·ª£c gi·ªõi h·∫°n <4.56 ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi coqui-tts\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  Transformers ch∆∞a ƒë∆∞·ª£c c√†i, ƒëang c√†i ƒë·∫∑t b·∫£n m·ªõi nh·∫•t...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\", \"--upgrade\", \"--force-reinstall\"], check=False)\n",
        "    print(\"‚úÖ ƒê√£ c√†i transformers (b·∫£n m·ªõi nh·∫•t)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. C√†i ƒë·∫∑t Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "# Theo t√†i li·ªáu Qwen: https://qwen.readthedocs.io/en/latest/\n",
        "# C√†i ƒë·∫∑t transformers b·∫£n m·ªõi nh·∫•t\n",
        "# L∆∞u √Ω: B·∫£n m·ªõi nh·∫•t c√≥ th·ªÉ kh√¥ng t∆∞∆°ng th√≠ch v·ªõi coqui-tts\n",
        "# N·∫øu g·∫∑p l·ªói v·ªõi coqui-tts, c√≥ th·ªÉ c·∫ßn downgrade v·ªÅ >=4.52.1,<4.56\n",
        "print(\"ƒêang c√†i ƒë·∫∑t transformers b·∫£n m·ªõi nh·∫•t...\")\n",
        "%pip install -q transformers --upgrade --force-reinstall\n",
        "print(\"‚úÖ Transformers ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (b·∫£n m·ªõi nh·∫•t)\")\n",
        "print(\"üí° N·∫øu g·∫∑p l·ªói v·ªõi coqui-tts, c√≥ th·ªÉ c·∫ßn: %pip install transformers>=4.52.1,<4.56\")\n",
        "\n",
        "# C√†i torch v·ªõi version t∆∞∆°ng th√≠ch v·ªõi coqui-tts\n",
        "# coqui-tts y√™u c·∫ßu torch<2.9,>=2.1\n",
        "print(\"ƒêang c√†i ƒë·∫∑t torch (t∆∞∆°ng th√≠ch v·ªõi coqui-tts/XTTS)...\")\n",
        "%pip install -q \"torch>=2.1,<2.9\" \"torchaudio>=2.1.0,<2.9\"\n",
        "print(\"‚úÖ Torch ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (version t∆∞∆°ng th√≠ch v·ªõi coqui-tts)\")\n",
        "%pip install -q gradio\n",
        "%pip install -q openai-whisper\n",
        "%pip install -q pydub\n",
        "%pip install -q soundfile\n",
        "%pip install -q accelerate\n",
        "%pip install -q bitsandbytes  # Cho quantization (theo Qwen docs)\n",
        "%pip install -q psutil  # Cho monitoring t√†i nguy√™n\n",
        "\n",
        "# Flash Attention 2 (t√πy ch·ªçn, ƒë·ªÉ tƒÉng t·ªëc - theo Qwen docs)\n",
        "# Uncomment d√≤ng d∆∞·ªõi n·∫øu mu·ªën c√†i Flash Attention 2 (c·∫ßn compile, c√≥ th·ªÉ m·∫•t th·ªùi gian)\n",
        "# %pip install -q flash-attn --no-build-isolation\n",
        "print(\"üí° Tip: C√≥ th·ªÉ c√†i flash-attn ƒë·ªÉ tƒÉng t·ªëc (t√πy ch·ªçn): pip install flash-attn --no-build-isolation\")\n",
        "\n",
        "# C√†i TTS (Coqui TTS) - th·ª≠ nhi·ªÅu c√°ch\n",
        "print(\"ƒêang c√†i TTS (Coqui TTS)...\")\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Option 1: Th·ª≠ c√†i TTS package tr·ª±c ti·∫øp t·ª´ PyPI\n",
        "print(\"Th·ª≠ c√†i TTS t·ª´ PyPI...\")\n",
        "result = subprocess.run(\n",
        "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"TTS\"], \n",
        "    capture_output=True, \n",
        "    text=True,\n",
        "    timeout=300\n",
        ")\n",
        "\n",
        "if result.returncode != 0:\n",
        "    print(\"‚ö†Ô∏è  Kh√¥ng th·ªÉ c√†i TTS t·ª´ PyPI, th·ª≠ c√†i t·ª´ source...\")\n",
        "    # Option 2: C√†i t·ª´ source (GitHub)\n",
        "    result2 = subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \n",
        "         \"git+https://github.com/coqui-ai/TTS.git\"], \n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=600\n",
        "    )\n",
        "    if result2.returncode != 0:\n",
        "        print(\"‚ö†Ô∏è  Kh√¥ng th·ªÉ c√†i TTS t·ª´ source\")\n",
        "\n",
        "# Ki·ªÉm tra xem TTS ƒë√£ ƒë∆∞·ª£c c√†i ch∆∞a\n",
        "try:\n",
        "    from TTS.api import TTS\n",
        "    print(\"‚úÖ TTS ƒë√£ ƒë∆∞·ª£c c√†i th√†nh c√¥ng!\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è  TTS kh√¥ng kh·∫£ d·ª•ng: {e}\")\n",
        "    print(\"S·∫Ω th·ª≠ c√†i l·∫°i khi load model...\")\n",
        "    \n",
        "    # Fallback: c√†i pyttsx3 n·∫øu TTS kh√¥ng ƒë∆∞·ª£c\n",
        "    print(\"ƒêang c√†i pyttsx3 l√†m fallback...\")\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pyttsx3\"], \n",
        "            capture_output=True,\n",
        "            timeout=60\n",
        "        )\n",
        "        print(\"‚úÖ pyttsx3 ƒë√£ ƒë∆∞·ª£c c√†i (c√≥ th·ªÉ c·∫ßn eSpeak)\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è  Kh√¥ng th·ªÉ c√†i pyttsx3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import whisper\n",
        "\n",
        "# Import transformers v·ªõi error handling\n",
        "try:\n",
        "    from transformers import Qwen3VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig, TextIteratorStreamer\n",
        "    print(\"‚úÖ ƒê√£ import Qwen3VLForConditionalGeneration th√†nh c√¥ng\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå L·ªói import Qwen3VLForConditionalGeneration: {e}\")\n",
        "    print(\"üí° Vui l√≤ng:\")\n",
        "    print(\"   1. Ch·∫°y cell tr∆∞·ªõc ƒë√≥ ƒë·ªÉ upgrade transformers\")\n",
        "    print(\"   2. Restart runtime (Runtime > Restart runtime)\")\n",
        "    print(\"   3. Ch·∫°y l·∫°i cell n√†y\")\n",
        "    print(\"üí° Ho·∫∑c ch·∫°y l·ªánh: !pip install transformers --upgrade --force-reinstall\")\n",
        "    raise\n",
        "\n",
        "import gradio as gr\n",
        "import io\n",
        "import base64\n",
        "import os\n",
        "from typing import Optional, Tuple\n",
        "import numpy as np\n",
        "import tempfile\n",
        "from functools import lru_cache\n",
        "import psutil  # Cho monitoring t√†i nguy√™n\n",
        "import time\n",
        "import re  # Cho text processing\n",
        "from threading import Thread  # Cho streaming\n",
        "\n",
        "# Import TTS v·ªõi fallback\n",
        "TTS_AVAILABLE = False\n",
        "pyttsx3_available = False\n",
        "\n",
        "try:\n",
        "    from TTS.api import TTS\n",
        "    TTS_AVAILABLE = True\n",
        "    print(\"Coqui TTS ƒë√£ s·∫µn s√†ng!\")\n",
        "except ImportError:\n",
        "    print(\"Coqui TTS kh√¥ng kh·∫£ d·ª•ng, s·∫Ω th·ª≠ pyttsx3...\")\n",
        "    try:\n",
        "        import pyttsx3\n",
        "        pyttsx3_available = True\n",
        "        print(\"pyttsx3 ƒë√£ s·∫µn s√†ng!\")\n",
        "    except:\n",
        "        print(\"C·∫£ TTS v√† pyttsx3 ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng. C·∫ßn c√†i m·ªôt trong hai.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. H√†m Monitoring T√†i nguy√™n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_system_resources():\n",
        "    \"\"\"\n",
        "    L·∫•y th√¥ng tin t√†i nguy√™n h·ªá th·ªëng (CPU, RAM, GPU)\n",
        "    \"\"\"\n",
        "    # CPU\n",
        "    cpu_percent = psutil.cpu_percent(interval=0.1)\n",
        "    cpu_count = psutil.cpu_count()\n",
        "    \n",
        "    # RAM\n",
        "    memory = psutil.virtual_memory()\n",
        "    ram_total_gb = memory.total / (1024**3)\n",
        "    ram_used_gb = memory.used / (1024**3)\n",
        "    ram_available_gb = memory.available / (1024**3)\n",
        "    ram_percent = memory.percent\n",
        "    \n",
        "    # GPU (n·∫øu c√≥)\n",
        "    gpu_info = \"\"\n",
        "    gpu_memory_used = 0\n",
        "    gpu_memory_total = 0\n",
        "    gpu_memory_percent = 0\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        gpu_count = torch.cuda.device_count()\n",
        "        gpu_info = f\"GPU: {torch.cuda.get_device_name(0)}\"\n",
        "        gpu_memory_used = torch.cuda.memory_allocated(0) / (1024**3)\n",
        "        gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "        gpu_memory_percent = (gpu_memory_used / gpu_memory_total) * 100\n",
        "    else:\n",
        "        gpu_info = \"GPU: Kh√¥ng c√≥\"\n",
        "    \n",
        "    return {\n",
        "        \"cpu_percent\": cpu_percent,\n",
        "        \"cpu_count\": cpu_count,\n",
        "        \"ram_total_gb\": ram_total_gb,\n",
        "        \"ram_used_gb\": ram_used_gb,\n",
        "        \"ram_available_gb\": ram_available_gb,\n",
        "        \"ram_percent\": ram_percent,\n",
        "        \"gpu_info\": gpu_info,\n",
        "        \"gpu_memory_used\": gpu_memory_used,\n",
        "        \"gpu_memory_total\": gpu_memory_total,\n",
        "        \"gpu_memory_percent\": gpu_memory_percent,\n",
        "    }\n",
        "\n",
        "def format_resources_info():\n",
        "    \"\"\"\n",
        "    Format th√¥ng tin t√†i nguy√™n th√†nh string ƒë·ªÉ hi·ªÉn th·ªã\n",
        "    \"\"\"\n",
        "    res = get_system_resources()\n",
        "    \n",
        "    info = f\"\"\"\n",
        "### üìä T√†i nguy√™n h·ªá th·ªëng:\n",
        "\n",
        "**CPU:**\n",
        "- S·ª≠ d·ª•ng: {res['cpu_percent']:.1f}% / {res['cpu_count']} cores\n",
        "- C√≤n l·∫°i: {100 - res['cpu_percent']:.1f}%\n",
        "\n",
        "**RAM:**\n",
        "- T·ªïng: {res['ram_total_gb']:.2f} GB\n",
        "- ƒê√£ d√πng: {res['ram_used_gb']:.2f} GB ({res['ram_percent']:.1f}%)\n",
        "- C√≤n l·∫°i: {res['ram_available_gb']:.2f} GB ({100 - res['ram_percent']:.1f}%)\n",
        "\n",
        "**{res['gpu_info']}**\n",
        "\"\"\"\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        info += f\"\"\"\n",
        "- T·ªïng: {res['gpu_memory_total']:.2f} GB\n",
        "- ƒê√£ d√πng: {res['gpu_memory_used']:.2f} GB ({res['gpu_memory_percent']:.1f}%)\n",
        "- C√≤n l·∫°i: {res['gpu_memory_total'] - res['gpu_memory_used']:.2f} GB ({100 - res['gpu_memory_percent']:.1f}%)\n",
        "\"\"\"\n",
        "    \n",
        "    return info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Authentication v·ªõi Hugging Face (n·∫øu c·∫ßn)\n",
        "\n",
        "N·∫øu model l√† private ho·∫∑c b·∫°n mu·ªën t·∫£i nhanh h∆°n, c√≥ th·ªÉ ƒëƒÉng nh·∫≠p Hugging Face.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment v√† th√™m token c·ªßa b·∫°n n·∫øu c·∫ßn (cho model private ho·∫∑c t·∫£i nhanh h∆°n)\n",
        "# from huggingface_hub import login\n",
        "# login(token=\"YOUR_HF_TOKEN_HERE\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Model v√† Speech Recognition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. C·∫•u h√¨nh Full C√¥ng Su·∫•t cho Model (T√πy ch·ªçn)\n",
        "\n",
        "N·∫øu GPU ƒë·ªß l·ªõn (>=20GB), c√≥ th·ªÉ t·∫Øt quantization v√† t·ªëi ∆∞u ƒë·ªÉ ch·∫°y full c√¥ng su·∫•t.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra v√† t·ªëi ∆∞u model cho full c√¥ng su·∫•t\n",
        "# Ch·ªâ ch·∫°y sau khi model ƒë√£ ƒë∆∞·ª£c load\n",
        "if 'model' in globals() and model is not None:\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"GPU: {gpu_name}\")\n",
        "        print(f\"GPU Memory: {gpu_memory_gb:.2f} GB\")\n",
        "        \n",
        "        # Ki·ªÉm tra xem model c√≥ ƒëang d√πng quantization kh√¥ng\n",
        "        is_quantized = hasattr(model, 'hf_quantizer') or hasattr(model, 'quantization_config')\n",
        "        \n",
        "        # N·∫øu GPU >= 20GB v√† model ƒëang d√πng quantization, c√≥ th·ªÉ reload kh√¥ng quantization\n",
        "        if gpu_memory_gb >= 20 and is_quantized:\n",
        "            print(\"\\n‚ö†Ô∏è  Model ƒëang d√πng quantization nh∆∞ng GPU ƒë·ªß l·ªõn.\")\n",
        "            print(\"   C√≥ th·ªÉ reload model kh√¥ng quantization ƒë·ªÉ full c√¥ng su·∫•t (t√πy ch·ªçn)\")\n",
        "            print(\"   Uncomment code b√™n d∆∞·ªõi ƒë·ªÉ reload:\")\n",
        "            print(\"\"\"\n",
        "# Uncomment ƒë·ªÉ reload model full precision:\n",
        "# model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
        "#     model_name,\n",
        "#     device_map=\"auto\",\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     trust_remote_code=True,\n",
        "#     attn_implementation=\"flash_attention_2\",\n",
        "# )\n",
        "            \"\"\")\n",
        "        elif gpu_memory_gb >= 20:\n",
        "            print(\"‚úÖ Model ƒë√£ ƒë∆∞·ª£c load v·ªõi full precision - s·∫µn s√†ng full c√¥ng su·∫•t!\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  GPU nh·ªè, model ƒëang d√πng quantization (ph√π h·ª£p)\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Kh√¥ng c√≥ GPU, model ƒëang d√πng quantization (ph√π h·ª£p)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Model ch∆∞a ƒë∆∞·ª£c load. Ch·∫°y cell load model tr∆∞·ªõc.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Whisper model cho Speech-to-Text (d√πng tiny ƒë·ªÉ nhanh h∆°n)\n",
        "# T·ªëi ∆∞u: Force load l√™n GPU n·∫øu c√≥\n",
        "print(\"ƒêang load Whisper model (tiny - nhanh nh·∫•t)...\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU c√≥ s·∫µn: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
        "    whisper_model = whisper.load_model(\"tiny\", device=\"cuda\")  # Force l√™n GPU\n",
        "    print(\"‚úÖ Whisper model ƒë√£ load tr√™n GPU!\")\n",
        "else:\n",
        "    whisper_model = whisper.load_model(\"tiny\", device=\"cpu\")\n",
        "    print(\"‚ö†Ô∏è  Whisper model ƒë√£ load tr√™n CPU (kh√¥ng c√≥ GPU)\")\n",
        "\n",
        "# Load Bank Model t·ª´ Hugging Face\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ƒêang t·∫£i Bank Model t·ª´ Hugging Face...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "model_name = \"hainguyen306201/bank-model-2b\"\n",
        "print(f\"Model: {model_name}\")\n",
        "\n",
        "# Ki·ªÉm tra xem model c√≥ t·ªìn t·∫°i kh√¥ng\n",
        "try:\n",
        "    from huggingface_hub import model_info\n",
        "    info = model_info(model_name)\n",
        "    print(f\"‚úÖ Model t√¨m th·∫•y tr√™n Hugging Face!\")\n",
        "    print(f\"   - Model ID: {info.modelId}\")\n",
        "    print(f\"   - Files: {len(info.siblings)} files\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Kh√¥ng th·ªÉ ki·ªÉm tra model info: {e}\")\n",
        "    print(\"   Ti·∫øp t·ª•c t·∫£i model...\")\n",
        "\n",
        "# C·∫•u h√¨nh quantization 4-bit ƒë·ªÉ ti·∫øt ki·ªám memory v√† tƒÉng t·ªëc\n",
        "# Theo t√†i li·ªáu Qwen: https://qwen.readthedocs.io/en/latest/\n",
        "print(\"\\nƒêang c·∫•u h√¨nh quantization (4-bit)...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",  # NF4 quantization - chu·∫©n Qwen\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,  # bfloat16 cho t√≠nh to√°n\n",
        "    bnb_4bit_use_double_quant=True,  # Double quantization ƒë·ªÉ ti·∫øt ki·ªám h∆°n\n",
        ")\n",
        "\n",
        "# Load model v·ªõi c√°c t√πy ch·ªçn ƒë·ªÉ ƒë·∫£m b·∫£o t·∫£i ƒë√∫ng v√† ch·∫°y tr√™n GPU\n",
        "print(\"\\nƒêang t·∫£i model (c√≥ th·ªÉ m·∫•t v√†i ph√∫t l·∫ßn ƒë·∫ßu)...\")\n",
        "\n",
        "# Ki·ªÉm tra GPU v√† quy·∫øt ƒë·ªãnh quantization\n",
        "if torch.cuda.is_available():\n",
        "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   GPU Memory: {gpu_memory_gb:.2f} GB\")\n",
        "    \n",
        "    # N·∫øu GPU >= 20GB, c√≥ th·ªÉ load kh√¥ng quantization ƒë·ªÉ nhanh h∆°n\n",
        "    if gpu_memory_gb >= 20:\n",
        "        print(\"‚úÖ GPU ƒë·ªß l·ªõn, s·∫Ω load model kh√¥ng quantization (full precision) ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô!\")\n",
        "        use_quantization = False\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  GPU nh·ªè, s·∫Ω d√πng quantization 4-bit ƒë·ªÉ ti·∫øt ki·ªám memory\")\n",
        "        use_quantization = True\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Kh√¥ng c√≥ GPU, s·∫Ω d√πng quantization 4-bit\")\n",
        "    use_quantization = True\n",
        "\n",
        "try:\n",
        "    if use_quantization:\n",
        "        # D√πng quantization cho GPU nh·ªè ho·∫∑c CPU\n",
        "        model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",  # T·ª± ƒë·ªông ƒë·∫∑t l√™n GPU n·∫øu c√≥\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            trust_remote_code=True,\n",
        "            resume_download=True,\n",
        "            force_download=False,\n",
        "        )\n",
        "    else:\n",
        "        # Load full precision tr√™n GPU l·ªõn (nhanh h∆°n)\n",
        "        # Theo t√†i li·ªáu Qwen: s·ª≠ d·ª•ng attn_implementation=\"flash_attention_2\" n·∫øu h·ªó tr·ª£\n",
        "        # Ki·ªÉm tra xem c√≥ flash-attn kh√¥ng\n",
        "        try:\n",
        "            import flash_attn\n",
        "            use_flash_attention = True\n",
        "            print(\"‚úÖ Flash Attention 2 ƒë∆∞·ª£c ph√°t hi·ªán, s·∫Ω s·ª≠ d·ª•ng ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô\")\n",
        "        except ImportError:\n",
        "            use_flash_attention = False\n",
        "            print(\"‚ö†Ô∏è  Flash Attention 2 ch∆∞a ƒë∆∞·ª£c c√†i, s·ª≠ d·ª•ng attention m·∫∑c ƒë·ªãnh\")\n",
        "            print(\"   C√≥ th·ªÉ c√†i: pip install flash-attn (t√πy ch·ªçn, ƒë·ªÉ tƒÉng t·ªëc)\")\n",
        "        \n",
        "        model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",  # T·ª± ƒë·ªông ƒë·∫∑t l√™n GPU\n",
        "            torch_dtype=torch.bfloat16,  # bfloat16 cho hi·ªáu su·∫•t t·ªët\n",
        "            trust_remote_code=True,  # C·∫ßn thi·∫øt cho Qwen models\n",
        "            resume_download=True,\n",
        "            force_download=False,\n",
        "            attn_implementation=\"flash_attention_2\" if use_flash_attention else \"sdpa\",  # Flash Attention 2 ho·∫∑c SDPA\n",
        "        )\n",
        "    \n",
        "    # ƒê·∫£m b·∫£o model tr√™n GPU v√† t·ªëi ∆∞u\n",
        "    if torch.cuda.is_available():\n",
        "        model_device = next(model.parameters()).device\n",
        "        print(f\"‚úÖ Model ƒë√£ ƒë∆∞·ª£c t·∫£i v√† load th√†nh c√¥ng!\")\n",
        "        print(f\"   Model device: {model_device}\")\n",
        "        if model_device.type == \"cuda\":\n",
        "            print(f\"   ‚úÖ Model ƒëang ch·∫°y tr√™n GPU: {torch.cuda.get_device_name(model_device.index)}\")\n",
        "            # T·ªëi ∆∞u: Compile model n·∫øu PyTorch >= 2.0 (tƒÉng t·ªëc ~20-30%)\n",
        "            try:\n",
        "                if hasattr(torch, 'compile') and torch.__version__ >= \"2.0.0\":\n",
        "                    print(\"   üîß ƒêang compile model ƒë·ªÉ tƒÉng t·ªëc (PyTorch 2.0+)...\")\n",
        "                    model = torch.compile(model, mode=\"reduce-overhead\")\n",
        "                    print(\"   ‚úÖ Model ƒë√£ ƒë∆∞·ª£c compile!\")\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ö†Ô∏è  Kh√¥ng th·ªÉ compile model: {e} (kh√¥ng ·∫£nh h∆∞·ªüng ch·ª©c nƒÉng)\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  Model ƒëang ch·∫°y tr√™n {model_device.type}, ƒëang chuy·ªÉn l√™n GPU...\")\n",
        "            model = model.to(\"cuda\")\n",
        "            # Th·ª≠ compile sau khi move\n",
        "            try:\n",
        "                if hasattr(torch, 'compile') and torch.__version__ >= \"2.0.0\":\n",
        "                    model = torch.compile(model, mode=\"reduce-overhead\")\n",
        "                    print(\"   ‚úÖ Model ƒë√£ ƒë∆∞·ª£c compile!\")\n",
        "            except:\n",
        "                pass\n",
        "    else:\n",
        "        print(\"‚úÖ Model ƒë√£ ƒë∆∞·ª£c t·∫£i v√† load th√†nh c√¥ng tr√™n CPU!\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå L·ªói khi t·∫£i model: {e}\")\n",
        "    print(\"\\nTh·ª≠ t·∫£i l·∫°i v·ªõi force_download=True...\")\n",
        "    if use_quantization:\n",
        "        model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            trust_remote_code=True,\n",
        "            force_download=True,\n",
        "        )\n",
        "    else:\n",
        "        # Retry v·ªõi flash attention n·∫øu c√≥\n",
        "        try:\n",
        "            import flash_attn\n",
        "            use_flash_attention = True\n",
        "        except ImportError:\n",
        "            use_flash_attention = False\n",
        "        \n",
        "        model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            trust_remote_code=True,\n",
        "            force_download=True,\n",
        "            attn_implementation=\"flash_attention_2\" if use_flash_attention else \"sdpa\",\n",
        "        )\n",
        "    print(\"‚úÖ Model ƒë√£ ƒë∆∞·ª£c t·∫£i l·∫°i th√†nh c√¥ng!\")\n",
        "    \n",
        "    # ƒê·∫£m b·∫£o model tr√™n GPU\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.to(\"cuda\")\n",
        "        print(f\"‚úÖ Model ƒë√£ ƒë∆∞·ª£c chuy·ªÉn l√™n GPU!\")\n",
        "\n",
        "# Load processor - theo chu·∫©n Qwen\n",
        "print(\"\\nƒêang t·∫£i processor...\")\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    model_name, \n",
        "    trust_remote_code=True,  # C·∫ßn thi·∫øt cho Qwen models\n",
        "    resume_download=True,\n",
        ")\n",
        "print(\"‚úÖ Processor ƒë√£ load!\")\n",
        "\n",
        "# Ki·ªÉm tra processor c√≥ ƒë√∫ng kh√¥ng\n",
        "if not hasattr(processor, 'apply_chat_template'):\n",
        "    print(\"‚ö†Ô∏è  Processor kh√¥ng c√≥ apply_chat_template, c√≥ th·ªÉ c√≥ v·∫•n ƒë·ªÅ\")\n",
        "else:\n",
        "    print(\"‚úÖ Processor c√≥ apply_chat_template - OK\")\n",
        "\n",
        "# Ki·ªÉm tra tokenizer\n",
        "if hasattr(processor, 'tokenizer'):\n",
        "    print(f\"‚úÖ Tokenizer: {type(processor.tokenizer).__name__}\")\n",
        "    if hasattr(processor.tokenizer, 'eos_token_id') and processor.tokenizer.eos_token_id:\n",
        "        print(f\"   EOS token ID: {processor.tokenizer.eos_token_id}\")\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è  EOS token ID kh√¥ng ƒë∆∞·ª£c set, s·∫Ω d√πng pad_token_id\")\n",
        "\n",
        "# Ki·ªÉm tra v√† hi·ªÉn th·ªã th√¥ng tin GPU\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìä Th√¥ng tin GPU v√† Model:\")\n",
        "print(\"=\"*50)\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
        "    \n",
        "    # Ki·ªÉm tra model ƒë√£ ƒë∆∞·ª£c load ch∆∞a\n",
        "    try:\n",
        "        if 'model' in globals() and model is not None:\n",
        "            model_device = next(model.parameters()).device\n",
        "            if model_device.type == \"cuda\":\n",
        "                print(f\"   ‚úÖ Model ƒëang ch·∫°y tr√™n GPU: {torch.cuda.get_device_name(model_device.index)}\")\n",
        "                print(f\"   GPU Memory ƒë√£ d√πng: {torch.cuda.memory_allocated(0) / (1024**3):.2f} GB\")\n",
        "            else:\n",
        "                print(f\"   ‚ö†Ô∏è  Model ƒëang ch·∫°y tr√™n {model_device.type}\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  Model ch∆∞a ƒë∆∞·ª£c load\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è  Kh√¥ng th·ªÉ ki·ªÉm tra model device: {e}\")\n",
        "    \n",
        "    # Ki·ªÉm tra Whisper model\n",
        "    try:\n",
        "        if 'whisper_model' in globals() and whisper_model is not None:\n",
        "            if hasattr(whisper_model, 'encoder') and hasattr(whisper_model.encoder, 'parameters'):\n",
        "                whisper_device = next(whisper_model.encoder.parameters()).device\n",
        "                if whisper_device.type == \"cuda\":\n",
        "                    print(f\"   ‚úÖ Whisper ƒëang ch·∫°y tr√™n GPU\")\n",
        "                else:\n",
        "                    print(f\"   ‚ö†Ô∏è  Whisper ƒëang ch·∫°y tr√™n CPU\")\n",
        "            else:\n",
        "                # N·∫øu kh√¥ng ki·ªÉm tra ƒë∆∞·ª£c, gi·∫£ ƒë·ªãnh ƒë√£ load ƒë√∫ng device\n",
        "                print(f\"   ‚úÖ Whisper ƒë√£ ƒë∆∞·ª£c load v·ªõi device ph√π h·ª£p\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  Whisper model ch∆∞a ƒë∆∞·ª£c load\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ÑπÔ∏è  Kh√¥ng th·ªÉ ki·ªÉm tra Whisper device: {e}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Kh√¥ng c√≥ GPU, t·∫•t c·∫£ ƒëang ch·∫°y tr√™n CPU\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ Bank Model ƒë√£ s·∫µn s√†ng ƒë·ªÉ s·ª≠ d·ª•ng!\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# T·ªëi ∆∞u: Warmup model ƒë·ªÉ tƒÉng t·ªëc l·∫ßn ƒë·∫ßu generate\n",
        "print(\"\\nüî• ƒêang warmup model (l·∫ßn ƒë·∫ßu c√≥ th·ªÉ ch·∫≠m)...\")\n",
        "try:\n",
        "    # Warmup v·ªõi m·ªôt c√¢u ng·∫Øn\n",
        "    warmup_messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello\"}]}]\n",
        "    warmup_inputs = processor.apply_chat_template(\n",
        "        warmup_messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    if torch.cuda.is_available():\n",
        "        warmup_inputs = {k: v.to(\"cuda\") if isinstance(v, torch.Tensor) else v for k, v in warmup_inputs.items()}\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        _ = model.generate(\n",
        "            **warmup_inputs,\n",
        "            max_new_tokens=10,\n",
        "            do_sample=False,\n",
        "            use_cache=True,\n",
        "        )\n",
        "    print(\"‚úÖ Model ƒë√£ ƒë∆∞·ª£c warmup - s·∫µn s√†ng generate nhanh!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Warmup kh√¥ng th√†nh c√¥ng (kh√¥ng ·∫£nh h∆∞·ªüng ch·ª©c nƒÉng): {e}\")\n",
        "\n",
        "# Load TTS offline (Coqui TTS ho·∫∑c pyttsx3) - Force GPU n·∫øu c√≥\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ƒêang load TTS model (offline)...\")\n",
        "print(\"=\"*50)\n",
        "tts = None\n",
        "tts_type = None\n",
        "\n",
        "# ƒê·∫£m b·∫£o TTS ch·∫°y tr√™n GPU n·∫øu c√≥\n",
        "use_gpu_tts = torch.cuda.is_available()\n",
        "if use_gpu_tts:\n",
        "    print(f\"üöÄ TTS s·∫Ω ch·∫°y tr√™n GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  TTS s·∫Ω ch·∫°y tr√™n CPU (kh√¥ng c√≥ GPU)\")\n",
        "\n",
        "# Ki·ªÉm tra l·∫°i TTS_AVAILABLE v√† th·ª≠ c√†i l·∫°i n·∫øu c·∫ßn\n",
        "if not TTS_AVAILABLE:\n",
        "    print(\"‚ö†Ô∏è  Coqui TTS ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t, ƒëang th·ª≠ c√†i l·∫°i...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    try:\n",
        "        # Th·ª≠ c√†i TTS t·ª´ PyPI\n",
        "        result = subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"TTS\"],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=300\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ ƒê√£ c√†i TTS t·ª´ PyPI, ƒëang import l·∫°i...\")\n",
        "            try:\n",
        "                from TTS.api import TTS\n",
        "                TTS_AVAILABLE = True\n",
        "                print(\"‚úÖ Coqui TTS ƒë√£ s·∫µn s√†ng!\")\n",
        "            except ImportError:\n",
        "                print(\"‚ö†Ô∏è  V·∫´n kh√¥ng th·ªÉ import TTS sau khi c√†i\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Kh√¥ng th·ªÉ c√†i TTS t·ª´ PyPI, th·ª≠ c√†i t·ª´ source...\")\n",
        "            result2 = subprocess.run(\n",
        "                [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"git+https://github.com/coqui-ai/TTS.git\"],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=600\n",
        "            )\n",
        "            if result2.returncode == 0:\n",
        "                print(\"‚úÖ ƒê√£ c√†i TTS t·ª´ source, ƒëang import l·∫°i...\")\n",
        "                try:\n",
        "                    from TTS.api import TTS\n",
        "                    TTS_AVAILABLE = True\n",
        "                    print(\"‚úÖ Coqui TTS ƒë√£ s·∫µn s√†ng!\")\n",
        "                except ImportError:\n",
        "                    print(\"‚ö†Ô∏è  V·∫´n kh√¥ng th·ªÉ import TTS sau khi c√†i t·ª´ source\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  L·ªói khi c√†i TTS: {e}\")\n",
        "\n",
        "if TTS_AVAILABLE:\n",
        "    try:\n",
        "        # Th·ª≠ load XTTS v2 (ch·∫•t l∆∞·ª£ng cao, h·ªó tr·ª£ nhi·ªÅu ng√¥n ng·ªØ)\n",
        "        tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=use_gpu_tts)\n",
        "        tts_type = \"coqui_xtts\"\n",
        "        if use_gpu_tts:\n",
        "            print(\"‚úÖ TTS model (XTTS v2) ƒë√£ load tr√™n GPU!\")\n",
        "        else:\n",
        "            print(\"‚úÖ TTS model (XTTS v2) ƒë√£ load tr√™n CPU!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Kh√¥ng th·ªÉ load XTTS v2: {e}\")\n",
        "        # Fallback: d√πng model ti·∫øng Vi·ªát nh·∫π h∆°n\n",
        "        print(\"ƒêang load TTS model ti·∫øng Vi·ªát (nh·∫π h∆°n)...\")\n",
        "        try:\n",
        "            tts = TTS(model_name=\"tts_models/vi/vietnamese\", gpu=use_gpu_tts)\n",
        "            tts_type = \"coqui_vi\"\n",
        "            if use_gpu_tts:\n",
        "                print(\"‚úÖ TTS model ti·∫øng Vi·ªát ƒë√£ load tr√™n GPU!\")\n",
        "            else:\n",
        "                print(\"‚úÖ TTS model ti·∫øng Vi·ªát ƒë√£ load tr√™n CPU!\")\n",
        "        except Exception as e2:\n",
        "            print(f\"Kh√¥ng th·ªÉ load TTS ti·∫øng Vi·ªát: {e2}\")\n",
        "            # Fallback cu·ªëi: d√πng model ƒë∆°n gi·∫£n nh·∫•t\n",
        "            print(\"ƒêang load TTS model ƒë∆°n gi·∫£n...\")\n",
        "            try:\n",
        "                tts = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC\", gpu=use_gpu_tts)\n",
        "                tts_type = \"coqui_en\"\n",
        "                if use_gpu_tts:\n",
        "                    print(\"‚úÖ TTS model ƒë∆°n gi·∫£n ƒë√£ load tr√™n GPU!\")\n",
        "                else:\n",
        "                    print(\"‚úÖ TTS model ƒë∆°n gi·∫£n ƒë√£ load tr√™n CPU!\")\n",
        "            except Exception as e3:\n",
        "                print(f\"Kh√¥ng th·ªÉ load Coqui TTS: {e3}\")\n",
        "                tts = None\n",
        "\n",
        "# Fallback: d√πng pyttsx3 n·∫øu TTS kh√¥ng kh·∫£ d·ª•ng\n",
        "if tts is None and pyttsx3_available:\n",
        "    try:\n",
        "        tts = pyttsx3.init()\n",
        "        tts_type = \"pyttsx3\"\n",
        "        # C·∫•u h√¨nh gi·ªçng n√≥i\n",
        "        voices = tts.getProperty('voices')\n",
        "        # T√¨m gi·ªçng ti·∫øng Vi·ªát n·∫øu c√≥\n",
        "        for voice in voices:\n",
        "            if 'vietnamese' in voice.name.lower() or 'vi' in voice.id.lower():\n",
        "                tts.setProperty('voice', voice.id)\n",
        "                break\n",
        "        print(\"TTS model (pyttsx3) ƒë√£ load!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Kh√¥ng th·ªÉ load pyttsx3: {e}\")\n",
        "        tts = None\n",
        "\n",
        "if tts is None:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"‚ö†Ô∏è  C·∫¢NH B√ÅO: Kh√¥ng c√≥ TTS n√†o kh·∫£ d·ª•ng!\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"C√≥ th·ªÉ th·ª≠ c√°c c√°ch sau:\")\n",
        "    print(\"1. Ch·∫°y l·∫°i cell n√†y ƒë·ªÉ th·ª≠ c√†i TTS t·ª± ƒë·ªông\")\n",
        "    print(\"2. C√†i th·ªß c√¥ng: %pip install TTS\")\n",
        "    print(\"3. Ho·∫∑c c√†i t·ª´ source: %pip install git+https://github.com/coqui-ai/TTS.git\")\n",
        "    print(\"4. Ho·∫∑c c√†i pyttsx3: %pip install pyttsx3\")\n",
        "    print(\"\\nL∆∞u √Ω: Server v·∫´n ho·∫°t ƒë·ªông nh∆∞ng s·∫Ω kh√¥ng c√≥ TTS (kh√¥ng ph√°t √¢m)\")\n",
        "    print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. H√†m x·ª≠ l√Ω Speech-to-Text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def speech_to_text(audio_path: Optional[str]) -> str:\n",
        "    \"\"\"\n",
        "    Chuy·ªÉn ƒë·ªïi file audio th√†nh text s·ª≠ d·ª•ng Whisper (t·ªëi ∆∞u t·ªëc ƒë·ªô)\n",
        "    T·ªëi ∆∞u: Greedy decoding, kh√¥ng beam search, t·ªëi ∆∞u I/O, ch·∫°y tr√™n GPU\n",
        "    \"\"\"\n",
        "    # Ki·ªÉm tra whisper_model ƒë√£ ƒë∆∞·ª£c load ch∆∞a\n",
        "    if 'whisper_model' not in globals() or whisper_model is None:\n",
        "        return \"[L·ªói: Whisper model ch∆∞a ƒë∆∞·ª£c load]\"\n",
        "    \n",
        "    if audio_path is None:\n",
        "        return \"\"\n",
        "    \n",
        "    try:\n",
        "        # ƒê·∫£m b·∫£o Whisper model tr√™n GPU n·∫øu c√≥\n",
        "        # Whisper model c√≥ th·ªÉ kh√¥ng c√≥ .parameters(), ki·ªÉm tra device kh√°c c√°ch\n",
        "        if torch.cuda.is_available():\n",
        "            # Whisper t·ª± ƒë·ªông s·ª≠ d·ª•ng GPU n·∫øu load v·ªõi device=\"cuda\"\n",
        "            # Kh√¥ng c·∫ßn move th·ªß c√¥ng v√¨ ƒë√£ load v·ªõi device=\"cuda\" ·ªü tr√™n\n",
        "            pass\n",
        "        \n",
        "        # T·ªëi ∆∞u: S·ª≠ d·ª•ng greedy decoding (beam_size=1) ƒë·ªÉ nhanh nh·∫•t\n",
        "        # N·∫øu c·∫ßn ch·∫•t l∆∞·ª£ng h∆°n, c√≥ th·ªÉ tƒÉng beam_size\n",
        "        use_fp16 = torch.cuda.is_available()  # S·ª≠ d·ª•ng fp16 n·∫øu c√≥ GPU\n",
        "        result = whisper_model.transcribe(\n",
        "            audio_path,\n",
        "            language=\"vi\",\n",
        "            fp16=use_fp16,  # S·ª≠ d·ª•ng fp16 n·∫øu c√≥ GPU\n",
        "            verbose=False,  # T·∫Øt verbose ƒë·ªÉ nhanh h∆°n\n",
        "            condition_on_previous_text=False,  # T·∫Øt ƒë·ªÉ nhanh h∆°n\n",
        "            initial_prompt=\"ƒê√¢y l√† m·ªôt cu·ªôc tr√≤ chuy·ªán b·∫±ng ti·∫øng Vi·ªát.\",  # Prompt ng·∫Øn\n",
        "            compression_ratio_threshold=2.4,\n",
        "            logprob_threshold=-1.0,\n",
        "            no_speech_threshold=0.6,\n",
        "            beam_size=1,  # Greedy decoding - nhanh nh·∫•t\n",
        "            best_of=1,  # Kh√¥ng c·∫ßn best_of khi beam_size=1\n",
        "            temperature=0.0,  # Deterministic - nhanh h∆°n\n",
        "        )\n",
        "        text = result[\"text\"].strip()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"L·ªói trong speech-to-text: {e}\")\n",
        "        # Fallback nhanh\n",
        "        try:\n",
        "            result = whisper_model.transcribe(\n",
        "                audio_path,\n",
        "                language=\"vi\",\n",
        "                fp16=torch.cuda.is_available(),\n",
        "                verbose=False,\n",
        "                beam_size=1,\n",
        "            )\n",
        "            return result[\"text\"].strip()\n",
        "        except:\n",
        "            return \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. H√†m x·ª≠ l√Ω v·ªõi Bank Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_with_model_stream(text: str):\n",
        "    \"\"\"\n",
        "    X·ª≠ l√Ω text v·ªõi Bank Model v·ªõi streaming (yield t·ª´ng ph·∫ßn text)\n",
        "    S·ª≠ d·ª•ng TextIteratorStreamer ƒë·ªÉ stream response\n",
        "    \"\"\"\n",
        "    from transformers import TextIteratorStreamer\n",
        "    from threading import Thread\n",
        "    import queue\n",
        "    \n",
        "    # S·ª≠ d·ª•ng global ƒë·ªÉ truy c·∫≠p model v√† processor\n",
        "    global model, processor\n",
        "    \n",
        "    # Ki·ªÉm tra model v√† processor ƒë√£ ƒë∆∞·ª£c load ch∆∞a\n",
        "    if 'model' not in globals() or model is None:\n",
        "        yield \"Xin l·ªói, model ch∆∞a ƒë∆∞·ª£c load. Vui l√≤ng ch·∫°y cell load model tr∆∞·ªõc.\"\n",
        "        return\n",
        "    \n",
        "    if 'processor' not in globals() or processor is None:\n",
        "        yield \"Xin l·ªói, processor ch∆∞a ƒë∆∞·ª£c load. Vui l√≤ng ch·∫°y cell load model tr∆∞·ªõc.\"\n",
        "        return\n",
        "    \n",
        "    if not text.strip():\n",
        "        yield \"Xin l·ªói, t√¥i kh√¥ng hi·ªÉu. B·∫°n c√≥ th·ªÉ vi·∫øt l·∫°i ƒë∆∞·ª£c kh√¥ng?\"\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        # T·∫°o messages cho model - theo chu·∫©n Qwen3\n",
        "        # Th√™m system message ƒë·ªÉ ƒë·ªãnh nghƒ©a AI l√† t∆∞ v·∫•n ng√¢n h√†ng\n",
        "        # Theo t√†i li·ªáu: https://qwen.readthedocs.io/en/latest/getting_started/quickstart.html\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"B·∫°n l√† m·ªôt chuy√™n gia t∆∞ v·∫•n ng√¢n h√†ng chuy√™n nghi·ªáp v√† th√¢n thi·ªán. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi m·ªçi c√¢u h·ªèi li√™n quan ƒë·∫øn ng√¢n h√†ng, bao g·ªìm: c√°c s·∫£n ph·∫©m v√† d·ªãch v·ª• ng√¢n h√†ng, t√†i kho·∫£n, th·∫ª, vay v·ªën, ti·∫øt ki·ªám, ƒë·∫ßu t∆∞, b·∫£o hi·ªÉm ng√¢n h√†ng, quy tr√¨nh giao d·ªãch, ph√≠ d·ªãch v·ª•, v√† c√°c v·∫•n ƒë·ªÅ t√†i ch√≠nh c√° nh√¢n. H√£y tr·∫£ l·ªùi m·ªôt c√°ch r√µ r√†ng, ch√≠nh x√°c v√† h·ªØu √≠ch. N·∫øu kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ th√¥ng tin, h√£y ƒë·ªÅ xu·∫•t kh√°ch h√†ng li√™n h·ªá tr·ª±c ti·∫øp v·ªõi ng√¢n h√†ng ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n chi ti·∫øt h∆°n.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": text\n",
        "            }\n",
        "        ]\n",
        "        \n",
        "        # Process v√† generate - theo ƒë√∫ng chu·∫©n Qwen3 t·ª´ t√†i li·ªáu\n",
        "        # Theo t√†i li·ªáu: d√πng apply_chat_template v·ªõi tokenize=False, sau ƒë√≥ tokenize ri√™ng\n",
        "        tokenizer = processor.tokenizer\n",
        "        \n",
        "        # B∆∞·ªõc 1: Apply chat template ƒë·ªÉ l·∫•y text (kh√¥ng tokenize)\n",
        "        text_formatted = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "        \n",
        "        # B∆∞·ªõc 2: Tokenize text ƒë√£ format\n",
        "        model_inputs = tokenizer([text_formatted], return_tensors=\"pt\")\n",
        "        \n",
        "        # ƒê·ªïi t√™n ƒë·ªÉ d√πng trong code c≈©\n",
        "        inputs = model_inputs\n",
        "        \n",
        "        # ƒê·∫£m b·∫£o input_ids l√† tensor 2D\n",
        "        if len(inputs[\"input_ids\"].shape) != 2:\n",
        "            inputs[\"input_ids\"] = inputs[\"input_ids\"].unsqueeze(0)\n",
        "        \n",
        "        # ƒê·∫£m b·∫£o model v√† inputs tr√™n GPU\n",
        "        # L∆∞u model device tr∆∞·ªõc khi c√≥ th·ªÉ thay ƒë·ªïi\n",
        "        if torch.cuda.is_available():\n",
        "            try:\n",
        "                model_device = next(model.parameters()).device\n",
        "                if model_device.type != \"cuda\":\n",
        "                    # S·ª≠ d·ª•ng global ƒë·ªÉ c·∫≠p nh·∫≠t model\n",
        "                    model = model.to(\"cuda\")\n",
        "                    model_device = next(model.parameters()).device\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  L·ªói khi ki·ªÉm tra model device: {e}\")\n",
        "                model_device = torch.device(\"cuda\")\n",
        "                try:\n",
        "                    model = model.to(\"cuda\")\n",
        "                except:\n",
        "                    model_device = torch.device(\"cpu\")\n",
        "            \n",
        "            device = model_device\n",
        "            # Move inputs l√™n ƒë√∫ng device\n",
        "            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
        "        else:\n",
        "            device = torch.device(\"cpu\")\n",
        "            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
        "        \n",
        "        # T·∫°o streamer v·ªõi timeout v√† error handling t·ªët h∆°n\n",
        "        # Theo t√†i li·ªáu Qwen: TextIteratorStreamer v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u\n",
        "        # ƒê·∫£m b·∫£o tokenizer c√≥ eos_token_id\n",
        "        tokenizer = processor.tokenizer\n",
        "        eos_token_id = getattr(tokenizer, 'eos_token_id', None) or getattr(tokenizer, 'pad_token_id', None)\n",
        "        \n",
        "        streamer = TextIteratorStreamer(\n",
        "            tokenizer,\n",
        "            skip_prompt=True,  # B·ªè prompt trong output\n",
        "            skip_special_tokens=True,  # B·ªè special tokens\n",
        "            timeout=60.0,  # Timeout 60 gi√¢y\n",
        "            clean_up_tokenization_spaces=True,  # Clean up spaces (t·ªët cho ti·∫øng Vi·ªát)\n",
        "        )\n",
        "        \n",
        "        # Generate trong thread ri√™ng\n",
        "        # Theo t√†i li·ªáu Qwen: c·∫•u h√¨nh generation chu·∫©n v√† t·ªëi ∆∞u\n",
        "        model.eval()\n",
        "        \n",
        "        # Clear cache tr∆∞·ªõc khi generate\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        # Generation config theo khuy·∫øn ngh·ªã Qwen3-Instruct (non-thinking mode)\n",
        "        # Theo t√†i li·ªáu: temperature=0.7, top_p=0.8, top_k=20, min_p=0\n",
        "        generation_kwargs = {\n",
        "            **inputs,\n",
        "            \"max_new_tokens\": 16384,  # Theo khuy·∫øn ngh·ªã Qwen3-Instruct\n",
        "            \"temperature\": 0.7,  # Khuy·∫øn ngh·ªã cho Qwen3-Instruct\n",
        "            \"top_p\": 0.8,  # Khuy·∫øn ngh·ªã cho Qwen3-Instruct\n",
        "            \"top_k\": 20,  # Khuy·∫øn ngh·ªã cho Qwen3-Instruct\n",
        "            \"do_sample\": True,  # B·∫≠t sampling\n",
        "            \"pad_token_id\": eos_token_id,  # Pad token\n",
        "            \"eos_token_id\": eos_token_id,  # EOS token (quan tr·ªçng)\n",
        "            \"use_cache\": True,  # KV cache - quan tr·ªçng cho t·ªëc ƒë·ªô v√† memory\n",
        "            \"repetition_penalty\": 1.1,  # Tr√°nh l·∫∑p l·∫°i\n",
        "            \"streamer\": streamer,  # Streamer cho streaming output\n",
        "        }\n",
        "        \n",
        "        # Generate trong thread v·ªõi error handling\n",
        "        generation_error = [None]\n",
        "        \n",
        "        def generate_with_error_handling():\n",
        "            try:\n",
        "                with torch.inference_mode():  # T·ªëi ∆∞u inference\n",
        "                    model.generate(**generation_kwargs)\n",
        "            except Exception as e:\n",
        "                generation_error[0] = e\n",
        "                print(f\"‚ùå L·ªói trong generation thread: {e}\")\n",
        "        \n",
        "        thread = Thread(target=generate_with_error_handling)\n",
        "        thread.daemon = True  # Daemon thread ƒë·ªÉ kh√¥ng block khi exit\n",
        "        thread.start()\n",
        "        \n",
        "        # Stream output v·ªõi timeout handling\n",
        "        generated_text = \"\"\n",
        "        try:\n",
        "            for new_text in streamer:\n",
        "                if generation_error[0]:\n",
        "                    raise generation_error[0]\n",
        "                generated_text += new_text\n",
        "                yield generated_text\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  L·ªói trong streaming: {e}\")\n",
        "            if generated_text:\n",
        "                yield generated_text  # Yield ph·∫ßn ƒë√£ generate ƒë∆∞·ª£c\n",
        "            else:\n",
        "                yield f\"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi generate: {str(e)}\"\n",
        "        finally:\n",
        "            # ƒê·∫£m b·∫£o thread k·∫øt th√∫c\n",
        "            thread.join(timeout=5.0)\n",
        "            if thread.is_alive():\n",
        "                print(\"‚ö†Ô∏è  Generation thread v·∫´n ch·∫°y sau timeout\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"L·ªói trong model processing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        yield f\"Xin l·ªói, c√≥ l·ªói x·∫£y ra: {str(e)}\"\n",
        "\n",
        "def process_with_model(text: str) -> str:\n",
        "    \"\"\"\n",
        "    X·ª≠ l√Ω text v·ªõi Bank Model (t·ªëi ∆∞u t·ªëc ƒë·ªô) - ch·ªâ text, kh√¥ng c√≥ ·∫£nh\n",
        "    \"\"\"\n",
        "    # Ki·ªÉm tra model v√† processor ƒë√£ ƒë∆∞·ª£c load ch∆∞a\n",
        "    if 'model' not in globals() or model is None:\n",
        "        return \"Xin l·ªói, model ch∆∞a ƒë∆∞·ª£c load. Vui l√≤ng ch·∫°y cell load model tr∆∞·ªõc.\"\n",
        "    \n",
        "    if 'processor' not in globals() or processor is None:\n",
        "        return \"Xin l·ªói, processor ch∆∞a ƒë∆∞·ª£c load. Vui l√≤ng ch·∫°y cell load model tr∆∞·ªõc.\"\n",
        "    \n",
        "    if not text.strip():\n",
        "        return \"Xin l·ªói, t√¥i kh√¥ng nghe r√µ. B·∫°n c√≥ th·ªÉ n√≥i l·∫°i ƒë∆∞·ª£c kh√¥ng?\"\n",
        "    \n",
        "    try:\n",
        "        # T·∫°o messages cho model - theo chu·∫©n Qwen3\n",
        "        # Th√™m system message ƒë·ªÉ ƒë·ªãnh nghƒ©a AI l√† t∆∞ v·∫•n ng√¢n h√†ng\n",
        "        # Theo t√†i li·ªáu: https://qwen.readthedocs.io/en/latest/getting_started/quickstart.html\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"B·∫°n l√† m·ªôt chuy√™n gia t∆∞ v·∫•n ng√¢n h√†ng chuy√™n nghi·ªáp v√† th√¢n thi·ªán. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi m·ªçi c√¢u h·ªèi li√™n quan ƒë·∫øn ng√¢n h√†ng, bao g·ªìm: c√°c s·∫£n ph·∫©m v√† d·ªãch v·ª• ng√¢n h√†ng, t√†i kho·∫£n, th·∫ª, vay v·ªën, ti·∫øt ki·ªám, ƒë·∫ßu t∆∞, b·∫£o hi·ªÉm ng√¢n h√†ng, quy tr√¨nh giao d·ªãch, ph√≠ d·ªãch v·ª•, v√† c√°c v·∫•n ƒë·ªÅ t√†i ch√≠nh c√° nh√¢n. H√£y tr·∫£ l·ªùi m·ªôt c√°ch r√µ r√†ng, ch√≠nh x√°c v√† h·ªØu √≠ch. N·∫øu kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ th√¥ng tin, h√£y ƒë·ªÅ xu·∫•t kh√°ch h√†ng li√™n h·ªá tr·ª±c ti·∫øp v·ªõi ng√¢n h√†ng ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n chi ti·∫øt h∆°n.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": text\n",
        "            }\n",
        "        ]\n",
        "        \n",
        "        # Process v√† generate - theo ƒë√∫ng chu·∫©n Qwen3 t·ª´ t√†i li·ªáu\n",
        "        # Theo t√†i li·ªáu: d√πng apply_chat_template v·ªõi tokenize=False, sau ƒë√≥ tokenize ri√™ng\n",
        "        tokenizer = processor.tokenizer\n",
        "        \n",
        "        # B∆∞·ªõc 1: Apply chat template ƒë·ªÉ l·∫•y text (kh√¥ng tokenize)\n",
        "        text_formatted = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "        \n",
        "        # B∆∞·ªõc 2: Tokenize text ƒë√£ format\n",
        "        model_inputs = tokenizer([text_formatted], return_tensors=\"pt\")\n",
        "        \n",
        "        # ƒê·ªïi t√™n ƒë·ªÉ d√πng trong code c≈©\n",
        "        inputs = model_inputs\n",
        "        \n",
        "        # ƒê·∫£m b·∫£o input_ids l√† tensor 2D\n",
        "        if len(inputs[\"input_ids\"].shape) != 2:\n",
        "            inputs[\"input_ids\"] = inputs[\"input_ids\"].unsqueeze(0)\n",
        "        \n",
        "        # L·∫•y eos_token_id\n",
        "        tokenizer = processor.tokenizer\n",
        "        eos_token_id = getattr(tokenizer, 'eos_token_id', None) or getattr(tokenizer, 'pad_token_id', None)\n",
        "        if eos_token_id is None:\n",
        "            raise ValueError(\"Tokenizer ph·∫£i c√≥ eos_token_id ho·∫∑c pad_token_id\")\n",
        "        \n",
        "        # ƒê·∫£m b·∫£o model v√† inputs tr√™n GPU\n",
        "        if torch.cuda.is_available():\n",
        "            # ƒê·∫£m b·∫£o model tr√™n GPU\n",
        "            try:\n",
        "                model_device = next(model.parameters()).device\n",
        "                if model_device.type != \"cuda\":\n",
        "                    print(\"‚ö†Ô∏è  Model kh√¥ng tr√™n GPU, ƒëang chuy·ªÉn l√™n GPU...\")\n",
        "                    model = model.to(\"cuda\")\n",
        "                    model_device = next(model.parameters()).device\n",
        "                    print(f\"‚úÖ Model ƒë√£ chuy·ªÉn l√™n GPU: {torch.cuda.get_device_name(model_device.index)}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  L·ªói khi ki·ªÉm tra model device: {e}\")\n",
        "                # Th·ª≠ chuy·ªÉn model l√™n GPU\n",
        "                try:\n",
        "                    model = model.to(\"cuda\")\n",
        "                    model_device = torch.device(\"cuda\")\n",
        "                    print(\"‚úÖ Model ƒë√£ ƒë∆∞·ª£c chuy·ªÉn l√™n GPU\")\n",
        "                except:\n",
        "                    model_device = torch.device(\"cpu\")\n",
        "                    print(\"‚ö†Ô∏è  Kh√¥ng th·ªÉ chuy·ªÉn model l√™n GPU, s·ª≠ d·ª•ng CPU\")\n",
        "            \n",
        "            # ƒê·∫£m b·∫£o inputs tr√™n GPU\n",
        "            device = model_device\n",
        "            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
        "        else:\n",
        "            device = torch.device(\"cpu\")\n",
        "            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
        "            print(\"‚ö†Ô∏è  Kh√¥ng c√≥ GPU, model ƒëang ch·∫°y tr√™n CPU\")\n",
        "        \n",
        "        # T·ªëi ∆∞u: S·ª≠ d·ª•ng torch.inference_mode thay v√¨ no_grad (nhanh h∆°n)\n",
        "        # V√† ƒë·∫£m b·∫£o model ·ªü eval mode\n",
        "        model.eval()\n",
        "        with torch.inference_mode():  # Nhanh h∆°n torch.no_grad()\n",
        "            # Clear cache tr∆∞·ªõc khi generate ƒë·ªÉ tr√°nh memory leak\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            # Generation config theo chu·∫©n Qwen - t·ªëi ∆∞u h∆°n\n",
        "            # Theo t√†i li·ªáu: https://qwen.readthedocs.io/en/latest/\n",
        "            # Best practices t·ª´ Qwen docs:\n",
        "            # Generation config theo khuy·∫øn ngh·ªã Qwen3-Instruct (non-thinking mode)\n",
        "            # Theo t√†i li·ªáu: temperature=0.7, top_p=0.8, top_k=20, min_p=0\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=16384,  # Theo khuy·∫øn ngh·ªã Qwen3-Instruct\n",
        "                temperature=0.7,  # Khuy·∫øn ngh·ªã cho Qwen3-Instruct\n",
        "                top_p=0.8,  # Khuy·∫øn ngh·ªã cho Qwen3-Instruct\n",
        "                top_k=20,  # Khuy·∫øn ngh·ªã cho Qwen3-Instruct\n",
        "                do_sample=True,  # B·∫≠t sampling\n",
        "                pad_token_id=eos_token_id,  # Pad token\n",
        "                eos_token_id=eos_token_id,  # EOS token (quan tr·ªçng)\n",
        "                use_cache=True,  # KV cache - quan tr·ªçng cho t·ªëc ƒë·ªô v√† memory\n",
        "                repetition_penalty=1.1,  # Tr√°nh l·∫∑p l·∫°i\n",
        "                output_scores=False,  # Kh√¥ng c·∫ßn scores ƒë·ªÉ tƒÉng t·ªëc\n",
        "                return_dict_in_generate=False,  # Kh√¥ng c·∫ßn dict ƒë·ªÉ tƒÉng t·ªëc\n",
        "            )\n",
        "        \n",
        "        # Decode response - theo ƒë√∫ng chu·∫©n Qwen3 t·ª´ t√†i li·ªáu\n",
        "        # Theo t√†i li·ªáu: https://qwen.readthedocs.io/en/latest/getting_started/quickstart.html\n",
        "        # L·∫•y output_ids t·ª´ v·ªã tr√≠ sau input_length\n",
        "        output_ids = generated_ids[0][len(inputs.input_ids[0]):].tolist()\n",
        "        \n",
        "        # Decode v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u\n",
        "        output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "        \n",
        "        # Post-process: Lo·∫°i b·ªè c√°c k√Ω t·ª± th·ª´a v√† normalize\n",
        "        output_text = output_text.strip()\n",
        "        \n",
        "        # Lo·∫°i b·ªè c√°c k√Ω t·ª± control v√† whitespace th·ª´a\n",
        "        import re\n",
        "        output_text = re.sub(r'\\s+', ' ', output_text)  # Normalize whitespace\n",
        "        output_text = output_text.strip()\n",
        "        \n",
        "        # Ki·ªÉm tra v√† x·ª≠ l√Ω empty response\n",
        "        if not output_text:\n",
        "            return \"Xin l·ªói, t√¥i kh√¥ng th·ªÉ t·∫°o response. Vui l√≤ng th·ª≠ l·∫°i.\"\n",
        "        \n",
        "        return output_text\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"L·ªói trong model processing: {e}\")\n",
        "        return f\"Xin l·ªói, c√≥ l·ªói x·∫£y ra: {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. H√†m Text-to-Speech\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_to_speech(text: str, lang: str = \"vi\") -> str:\n",
        "    \"\"\"\n",
        "    Chuy·ªÉn ƒë·ªïi text th√†nh file audio s·ª≠ d·ª•ng TTS (offline, t·ªëi ∆∞u t·ªëc ƒë·ªô)\n",
        "    T·ªëi ∆∞u: T·ªëi ∆∞u I/O, gi·∫£m overhead\n",
        "    C√≥ nhi·ªÅu fallback: Coqui TTS -> pyttsx3 -> gTTS\n",
        "    \"\"\"\n",
        "    if not text.strip():\n",
        "        return None\n",
        "    \n",
        "    # Ki·ªÉm tra tts ƒë√£ ƒë∆∞·ª£c load ch∆∞a\n",
        "    tts_available = 'tts' in globals() and tts is not None\n",
        "    tts_type_available = 'tts_type' in globals() and tts_type is not None\n",
        "    \n",
        "    # N·∫øu TTS c≈© ch∆∞a ƒë∆∞·ª£c load, th·ª≠ load ho·∫∑c d√πng fallback\n",
        "    if not tts_available:\n",
        "        print(\"‚ö†Ô∏è  TTS c≈© ch∆∞a ƒë∆∞·ª£c load, th·ª≠ c√°c TTS kh√°c...\")\n",
        "        # Th·ª≠ gTTS tr∆∞·ªõc (nhanh, mi·ªÖn ph√≠, c·∫ßn internet)\n",
        "        try:\n",
        "            from gtts import gTTS\n",
        "            import tempfile\n",
        "            import os\n",
        "            \n",
        "            fd, audio_path = tempfile.mkstemp(suffix=\".mp3\", prefix=\"gtts_\")\n",
        "            os.close(fd)\n",
        "            \n",
        "            tts_gtts = gTTS(text=text, lang='vi', slow=False)\n",
        "            tts_gtts.save(audio_path)\n",
        "            \n",
        "            if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:\n",
        "                print(\"‚úÖ ƒê√£ d√πng gTTS (c·∫ßn internet)\")\n",
        "                return audio_path\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  gTTS kh√¥ng kh·∫£ d·ª•ng: {e}\")\n",
        "        \n",
        "        # N·∫øu gTTS c≈©ng kh√¥ng ƒë∆∞·ª£c, tr·∫£ v·ªÅ None\n",
        "        print(\"‚ùå Kh√¥ng c√≥ TTS n√†o kh·∫£ d·ª•ng!\")\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        # Gi·ªõi h·∫°n ƒë·ªô d√†i ƒë·ªÉ tr√°nh TTS qu√° ch·∫≠m (c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh)\n",
        "        max_chars = 500  # Gi·ªõi h·∫°n ƒë·ªÉ TTS kh√¥ng qu√° ch·∫≠m\n",
        "        if len(text) > max_chars:\n",
        "            text = text[:max_chars] + \"...\"\n",
        "        \n",
        "        # T·ªëi ∆∞u: S·ª≠ d·ª•ng tempfile v·ªõi mode binary ƒë·ªÉ nhanh h∆°n\n",
        "        import tempfile\n",
        "        import os\n",
        "        fd, audio_path = tempfile.mkstemp(suffix=\".wav\", prefix=\"tts_\")\n",
        "        os.close(fd)  # ƒê√≥ng file descriptor ngay, ch·ªâ c·∫ßn path\n",
        "        \n",
        "        try:\n",
        "            # S·ª≠ d·ª•ng TTS ƒë√£ ƒë∆∞·ª£c load\n",
        "            current_tts_type = tts_type if tts_type_available else None\n",
        "            \n",
        "            if current_tts_type == \"pyttsx3\":\n",
        "                # S·ª≠ d·ª•ng pyttsx3 (nhanh nh·∫•t)\n",
        "                tts.save_to_file(text, audio_path)\n",
        "                tts.runAndWait()\n",
        "            elif current_tts_type and current_tts_type.startswith(\"coqui\"):\n",
        "                # S·ª≠ d·ª•ng Coqui TTS v·ªõi settings t·ªëi ∆∞u t·ªëc ƒë·ªô\n",
        "                if current_tts_type == \"coqui_xtts\":\n",
        "                    # XTTS v2 - h·ªó tr·ª£ multilingual (t·ªëi ∆∞u t·ªëc ƒë·ªô)\n",
        "                    tts.tts_to_file(\n",
        "                        text=text,\n",
        "                        file_path=audio_path,\n",
        "                        language=lang,\n",
        "                        speaker_wav=None,\n",
        "                        speed=1.3,  # TƒÉng t·ªëc ƒë·ªô ph√°t ƒë·ªÉ nhanh h∆°n\n",
        "                    )\n",
        "                else:\n",
        "                    # Model kh√°c\n",
        "                    tts.tts_to_file(text=text, file_path=audio_path)\n",
        "            else:\n",
        "                # Fallback: th·ª≠ d√πng API c·ªßa TTS\n",
        "                if hasattr(tts, 'tts_to_file'):\n",
        "                    tts.tts_to_file(text=text, file_path=audio_path)\n",
        "                elif hasattr(tts, 'save_to_file'):\n",
        "                    tts.save_to_file(text, audio_path)\n",
        "                    if hasattr(tts, 'runAndWait'):\n",
        "                        tts.runAndWait()\n",
        "                else:\n",
        "                    print(\"‚ö†Ô∏è  TTS kh√¥ng h·ªó tr·ª£ c√°c method ph·ªï bi·∫øn, th·ª≠ gTTS...\")\n",
        "                    # Fallback v·ªÅ gTTS\n",
        "                    try:\n",
        "                        from gtts import gTTS\n",
        "                        tts_gtts = gTTS(text=text, lang='vi', slow=False)\n",
        "                        tts_gtts.save(audio_path)\n",
        "                        if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:\n",
        "                            print(\"‚úÖ ƒê√£ d√πng gTTS (c·∫ßn internet)\")\n",
        "                            return audio_path\n",
        "                    except:\n",
        "                        pass\n",
        "                    return None\n",
        "            \n",
        "            return audio_path\n",
        "        except Exception as e:\n",
        "            # Cleanup n·∫øu l·ªói\n",
        "            if os.path.exists(audio_path):\n",
        "                try:\n",
        "                    os.remove(audio_path)\n",
        "                except:\n",
        "                    pass\n",
        "            raise e\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"L·ªói trong text-to-speech: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. H√†m x·ª≠ l√Ω Voice Chat ch√≠nh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def voice_chat(audio_input, chat_history=None):\n",
        "    \"\"\"\n",
        "    H√†m ch√≠nh x·ª≠ l√Ω voice chat (t·ªëi ∆∞u full c√¥ng su·∫•t) - ch·ªâ voice, kh√¥ng c√≥ ·∫£nh\n",
        "    \"\"\"\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    if chat_history is None:\n",
        "        chat_history = []\n",
        "    \n",
        "    # 1. Speech-to-Text\n",
        "    if audio_input is None:\n",
        "        return chat_history, None, format_resources_info()\n",
        "    \n",
        "    user_text = speech_to_text(audio_input)\n",
        "    stt_time = time.time() - start_time\n",
        "    \n",
        "    if not user_text:\n",
        "        response_text = \"Xin l·ªói, t√¥i kh√¥ng nghe r√µ. B·∫°n c√≥ th·ªÉ n√≥i l·∫°i ƒë∆∞·ª£c kh√¥ng?\"\n",
        "        chat_history.append((\"[Kh√¥ng nghe r√µ]\", response_text))\n",
        "        audio_output = text_to_speech(response_text)\n",
        "        return chat_history, audio_output, format_resources_info()\n",
        "    \n",
        "    # 2. Process v·ªõi model (full c√¥ng su·∫•t - c√≥ th·ªÉ m·∫•t v√†i gi√¢y nh∆∞ng ch·∫•t l∆∞·ª£ng cao)\n",
        "    model_start = time.time()\n",
        "    response_text = process_with_model(user_text)\n",
        "    model_time = time.time() - model_start\n",
        "    \n",
        "    # 3. C·∫≠p nh·∫≠t chat history ngay l·∫≠p t·ª©c\n",
        "    chat_history.append((user_text, response_text))\n",
        "    \n",
        "    # 4. Text-to-Speech\n",
        "    tts_start = time.time()\n",
        "    audio_output = text_to_speech(response_text)\n",
        "    tts_time = time.time() - tts_start\n",
        "    \n",
        "    total_time = time.time() - start_time\n",
        "    \n",
        "    # 5. C·∫≠p nh·∫≠t th√¥ng tin t√†i nguy√™n\n",
        "    resources_info = format_resources_info()\n",
        "    \n",
        "    # Debug: In th·ªùi gian x·ª≠ l√Ω v√† th√¥ng tin\n",
        "    print(f\"‚ö° T·ªëc ƒë·ªô: STT={stt_time:.2f}s, Model={model_time:.2f}s, TTS={tts_time:.2f}s, Total={total_time:.2f}s\")\n",
        "    print(f\"üìù Response length: {len(response_text)} k√Ω t·ª±\")\n",
        "    \n",
        "    return chat_history, audio_output, resources_info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. T·∫°o Gradio Interface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T·∫°o Gradio interface - Text Chat + Voice Input v·ªõi Streaming (b·ªè TTS)\n",
        "with gr.Blocks(title=\"Chat v·ªõi Bank Model - Text & Voice Input\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üí¨ Chat v·ªõi Bank Model - Text & Voice Input\n",
        "    \n",
        "    Chat b·∫±ng text HO·∫∂C gi·ªçng n√≥i v·ªõi AI model, response ƒë∆∞·ª£c stream realtime!\n",
        "    - ‚úçÔ∏è Nh·∫≠p text v√† nh·∫•n Enter ho·∫∑c n√∫t G·ª≠i\n",
        "    - üéôÔ∏è Ho·∫∑c n√≥i v√†o microphone\n",
        "    - ‚ö° Response ƒë∆∞·ª£c stream t·ª´ng ph·∫ßn (kh√¥ng c·∫ßn ch·ªù)\n",
        "    \"\"\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            chatbot = gr.Chatbot(\n",
        "                label=\"üí¨ L·ªãch s·ª≠ chat\",\n",
        "                height=400,\n",
        "                show_label=True,\n",
        "                type=\"tuples\",\n",
        "                allow_tags=False\n",
        "            )\n",
        "        \n",
        "        with gr.Column(scale=1):\n",
        "            # Tab cho Text v√† Voice input\n",
        "            with gr.Tabs():\n",
        "                with gr.Tab(\"‚úçÔ∏è Text Input\"):\n",
        "                    text_input = gr.Textbox(\n",
        "                        label=\"Nh·∫≠p c√¢u h·ªèi\",\n",
        "                        placeholder=\"Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n ·ªü ƒë√¢y...\",\n",
        "                        lines=3,\n",
        "                        show_label=True\n",
        "                    )\n",
        "                    text_submit_btn = gr.Button(\"G·ª≠i Text\", variant=\"primary\", size=\"lg\")\n",
        "                \n",
        "                with gr.Tab(\"üéôÔ∏è Voice Input\"):\n",
        "                    audio_input = gr.Audio(\n",
        "                        sources=[\"microphone\"],\n",
        "                        type=\"filepath\",\n",
        "                        label=\"N√≥i v√†o ƒë√¢y\",\n",
        "                        show_label=True\n",
        "                    )\n",
        "                    audio_submit_btn = gr.Button(\"G·ª≠i Voice\", variant=\"primary\", size=\"lg\")\n",
        "            \n",
        "            clear_btn = gr.Button(\"X√≥a l·ªãch s·ª≠\", variant=\"secondary\")\n",
        "            \n",
        "            # Hi·ªÉn th·ªã t√†i nguy√™n h·ªá th·ªëng\n",
        "            resources_display = gr.Markdown(\n",
        "                value=format_resources_info(),\n",
        "                label=\"üìä T√†i nguy√™n h·ªá th·ªëng\"\n",
        "            )\n",
        "    \n",
        "    # H√†m x·ª≠ l√Ω text input v·ªõi streaming\n",
        "    def chat_text_stream(user_text, history):\n",
        "        if not user_text or not user_text.strip():\n",
        "            return history, \"\"  # Return empty string ƒë·ªÉ clear text input\n",
        "        \n",
        "        # Th√™m user message\n",
        "        if history is None:\n",
        "            history = []\n",
        "        history.append((user_text.strip(), None))\n",
        "        \n",
        "        # Stream response\n",
        "        response_text = \"\"\n",
        "        try:\n",
        "            for partial_response in process_with_model_stream(user_text.strip()):\n",
        "                response_text = partial_response\n",
        "                history[-1] = (user_text.strip(), response_text)\n",
        "                yield history, \"\"  # Clear text input sau m·ªói update\n",
        "        except Exception as e:\n",
        "            print(f\"L·ªói trong chat_stream: {e}\")\n",
        "            response_text = f\"Xin l·ªói, c√≥ l·ªói x·∫£y ra: {str(e)}\"\n",
        "            history[-1] = (user_text.strip(), response_text)\n",
        "        \n",
        "        # Final yield - clear text input\n",
        "        yield history, \"\"\n",
        "    \n",
        "    # H√†m x·ª≠ l√Ω voice input v·ªõi streaming\n",
        "    def chat_voice_stream(audio_input, history):\n",
        "        if audio_input is None:\n",
        "            return history\n",
        "        \n",
        "        # Speech-to-Text\n",
        "        if 'whisper_model' not in globals() or whisper_model is None:\n",
        "            error_msg = \"Whisper model ch∆∞a ƒë∆∞·ª£c load\"\n",
        "            if history is None:\n",
        "                history = []\n",
        "            history.append((\"[L·ªói]\", error_msg))\n",
        "            return history\n",
        "        \n",
        "        user_text = speech_to_text(audio_input)\n",
        "        \n",
        "        if not user_text or not user_text.strip():\n",
        "            error_msg = \"Xin l·ªói, t√¥i kh√¥ng nghe r√µ. B·∫°n c√≥ th·ªÉ n√≥i l·∫°i ƒë∆∞·ª£c kh√¥ng?\"\n",
        "            if history is None:\n",
        "                history = []\n",
        "            history.append((\"[Kh√¥ng nghe r√µ]\", error_msg))\n",
        "            return history\n",
        "        \n",
        "        # Th√™m user message\n",
        "        if history is None:\n",
        "            history = []\n",
        "        history.append((user_text.strip(), None))\n",
        "        \n",
        "        # Stream response\n",
        "        response_text = \"\"\n",
        "        try:\n",
        "            for partial_response in process_with_model_stream(user_text.strip()):\n",
        "                response_text = partial_response\n",
        "                history[-1] = (user_text.strip(), response_text)\n",
        "                yield history\n",
        "        except Exception as e:\n",
        "            print(f\"L·ªói trong chat_voice_stream: {e}\")\n",
        "            response_text = f\"Xin l·ªói, c√≥ l·ªói x·∫£y ra: {str(e)}\"\n",
        "            history[-1] = (user_text.strip(), response_text)\n",
        "            yield history\n",
        "    \n",
        "    # Event handlers\n",
        "    text_submit_btn.click(\n",
        "        fn=chat_text_stream,\n",
        "        inputs=[text_input, chatbot],\n",
        "        outputs=[chatbot, text_input],\n",
        "        show_progress=False\n",
        "    )\n",
        "    \n",
        "    text_input.submit(\n",
        "        fn=chat_text_stream,\n",
        "        inputs=[text_input, chatbot],\n",
        "        outputs=[chatbot, text_input],\n",
        "        show_progress=False\n",
        "    )\n",
        "    \n",
        "    audio_submit_btn.click(\n",
        "        fn=chat_voice_stream,\n",
        "        inputs=[audio_input, chatbot],\n",
        "        outputs=[chatbot],\n",
        "        show_progress=False\n",
        "    )\n",
        "    \n",
        "    audio_input.stop_recording(\n",
        "        fn=chat_voice_stream,\n",
        "        inputs=[audio_input, chatbot],\n",
        "        outputs=[chatbot],\n",
        "        show_progress=False\n",
        "    )\n",
        "    \n",
        "    clear_btn.click(\n",
        "        fn=lambda: [],\n",
        "        outputs=[chatbot]\n",
        "    )\n",
        "    \n",
        "    # Auto-refresh t√†i nguy√™n khi load page\n",
        "    demo.load(\n",
        "        fn=lambda: format_resources_info(),\n",
        "        inputs=None,\n",
        "        outputs=resources_display\n",
        "    )\n",
        "    \n",
        "    gr.Markdown(\"\"\"\n",
        "    ### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:\n",
        "    1. **Text Input**: \n",
        "       - ‚úçÔ∏è Nh·∫≠p c√¢u h·ªèi v√†o √¥ text\n",
        "       - ‚èé Nh·∫•n Enter ho·∫∑c n√∫t \"G·ª≠i Text\"\n",
        "    \n",
        "    2. **Voice Input**:\n",
        "       - üéôÔ∏è Nh·∫•n n√∫t microphone v√† b·∫Øt ƒë·∫ßu n√≥i\n",
        "       - ‚èπÔ∏è D·ª´ng recording ho·∫∑c nh·∫•n \"G·ª≠i Voice\"\n",
        "    \n",
        "    3. ‚ö° Xem response ƒë∆∞·ª£c stream t·ª´ng ph·∫ßn (kh√¥ng c·∫ßn ch·ªù)\n",
        "    4. üìä Xem t√†i nguy√™n h·ªá th·ªëng ·ªü b√™n ph·∫£i\n",
        "    \n",
        "    ### ‚ö° T√≠nh nƒÉng:\n",
        "    - Response ƒë∆∞·ª£c stream realtime, kh√¥ng c·∫ßn ch·ªù to√†n b·ªô\n",
        "    - H·ªó tr·ª£ c·∫£ text v√† voice input\n",
        "    - Model ch·∫°y tr√™n GPU ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô\n",
        "    - B·ªè TTS (AI kh√¥ng tr·∫£ l·ªùi b·∫±ng gi·ªçng n√≥i)\n",
        "    \n",
        "    ### L∆∞u √Ω:\n",
        "    - C√≥ th·ªÉ d√πng text HO·∫∂C voice ƒë·ªÉ input\n",
        "    - Response ch·ªâ hi·ªÉn th·ªã text (kh√¥ng c√≥ audio)\n",
        "    - Response ƒë∆∞·ª£c stream t·ª´ng token\n",
        "    \"\"\")\n",
        "\n",
        "print(\"‚úÖ Gradio interface ƒë√£ ƒë∆∞·ª£c t·∫°o v·ªõi Text & Voice Input + Streaming (b·ªè TTS)!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Realtime Voice Chat 1-1 (N√¢ng cao)\n",
        "\n",
        "T√≠nh nƒÉng realtime voice chat cho ph√©p b·∫°n n√≥i chuy·ªán tr·ª±c ti·∫øp v·ªõi AI nh∆∞ m·ªôt cu·ªôc g·ªçi ƒëi·ªán tho·∫°i.\n",
        "\n",
        "### Models ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t:\n",
        "\n",
        "**STT (Speech-to-Text) - Nh·∫≠n d·∫°ng gi·ªçng n√≥i:**\n",
        "1. **faster-whisper** ‚≠ê (Khuy·∫øn ngh·ªã)\n",
        "   - Nhanh h∆°n Whisper 4x\n",
        "   - H·ªó tr·ª£ streaming realtime\n",
        "   - Offline, kh√¥ng c·∫ßn API key\n",
        "   - Ch√≠nh x√°c cao\n",
        "\n",
        "2. **Whisper** (Hi·ªán t·∫°i)\n",
        "   - Ch√≠nh x√°c cao nh∆∞ng ch·∫≠m h∆°n\n",
        "   - C√≥ th·ªÉ d√πng v·ªõi streaming mode\n",
        "\n",
        "**TTS (Text-to-Speech) - T·ªïng h·ª£p gi·ªçng n√≥i:**\n",
        "1. **Edge TTS** ‚≠ê (Khuy·∫øn ngh·ªã cho realtime)\n",
        "   - Mi·ªÖn ph√≠, kh√¥ng c·∫ßn API key\n",
        "   - Realtime, ƒë·ªô tr·ªÖ th·∫•p\n",
        "   - H·ªó tr·ª£ ti·∫øng Vi·ªát t·ªët\n",
        "   - Ch·∫•t l∆∞·ª£ng cao\n",
        "\n",
        "2. **Piper TTS**\n",
        "   - Nh·∫π, nhanh, offline\n",
        "   - H·ªó tr·ª£ ti·∫øng Vi·ªát\n",
        "   - Ph√π h·ª£p cho realtime\n",
        "\n",
        "3. **Coqui XTTS v2** (ƒê√£ c√≥)\n",
        "   - Ch·∫•t l∆∞·ª£ng cao nh∆∞ng ch·∫≠m h∆°n\n",
        "   - Ph√π h·ª£p cho non-realtime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán cho Realtime Voice Chat\n",
        "print(\"üîß ƒêang c√†i ƒë·∫∑t c√°c th∆∞ vi·ªán cho Realtime Voice Chat...\")\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# faster-whisper - STT nhanh h∆°n, h·ªó tr·ª£ streaming\n",
        "print(\"\\n1. C√†i ƒë·∫∑t faster-whisper (STT nhanh h∆°n Whisper 4x)...\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"faster-whisper\"], check=False)\n",
        "\n",
        "# Edge TTS - TTS realtime mi·ªÖn ph√≠ t·ª´ Microsoft\n",
        "print(\"\\n2. C√†i ƒë·∫∑t Edge TTS (TTS realtime mi·ªÖn ph√≠)...\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"edge-tts\"], check=False)\n",
        "\n",
        "# nest_asyncio - ƒê·ªÉ tr√°nh l·ªói \"event loop is already running\"\n",
        "print(\"\\n3. C√†i ƒë·∫∑t nest-asyncio (ƒë·ªÉ x·ª≠ l√Ω async trong notebook)...\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"nest-asyncio\"], check=False)\n",
        "\n",
        "# gTTS - Google Text-to-Speech (fallback, c·∫ßn internet)\n",
        "print(\"\\n4. C√†i ƒë·∫∑t gTTS (Google TTS - fallback, c·∫ßn internet)...\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gtts\"], check=False)\n",
        "\n",
        "# Piper TTS - TTS nh·∫π, nhanh, offline (t√πy ch·ªçn)\n",
        "print(\"\\n5. C√†i ƒë·∫∑t Piper TTS (TTS nh·∫π, offline - t√πy ch·ªçn)...\")\n",
        "try:\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"piper-tts\"], check=False)\n",
        "    print(\"   ‚úÖ Piper TTS ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "except:\n",
        "    print(\"   ‚ö†Ô∏è  Kh√¥ng th·ªÉ c√†i Piper TTS (kh√¥ng b·∫Øt bu·ªôc)\")\n",
        "\n",
        "# Ki·ªÉm tra c√°c th∆∞ vi·ªán ƒë√£ c√†i\n",
        "print(\"\\n‚úÖ ƒêang ki·ªÉm tra c√°c th∆∞ vi·ªán...\")\n",
        "try:\n",
        "    import faster_whisper\n",
        "    print(\"   ‚úÖ faster-whisper: OK\")\n",
        "except ImportError:\n",
        "    print(\"   ‚ùå faster-whisper: Ch∆∞a c√†i\")\n",
        "\n",
        "try:\n",
        "    import edge_tts\n",
        "    print(\"   ‚úÖ edge-tts: OK\")\n",
        "except ImportError:\n",
        "    print(\"   ‚ùå edge-tts: Ch∆∞a c√†i\")\n",
        "\n",
        "try:\n",
        "    import nest_asyncio\n",
        "    print(\"   ‚úÖ nest-asyncio: OK\")\n",
        "except ImportError:\n",
        "    print(\"   ‚ùå nest-asyncio: Ch∆∞a c√†i\")\n",
        "\n",
        "try:\n",
        "    from gtts import gTTS\n",
        "    print(\"   ‚úÖ gtts: OK\")\n",
        "except ImportError:\n",
        "    print(\"   ‚ö†Ô∏è  gtts: Ch∆∞a c√†i (kh√¥ng b·∫Øt bu·ªôc)\")\n",
        "\n",
        "try:\n",
        "    import piper\n",
        "    print(\"   ‚úÖ piper-tts: OK\")\n",
        "except ImportError:\n",
        "    print(\"   ‚ö†Ô∏è  piper-tts: Ch∆∞a c√†i (kh√¥ng b·∫Øt bu·ªôc)\")\n",
        "\n",
        "print(\"\\n‚úÖ Ho√†n t·∫•t c√†i ƒë·∫∑t!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load v√† kh·ªüi t·∫°o c√°c model cho Realtime Voice Chat\n",
        "\n",
        "# 1. faster-whisper cho STT realtime\n",
        "print(\"üéôÔ∏è ƒêang load faster-whisper model (STT realtime)...\")\n",
        "faster_whisper_model = None\n",
        "\n",
        "try:\n",
        "    from faster_whisper import WhisperModel\n",
        "    \n",
        "    # Ch·ªçn model size: tiny, base, small, medium, large\n",
        "    # tiny: nhanh nh·∫•t (~39M), base: c√¢n b·∫±ng (~74M), small: ch√≠nh x√°c h∆°n (~244M)\n",
        "    # Cho realtime, khuy·∫øn ngh·ªã: tiny ho·∫∑c base\n",
        "    whisper_model_size = \"base\"  # C√≥ th·ªÉ ƒë·ªïi th√†nh \"tiny\" ƒë·ªÉ nhanh h∆°n ho·∫∑c \"small\" ƒë·ªÉ ch√≠nh x√°c h∆°n\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            faster_whisper_model = WhisperModel(\n",
        "                whisper_model_size,\n",
        "                device=\"cuda\",\n",
        "                compute_type=\"float16\"  # Nhanh h∆°n tr√™n GPU\n",
        "            )\n",
        "            print(f\"‚úÖ faster-whisper ƒë√£ load tr√™n GPU (model: {whisper_model_size})\")\n",
        "            print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Kh√¥ng th·ªÉ load tr√™n GPU: {e}, th·ª≠ CPU...\")\n",
        "            faster_whisper_model = WhisperModel(\n",
        "                whisper_model_size,\n",
        "                device=\"cpu\",\n",
        "                compute_type=\"int8\"  # T·ªëi ∆∞u cho CPU\n",
        "            )\n",
        "            print(f\"‚úÖ faster-whisper ƒë√£ load tr√™n CPU (model: {whisper_model_size})\")\n",
        "    else:\n",
        "        faster_whisper_model = WhisperModel(\n",
        "            whisper_model_size,\n",
        "            device=\"cpu\",\n",
        "            compute_type=\"int8\"  # T·ªëi ∆∞u cho CPU\n",
        "        )\n",
        "        print(f\"‚úÖ faster-whisper ƒë√£ load tr√™n CPU (model: {whisper_model_size})\")\n",
        "    \n",
        "    # Test model v·ªõi m·ªôt ƒëo·∫°n audio ng·∫Øn (n·∫øu c√≥)\n",
        "    print(\"   ‚ÑπÔ∏è  Model ƒë√£ s·∫µn s√†ng cho realtime STT\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  faster-whisper ch∆∞a ƒë∆∞·ª£c c√†i, s·∫Ω d√πng Whisper th√¥ng th∆∞·ªùng\")\n",
        "    print(\"   üí° Ch·∫°y cell c√†i ƒë·∫∑t dependencies tr∆∞·ªõc\")\n",
        "    faster_whisper_model = None\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  L·ªói khi load faster-whisper: {e}\")\n",
        "    print(\"   üí° S·∫Ω d√πng Whisper th√¥ng th∆∞·ªùng l√†m fallback\")\n",
        "    faster_whisper_model = None\n",
        "\n",
        "# 2. Edge TTS cho TTS realtime\n",
        "print(\"\\nüîä ƒêang kh·ªüi t·∫°o Edge TTS (TTS realtime)...\")\n",
        "edge_tts_available = False\n",
        "edge_tts_voice = None\n",
        "\n",
        "try:\n",
        "    import edge_tts\n",
        "    import asyncio\n",
        "    \n",
        "    # L·∫•y danh s√°ch gi·ªçng n√≥i ti·∫øng Vi·ªát\n",
        "    async def get_voices():\n",
        "        try:\n",
        "            voices = await edge_tts.list_voices()\n",
        "            vietnamese_voices = [v for v in voices if \"vi\" in v[\"Locale\"].lower()]\n",
        "            return vietnamese_voices\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è  L·ªói khi l·∫•y danh s√°ch gi·ªçng n√≥i: {e}\")\n",
        "            return []\n",
        "    \n",
        "    # Ch·ªçn gi·ªçng n√≥i ti·∫øng Vi·ªát\n",
        "    try:\n",
        "        # T·∫°o event loop m·ªõi n·∫øu ch∆∞a c√≥\n",
        "        try:\n",
        "            loop = asyncio.get_event_loop()\n",
        "        except RuntimeError:\n",
        "            loop = asyncio.new_event_loop()\n",
        "            asyncio.set_event_loop(loop)\n",
        "        \n",
        "        voices = loop.run_until_complete(get_voices())\n",
        "        \n",
        "        if voices:\n",
        "            # ∆Øu ti√™n ch·ªçn gi·ªçng n·ªØ (th∆∞·ªùng c√≥ t√™n ch·ª©a \"Female\" ho·∫∑c \"HoaiMy\")\n",
        "            vietnamese_voice = None\n",
        "            preferred_voices = [\"HoaiMy\", \"female\", \"n·ªØ\"]\n",
        "            \n",
        "            for preferred in preferred_voices:\n",
        "                for v in voices:\n",
        "                    if preferred.lower() in v[\"ShortName\"].lower():\n",
        "                        vietnamese_voice = v[\"ShortName\"]\n",
        "                        print(f\"   ‚úÖ T√¨m th·∫•y gi·ªçng ∆∞u ti√™n: {vietnamese_voice}\")\n",
        "                        break\n",
        "                if vietnamese_voice:\n",
        "                    break\n",
        "            \n",
        "            if not vietnamese_voice:\n",
        "                vietnamese_voice = voices[0][\"ShortName\"]\n",
        "                print(f\"   ‚úÖ S·ª≠ d·ª•ng gi·ªçng ƒë·∫ßu ti√™n: {vietnamese_voice}\")\n",
        "            \n",
        "            edge_tts_voice = vietnamese_voice\n",
        "            edge_tts_available = True\n",
        "            print(f\"‚úÖ Edge TTS ƒë√£ s·∫µn s√†ng (gi·ªçng: {edge_tts_voice})\")\n",
        "        else:\n",
        "            # Fallback: d√πng gi·ªçng m·∫∑c ƒë·ªãnh\n",
        "            edge_tts_voice = \"vi-VN-HoaiMyNeural\"  # Gi·ªçng n·ªØ ti·∫øng Vi·ªát\n",
        "            edge_tts_available = True\n",
        "            print(f\"‚úÖ Edge TTS ƒë√£ s·∫µn s√†ng (gi·ªçng m·∫∑c ƒë·ªãnh: {edge_tts_voice})\")\n",
        "            print(\"   ‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y danh s√°ch gi·ªçng, d√πng gi·ªçng m·∫∑c ƒë·ªãnh\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  L·ªói khi kh·ªüi t·∫°o Edge TTS: {e}\")\n",
        "        # Fallback: d√πng gi·ªçng m·∫∑c ƒë·ªãnh\n",
        "        edge_tts_voice = \"vi-VN-HoaiMyNeural\"\n",
        "        edge_tts_available = True\n",
        "        print(f\"   ‚úÖ S·ª≠ d·ª•ng gi·ªçng m·∫∑c ƒë·ªãnh: {edge_tts_voice}\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  edge-tts ch∆∞a ƒë∆∞·ª£c c√†i\")\n",
        "    print(\"   üí° Ch·∫°y cell c√†i ƒë·∫∑t dependencies tr∆∞·ªõc\")\n",
        "    edge_tts_available = False\n",
        "    edge_tts_voice = None\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  L·ªói khi import edge-tts: {e}\")\n",
        "    edge_tts_available = False\n",
        "    edge_tts_voice = None\n",
        "\n",
        "# T√≥m t·∫Øt tr·∫°ng th√°i\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä T√≥m t·∫Øt tr·∫°ng th√°i models:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"   STT (faster-whisper): {'‚úÖ S·∫µn s√†ng' if faster_whisper_model else '‚ùå Ch∆∞a s·∫µn s√†ng'}\")\n",
        "print(f\"   TTS (Edge TTS): {'‚úÖ S·∫µn s√†ng' if edge_tts_available else '‚ùå Ch∆∞a s·∫µn s√†ng'}\")\n",
        "if edge_tts_available and edge_tts_voice:\n",
        "    print(f\"   Gi·ªçng TTS: {edge_tts_voice}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if not faster_whisper_model:\n",
        "    print(\"\\n‚ö†Ô∏è  L∆∞u √Ω: faster-whisper ch∆∞a s·∫µn s√†ng, h·ªá th·ªëng s·∫Ω d√πng Whisper th√¥ng th∆∞·ªùng (ch·∫≠m h∆°n)\")\n",
        "if not edge_tts_available:\n",
        "    print(\"\\n‚ö†Ô∏è  L∆∞u √Ω: Edge TTS ch∆∞a s·∫µn s√†ng, h·ªá th·ªëng s·∫Ω d√πng TTS c≈© (ch·∫≠m h∆°n)\")\n",
        "\n",
        "print(\"\\n‚úÖ Ho√†n t·∫•t kh·ªüi t·∫°o models cho Realtime Voice Chat!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# H√†m x·ª≠ l√Ω Realtime STT v√† TTS\n",
        "\n",
        "def realtime_speech_to_text(audio_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Chuy·ªÉn ƒë·ªïi audio th√†nh text s·ª≠ d·ª•ng faster-whisper (nhanh h∆°n)\n",
        "    \"\"\"\n",
        "    if faster_whisper_model is not None:\n",
        "        try:\n",
        "            # faster-whisper nhanh h∆°n v√† h·ªó tr·ª£ streaming\n",
        "            segments, info = faster_whisper_model.transcribe(\n",
        "                audio_path,\n",
        "                language=\"vi\",\n",
        "                beam_size=1,  # Greedy decoding - nhanh nh·∫•t\n",
        "                vad_filter=True,  # Voice Activity Detection - lo·∫°i b·ªè kho·∫£ng l·∫∑ng\n",
        "                vad_parameters=dict(min_silence_duration_ms=500)\n",
        "            )\n",
        "            \n",
        "            # L·∫•y text t·ª´ segments\n",
        "            text = \" \".join([segment.text for segment in segments])\n",
        "            return text.strip()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  L·ªói faster-whisper: {e}, d√πng Whisper th√¥ng th∆∞·ªùng\")\n",
        "            # Fallback v·ªÅ Whisper th√¥ng th∆∞·ªùng\n",
        "            return speech_to_text(audio_path)\n",
        "    else:\n",
        "        # Fallback v·ªÅ Whisper th√¥ng th∆∞·ªùng\n",
        "        return speech_to_text(audio_path)\n",
        "\n",
        "async def realtime_text_to_speech_async(text: str, voice: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Chuy·ªÉn ƒë·ªïi text th√†nh audio s·ª≠ d·ª•ng Edge TTS (realtime, mi·ªÖn ph√≠)\n",
        "    \"\"\"\n",
        "    if not edge_tts_available:\n",
        "        # Fallback v·ªÅ TTS c≈©\n",
        "        return text_to_speech(text)\n",
        "    \n",
        "    try:\n",
        "        import edge_tts\n",
        "        import tempfile\n",
        "        import os\n",
        "        \n",
        "        # Ch·ªçn gi·ªçng n√≥i\n",
        "        if voice is None:\n",
        "            voice = edge_tts_voice\n",
        "        \n",
        "        # Ki·ªÉm tra voice c√≥ h·ª£p l·ªá kh√¥ng\n",
        "        if not voice:\n",
        "            raise ValueError(\"Voice kh√¥ng ƒë∆∞·ª£c set\")\n",
        "        \n",
        "        # T·∫°o file audio t·∫°m\n",
        "        fd, audio_path = tempfile.mkstemp(suffix=\".mp3\", prefix=\"edge_tts_\")\n",
        "        os.close(fd)\n",
        "        \n",
        "        # Generate audio v·ªõi Edge TTS - s·ª≠ d·ª•ng c√°ch g·ªçi ƒë√∫ng\n",
        "        # Edge TTS c·∫ßn text kh√¥ng r·ªóng v√† voice h·ª£p l·ªá\n",
        "        if not text or not text.strip():\n",
        "            raise ValueError(\"Text kh√¥ng ƒë∆∞·ª£c r·ªóng\")\n",
        "        \n",
        "        # T·∫°o communicate object v·ªõi text ƒë√£ clean\n",
        "        text_clean = text.strip()\n",
        "        \n",
        "        # Ki·ªÉm tra voice c√≥ trong danh s√°ch kh√¥ng (optional)\n",
        "        communicate = edge_tts.Communicate(text_clean, voice)\n",
        "        \n",
        "        # Th·ª≠ c√°ch 1: D√πng save() (ƒë∆°n gi·∫£n nh·∫•t)\n",
        "        try:\n",
        "            await communicate.save(audio_path)\n",
        "            # Ki·ªÉm tra file ƒë√£ ƒë∆∞·ª£c t·∫°o ch∆∞a\n",
        "            if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:\n",
        "                return audio_path\n",
        "            else:\n",
        "                raise ValueError(\"File audio r·ªóng sau khi save\")\n",
        "        except Exception as e1:\n",
        "            print(f\"   ‚ö†Ô∏è  L·ªói khi d√πng save(): {e1}\")\n",
        "            # Th·ª≠ c√°ch 2: D√πng stream() v√† ghi t·ª´ng chunk\n",
        "            try:\n",
        "                # X√≥a file c≈© n·∫øu c√≥\n",
        "                if os.path.exists(audio_path):\n",
        "                    os.remove(audio_path)\n",
        "                \n",
        "                # T·∫°o l·∫°i communicate object\n",
        "                communicate = edge_tts.Communicate(text_clean, voice)\n",
        "                \n",
        "                # Ghi t·ª´ng chunk\n",
        "                audio_data = b\"\"\n",
        "                async for chunk in communicate.stream():\n",
        "                    if chunk[\"type\"] == \"audio\":\n",
        "                        audio_data += chunk[\"data\"]\n",
        "                \n",
        "                # Ghi to√†n b·ªô data v√†o file\n",
        "                if len(audio_data) > 0:\n",
        "                    with open(audio_path, \"wb\") as f:\n",
        "                        f.write(audio_data)\n",
        "                    \n",
        "                    # Ki·ªÉm tra file\n",
        "                    if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:\n",
        "                        return audio_path\n",
        "                    else:\n",
        "                        raise ValueError(\"File audio r·ªóng sau khi stream\")\n",
        "                else:\n",
        "                    raise ValueError(\"Kh√¥ng nh·∫≠n ƒë∆∞·ª£c audio data\")\n",
        "            except Exception as e2:\n",
        "                print(f\"   ‚ö†Ô∏è  L·ªói khi d√πng stream(): {e2}\")\n",
        "                # Th·ª≠ l·∫°i v·ªõi voice m·∫∑c ƒë·ªãnh n·∫øu voice hi·ªán t·∫°i l·ªói\n",
        "                if voice != \"vi-VN-HoaiMyNeural\":\n",
        "                    print(f\"   üí° Th·ª≠ l·∫°i v·ªõi voice m·∫∑c ƒë·ªãnh...\")\n",
        "                    try:\n",
        "                        communicate_default = edge_tts.Communicate(text_clean, \"vi-VN-HoaiMyNeural\")\n",
        "                        await communicate_default.save(audio_path)\n",
        "                        if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:\n",
        "                            return audio_path\n",
        "                    except:\n",
        "                        pass\n",
        "                raise e2\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  L·ªói Edge TTS: {e}, th·ª≠ TTS kh√°c...\")\n",
        "        # Fallback v·ªÅ TTS c≈© ho·∫∑c gTTS\n",
        "        return text_to_speech(text)\n",
        "\n",
        "def realtime_text_to_speech(text: str, voice: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Wrapper ƒë·ªìng b·ªô cho realtime_text_to_speech_async v·ªõi nhi·ªÅu fallback\n",
        "    \"\"\"\n",
        "    import asyncio\n",
        "    import tempfile\n",
        "    import os\n",
        "    \n",
        "    # Th·ª≠ Edge TTS tr∆∞·ªõc\n",
        "    if edge_tts_available:\n",
        "        try:\n",
        "            # S·ª≠ d·ª•ng nest_asyncio ƒë·ªÉ tr√°nh l·ªói \"event loop is already running\"\n",
        "            try:\n",
        "                import nest_asyncio\n",
        "                nest_asyncio.apply()\n",
        "            except ImportError:\n",
        "                pass\n",
        "            \n",
        "            try:\n",
        "                loop = asyncio.get_event_loop()\n",
        "                if loop.is_running():\n",
        "                    # N·∫øu loop ƒëang ch·∫°y, d√πng run_coroutine_threadsafe\n",
        "                    import concurrent.futures\n",
        "                    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "                        future = executor.submit(\n",
        "                            lambda: asyncio.run(realtime_text_to_speech_async(text, voice))\n",
        "                        )\n",
        "                        return future.result(timeout=30)\n",
        "                else:\n",
        "                    return loop.run_until_complete(realtime_text_to_speech_async(text, voice))\n",
        "            except RuntimeError:\n",
        "                # T·∫°o loop m·ªõi\n",
        "                loop = asyncio.new_event_loop()\n",
        "                asyncio.set_event_loop(loop)\n",
        "                try:\n",
        "                    return loop.run_until_complete(realtime_text_to_speech_async(text, voice))\n",
        "                finally:\n",
        "                    loop.close()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  L·ªói khi g·ªçi Edge TTS: {e}, th·ª≠ TTS kh√°c...\")\n",
        "    \n",
        "    # Fallback 1: TTS c≈© (Coqui TTS ho·∫∑c pyttsx3)\n",
        "    try:\n",
        "        result = text_to_speech(text)\n",
        "        if result:\n",
        "            return result\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  L·ªói TTS c≈©: {e}, th·ª≠ gTTS...\")\n",
        "    \n",
        "    # Fallback 2: gTTS (Google Text-to-Speech) - c·∫ßn internet nh∆∞ng mi·ªÖn ph√≠\n",
        "    try:\n",
        "        from gtts import gTTS\n",
        "        import tempfile\n",
        "        \n",
        "        fd, audio_path = tempfile.mkstemp(suffix=\".mp3\", prefix=\"gtts_\")\n",
        "        os.close(fd)\n",
        "        \n",
        "        # T·∫°o audio v·ªõi gTTS\n",
        "        tts = gTTS(text=text, lang='vi', slow=False)\n",
        "        tts.save(audio_path)\n",
        "        \n",
        "        if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:\n",
        "            print(\"‚úÖ ƒê√£ d√πng gTTS (c·∫ßn internet)\")\n",
        "            return audio_path\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è  gTTS ch∆∞a ƒë∆∞·ª£c c√†i, c√†i ƒë·∫∑t...\")\n",
        "        import subprocess\n",
        "        import sys\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gtts\"], check=False)\n",
        "        try:\n",
        "            from gtts import gTTS\n",
        "            fd, audio_path = tempfile.mkstemp(suffix=\".mp3\", prefix=\"gtts_\")\n",
        "            os.close(fd)\n",
        "            tts = gTTS(text=text, lang='vi', slow=False)\n",
        "            tts.save(audio_path)\n",
        "            if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:\n",
        "                print(\"‚úÖ ƒê√£ d√πng gTTS (c·∫ßn internet)\")\n",
        "                return audio_path\n",
        "        except:\n",
        "            pass\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  L·ªói gTTS: {e}\")\n",
        "    \n",
        "    # Fallback 3: Piper TTS (n·∫øu c√≥)\n",
        "    try:\n",
        "        import piper\n",
        "        import tempfile\n",
        "        \n",
        "        fd, audio_path = tempfile.mkstemp(suffix=\".wav\", prefix=\"piper_\")\n",
        "        os.close(fd)\n",
        "        \n",
        "        # Piper TTS c·∫ßn model, t·∫°m th·ªùi b·ªè qua\n",
        "        print(\"‚ö†Ô∏è  Piper TTS c·∫ßn c·∫•u h√¨nh model, b·ªè qua\")\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # N·∫øu t·∫•t c·∫£ ƒë·ªÅu fail, tr·∫£ v·ªÅ None\n",
        "    print(\"‚ùå Kh√¥ng c√≥ TTS n√†o kh·∫£ d·ª•ng!\")\n",
        "    return None\n",
        "\n",
        "print(\"‚úÖ C√°c h√†m Realtime STT v√† TTS ƒë√£ ƒë∆∞·ª£c t·∫°o!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T·∫°o Gradio Interface cho Realtime Streaming Voice Chat 1-1\n",
        "# V·ªõi TTS streaming (n√≥i ngay khi c√≥ text) v√† STT streaming (hi·ªÉn th·ªã text d·∫ßn)\n",
        "# Auto-send sau 1 gi√¢y khi d·ª´ng n√≥i\n",
        "\n",
        "import threading\n",
        "import time\n",
        "\n",
        "with gr.Blocks(\n",
        "    title=\"Realtime Streaming Voice Chat v·ªõi AI T∆∞ v·∫•n Ng√¢n h√†ng\"\n",
        ") as demo_realtime:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üéôÔ∏è Realtime Streaming Voice Chat v·ªõi AI T∆∞ v·∫•n Ng√¢n h√†ng\n",
        "    \n",
        "    <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; color: white;\">\n",
        "    <h2 style=\"color: white; margin: 0;\">N√≥i chuy·ªán tr·ª±c ti·∫øp v·ªõi AI nh∆∞ cu·ªôc g·ªçi ƒëi·ªán tho·∫°i!</h2>\n",
        "    </div>\n",
        "    \n",
        "    ### ‚ö° T√≠nh nƒÉng:\n",
        "    - üé§ **Voice Input**: Nh·∫•n n√∫t ƒë·ªÉ ghi √¢m, nh·∫•n Stop ƒë·ªÉ g·ª≠i\n",
        "    - ü§ñ **Response Streaming**: AI tr·∫£ l·ªùi t·ª´ng token (hi·ªÉn th·ªã text)\n",
        "    - üîä **TTS**: AI ph√°t √¢m khi response ho√†n t·∫•t (d√πng gTTS)\n",
        "    \"\"\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            # Chatbot v·ªõi giao di·ªán ƒë·∫πp\n",
        "            chatbot_realtime = gr.Chatbot(\n",
        "                label=\"üí¨ Cu·ªôc tr√≤ chuy·ªán\",\n",
        "                height=450,\n",
        "                show_label=True,\n",
        "                type=\"messages\",  # D√πng messages format thay v√¨ tuples\n",
        "                allow_tags=False\n",
        "                # Note: show_copy_button ƒë√£ deprecated, b·ªè qua n·∫øu kh√¥ng h·ªó tr·ª£\n",
        "            )\n",
        "            \n",
        "            with gr.Row():\n",
        "                # Text hi·ªÉn th·ªã khi ƒëang n√≥i (STT streaming) - giao di·ªán ƒë·∫πp h∆°n\n",
        "                stt_streaming_text = gr.Textbox(\n",
        "                    label=\"üé§ ƒêang nghe b·∫°n n√≥i...\",\n",
        "                    placeholder=\"Text s·∫Ω hi·ªÉn th·ªã d·∫ßn ·ªü ƒë√¢y khi b·∫°n n√≥i...\",\n",
        "                    lines=2,\n",
        "                    show_label=True,\n",
        "                    interactive=False,\n",
        "                    container=True,\n",
        "                    scale=3\n",
        "                )\n",
        "                \n",
        "                # Status indicator v·ªõi icon\n",
        "                status_display = gr.Markdown(\n",
        "                    value=\"<div style='padding: 10px; background: #f0f0f0; border-radius: 5px;'>üí° **S·∫µn s√†ng** - Nh·∫•n microphone v√† b·∫Øt ƒë·∫ßu n√≥i</div>\"\n",
        "                )\n",
        "            \n",
        "            # Audio output ·∫©n (t·ª± ƒë·ªông ph√°t)\n",
        "            audio_output_realtime = gr.Audio(\n",
        "                type=\"filepath\",\n",
        "                visible=True,  # Hi·ªÉn th·ªã ƒë·ªÉ user c√≥ th·ªÉ th·∫•y v√† control\n",
        "                autoplay=True,  # T·ª± ƒë·ªông ph√°t\n",
        "                label=\"üîä Audio Response\"\n",
        "            )\n",
        "        \n",
        "        with gr.Column(scale=1):\n",
        "            # Voice input v·ªõi giao di·ªán ƒë·∫πp\n",
        "            with gr.Group():\n",
        "                gr.Markdown(\"### üéôÔ∏è ƒêi·ªÅu khi·ªÉn\")\n",
        "                audio_input_realtime = gr.Audio(\n",
        "                    sources=[\"microphone\"],\n",
        "                    type=\"filepath\",\n",
        "                    label=\"N√≥i v√†o ƒë√¢y (nh·∫•n ƒë·ªÉ ghi √¢m)\",\n",
        "                    show_label=True\n",
        "                    # B·ªè streaming=True ƒë·ªÉ ch·ªâ ghi √¢m khi nh·∫•n n√∫t\n",
        "                )\n",
        "                gr.Markdown(\"\"\"\n",
        "                <div style=\"font-size: 12px; color: #666; padding: 10px; background: #f9f9f9; border-radius: 5px;\">\n",
        "                üí° **H∆∞·ªõng d·∫´n**:<br>\n",
        "                ‚Ä¢ Nh·∫•n n√∫t microphone ƒë·ªÉ b·∫Øt ƒë·∫ßu ghi √¢m<br>\n",
        "                ‚Ä¢ N√≥i xong th√¨ nh·∫•n Stop ƒë·ªÉ g·ª≠i<br>\n",
        "                ‚Ä¢ AI s·∫Ω tr·∫£ l·ªùi v√† ph√°t √¢m khi ho√†n t·∫•t\n",
        "                </div>\n",
        "                \"\"\")\n",
        "            \n",
        "            # Buttons\n",
        "            clear_chat_btn = gr.Button(\n",
        "                \"üóëÔ∏è X√≥a l·ªãch s·ª≠\", \n",
        "                variant=\"secondary\", \n",
        "                size=\"lg\",\n",
        "                scale=1\n",
        "            )\n",
        "            \n",
        "            # Hi·ªÉn th·ªã t√†i nguy√™n h·ªá th·ªëng\n",
        "            resources_display_realtime = gr.Markdown(\n",
        "                value=format_resources_info(),\n",
        "                label=\"üìä T√†i nguy√™n h·ªá th·ªëng\"\n",
        "            )\n",
        "    \n",
        "    # Bi·∫øn global ƒë·ªÉ l∆∞u tr·ªØ audio chunks cho TTS streaming\n",
        "    audio_queue = []\n",
        "    last_audio_time = 0\n",
        "    \n",
        "    # H√†m helper ƒë·ªÉ normalize audio input t·ª´ Gradio\n",
        "    def normalize_audio_input(audio_input):\n",
        "        \"\"\"\n",
        "        Chuy·ªÉn ƒë·ªïi audio input t·ª´ Gradio th√†nh file path\n",
        "        Gradio c√≥ th·ªÉ tr·∫£ v·ªÅ: string path, tuple (sample_rate, audio_data), ho·∫∑c dict\n",
        "        \"\"\"\n",
        "        if audio_input is None:\n",
        "            return None\n",
        "        \n",
        "        import os\n",
        "        import tempfile\n",
        "        import soundfile as sf\n",
        "        import numpy as np\n",
        "        \n",
        "        # N·∫øu l√† string path\n",
        "        if isinstance(audio_input, str):\n",
        "            # Ki·ªÉm tra file c√≥ t·ªìn t·∫°i v√† c√≥ th·ªÉ ƒë·ªçc ƒë∆∞·ª£c kh√¥ng\n",
        "            if os.path.exists(audio_input) and os.path.getsize(audio_input) > 0:\n",
        "                # ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ ƒë·∫£m b·∫£o file ƒë∆∞·ª£c ghi ƒë·∫ßy ƒë·ªß\n",
        "                import time\n",
        "                time.sleep(0.1)\n",
        "                return audio_input\n",
        "            else:\n",
        "                return None\n",
        "        \n",
        "        # N·∫øu l√† tuple (sample_rate, audio_data)\n",
        "        elif isinstance(audio_input, tuple) and len(audio_input) == 2:\n",
        "            sample_rate, audio_data = audio_input\n",
        "            # Convert numpy array th√†nh file WAV\n",
        "            fd, temp_path = tempfile.mkstemp(suffix=\".wav\")\n",
        "            os.close(fd)\n",
        "            try:\n",
        "                sf.write(temp_path, audio_data, sample_rate)\n",
        "                return temp_path\n",
        "            except Exception as e:\n",
        "                print(f\"L·ªói khi convert audio tuple: {e}\")\n",
        "                if os.path.exists(temp_path):\n",
        "                    os.remove(temp_path)\n",
        "                return None\n",
        "        \n",
        "        # N·∫øu l√† dict (Gradio m·ªõi)\n",
        "        elif isinstance(audio_input, dict):\n",
        "            if 'path' in audio_input:\n",
        "                path = audio_input['path']\n",
        "                if os.path.exists(path) and os.path.getsize(path) > 0:\n",
        "                    import time\n",
        "                    time.sleep(0.1)\n",
        "                    return path\n",
        "            elif 'data' in audio_input:\n",
        "                # C√≥ audio data, c·∫ßn convert\n",
        "                return normalize_audio_input(audio_input['data'])\n",
        "            return None\n",
        "        \n",
        "        return None\n",
        "    \n",
        "    # H√†m x·ª≠ l√Ω STT streaming (hi·ªÉn th·ªã text d·∫ßn khi ƒëang n√≥i)\n",
        "    def stt_streaming_handler(audio_input):\n",
        "        \"\"\"\n",
        "        X·ª≠ l√Ω STT streaming - hi·ªÉn th·ªã text d·∫ßn khi ƒëang n√≥i\n",
        "        \"\"\"\n",
        "        if audio_input is None:\n",
        "            return \"\"\n",
        "        \n",
        "        try:\n",
        "            # Normalize audio input th√†nh file path\n",
        "            audio_path = normalize_audio_input(audio_input)\n",
        "            if audio_path is None or not os.path.exists(audio_path):\n",
        "                return \"\"\n",
        "            \n",
        "            # Ki·ªÉm tra file c√≥ d·ªØ li·ªáu kh√¥ng\n",
        "            if os.path.getsize(audio_path) == 0:\n",
        "                return \"\"\n",
        "            \n",
        "            # D√πng faster-whisper v·ªõi streaming (n·∫øu c√≥)\n",
        "            if faster_whisper_model is not None:\n",
        "                # Transcribe v·ªõi streaming - l·∫•y segments ngay\n",
        "                segments, info = faster_whisper_model.transcribe(\n",
        "                    audio_path,\n",
        "                    language=\"vi\",\n",
        "                    beam_size=1,\n",
        "                    vad_filter=True,\n",
        "                    vad_parameters=dict(min_silence_duration_ms=300)  # Nh·∫°y h∆°n\n",
        "                )\n",
        "                \n",
        "                # L·∫•y text t·ª´ segments - hi·ªÉn th·ªã d·∫ßn\n",
        "                text_parts = []\n",
        "                for segment in segments:\n",
        "                    text_parts.append(segment.text)\n",
        "                \n",
        "                text = \" \".join(text_parts)\n",
        "                return text.strip()\n",
        "            else:\n",
        "                # Fallback v·ªÅ Whisper th√¥ng th∆∞·ªùng\n",
        "                return speech_to_text(audio_path)\n",
        "        except Exception as e:\n",
        "            print(f\"L·ªói STT streaming: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return \"\"\n",
        "    \n",
        "    # H√†m TTS ƒë∆°n gi·∫£n ch·ªâ d√πng gTTS\n",
        "    def simple_gtts(text: str) -> str:\n",
        "        \"\"\"\n",
        "        Chuy·ªÉn ƒë·ªïi text th√†nh audio ch·ªâ d√πng gTTS (b·ªè c√°c TTS kh√°c)\n",
        "        \"\"\"\n",
        "        if not text or not text.strip():\n",
        "            print(\"‚ö†Ô∏è  simple_gtts: Text r·ªóng\")\n",
        "            return None\n",
        "        \n",
        "        try:\n",
        "            from gtts import gTTS\n",
        "            import tempfile\n",
        "            import os\n",
        "            \n",
        "            print(f\"üîä ƒêang t·∫°o audio v·ªõi gTTS cho text: {text[:50]}...\")\n",
        "            \n",
        "            fd, audio_path = tempfile.mkstemp(suffix=\".mp3\", prefix=\"gtts_\")\n",
        "            os.close(fd)\n",
        "            \n",
        "            # T·∫°o audio v·ªõi gTTS\n",
        "            tts = gTTS(text=text.strip(), lang='vi', slow=False)\n",
        "            tts.save(audio_path)\n",
        "            \n",
        "            # Ki·ªÉm tra file ƒë√£ ƒë∆∞·ª£c t·∫°o ch∆∞a\n",
        "            if os.path.exists(audio_path):\n",
        "                file_size = os.path.getsize(audio_path)\n",
        "                if file_size > 0:\n",
        "                    print(f\"‚úÖ ƒê√£ t·∫°o audio th√†nh c√¥ng: {audio_path} ({file_size} bytes)\")\n",
        "                    return audio_path\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è  File audio r·ªóng: {audio_path}\")\n",
        "                    return None\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  File audio kh√¥ng t·ªìn t·∫°i: {audio_path}\")\n",
        "                return None\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è  gTTS ch∆∞a ƒë∆∞·ª£c c√†i, ƒëang c√†i ƒë·∫∑t...\")\n",
        "            import subprocess\n",
        "            import sys\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gtts\"], check=False)\n",
        "            try:\n",
        "                from gtts import gTTS\n",
        "                import tempfile\n",
        "                import os\n",
        "                fd, audio_path = tempfile.mkstemp(suffix=\".mp3\", prefix=\"gtts_\")\n",
        "                os.close(fd)\n",
        "                tts = gTTS(text=text.strip(), lang='vi', slow=False)\n",
        "                tts.save(audio_path)\n",
        "                if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:\n",
        "                    print(f\"‚úÖ ƒê√£ t·∫°o audio th√†nh c√¥ng sau khi c√†i gTTS: {audio_path}\")\n",
        "                    return audio_path\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è  Kh√¥ng th·ªÉ t·∫°o audio sau khi c√†i gTTS\")\n",
        "                    return None\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  L·ªói khi c√†i/c√†i gTTS: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  L·ªói gTTS: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "    \n",
        "    # H√†m x·ª≠ l√Ω realtime voice chat - ch·ªâ TTS khi response ho√†n t·∫•t\n",
        "    def realtime_voice_chat_streaming(audio_input, history):\n",
        "        \"\"\"\n",
        "        X·ª≠ l√Ω realtime voice chat:\n",
        "        - STT: Chuy·ªÉn ƒë·ªïi audio th√†nh text\n",
        "        - Model: Stream response t·ª´ng token (ch·ªâ hi·ªÉn th·ªã text)\n",
        "        - TTS: Ch·ªâ ph√°t √¢m khi response ho√†n t·∫•t (d√πng gTTS)\n",
        "        \"\"\"\n",
        "        if audio_input is None:\n",
        "            status_html = \"<div style='padding: 10px; background: #e8f5e9; border-radius: 5px;'>üí° **S·∫µn s√†ng** - Nh·∫•n microphone v√† b·∫Øt ƒë·∫ßu n√≥i</div>\"\n",
        "            return history, \"\", None, status_html\n",
        "        \n",
        "        # 1. STT - Chuy·ªÉn ƒë·ªïi audio th√†nh text\n",
        "        user_text = stt_streaming_handler(audio_input)\n",
        "        \n",
        "        if not user_text or not user_text.strip():\n",
        "            status_html = \"<div style='padding: 10px; background: #fff3e0; border-radius: 5px;'>‚ö†Ô∏è **Kh√¥ng nghe r√µ** - Vui l√≤ng n√≥i l·∫°i</div>\"\n",
        "            return history, \"\", None, status_html\n",
        "        \n",
        "        # 2. Th√™m user message v√†o history (format messages)\n",
        "        if history is None:\n",
        "            history = []\n",
        "        history.append({\"role\": \"user\", \"content\": user_text.strip()})\n",
        "        \n",
        "        status_html = f\"<div style='padding: 10px; background: #e3f2fd; border-radius: 5px;'>ü§ñ **AI ƒëang suy nghƒ©** v·ªÅ: '{user_text.strip()[:40]}...'</div>\"\n",
        "        yield history, \"\", None, status_html  # Clear STT text, update status\n",
        "        \n",
        "        # 3. Process v·ªõi model (streaming) - ch·ªâ hi·ªÉn th·ªã text, kh√¥ng TTS streaming\n",
        "        response_text = \"\"\n",
        "        \n",
        "        # Th√™m assistant message v·ªõi content r·ªóng ƒë·ªÉ update sau\n",
        "        history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
        "        \n",
        "        try:\n",
        "            # Stream response t·ª´ng token - ch·ªâ update UI, kh√¥ng TTS\n",
        "            for partial_response in process_with_model_stream(user_text.strip()):\n",
        "                response_text = partial_response\n",
        "                history[-1] = {\"role\": \"assistant\", \"content\": response_text}\n",
        "                \n",
        "                # Ch·ªâ update UI, kh√¥ng TTS streaming\n",
        "                status_html = f\"<div style='padding: 10px; background: #e3f2fd; border-radius: 5px;'>ü§ñ **AI ƒëang tr·∫£ l·ªùi**... ({len(response_text)} k√Ω t·ª±)</div>\"\n",
        "                yield history, \"\", None, status_html\n",
        "            \n",
        "            # 4. Khi response ho√†n t·∫•t, m·ªõi g·ªçi TTS (gTTS)\n",
        "            if response_text and response_text.strip():\n",
        "                print(f\"üîä B·∫Øt ƒë·∫ßu t·∫°o audio cho response: {response_text[:100]}...\")\n",
        "                status_html = \"<div style='padding: 10px; background: #f3e5f5; border-radius: 5px;'>üîä **ƒêang t·∫°o audio**...</div>\"\n",
        "                yield history, \"\", None, status_html\n",
        "                \n",
        "                try:\n",
        "                    audio_output = simple_gtts(response_text.strip())\n",
        "                    print(f\"üîä K·∫øt qu·∫£ simple_gtts: {audio_output}\")\n",
        "                    if audio_output:\n",
        "                        # Ki·ªÉm tra file c√≥ t·ªìn t·∫°i kh√¥ng\n",
        "                        import os\n",
        "                        if os.path.exists(audio_output):\n",
        "                            file_size = os.path.getsize(audio_output)\n",
        "                            print(f\"‚úÖ Audio file t·ªìn t·∫°i: {audio_output} ({file_size} bytes)\")\n",
        "                            status_html = \"<div style='padding: 10px; background: #e8f5e9; border-radius: 5px;'>‚úÖ **Ho√†n t·∫•t** - ƒêang ph√°t audio</div>\"\n",
        "                            yield history, \"\", audio_output, status_html\n",
        "                        else:\n",
        "                            print(f\"‚ö†Ô∏è  Audio file kh√¥ng t·ªìn t·∫°i: {audio_output}\")\n",
        "                            status_html = \"<div style='padding: 10px; background: #fff3e0; border-radius: 5px;'>‚ö†Ô∏è **File audio kh√¥ng t·ªìn t·∫°i**</div>\"\n",
        "                            yield history, \"\", None, status_html\n",
        "                    else:\n",
        "                        print(\"‚ö†Ô∏è  simple_gtts tr·∫£ v·ªÅ None\")\n",
        "                        status_html = \"<div style='padding: 10px; background: #fff3e0; border-radius: 5px;'>‚ö†Ô∏è **Kh√¥ng th·ªÉ t·∫°o audio** - Nh∆∞ng ƒë√£ c√≥ text response</div>\"\n",
        "                        yield history, \"\", None, status_html\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  L·ªói TTS: {e}\")\n",
        "                    import traceback\n",
        "                    traceback.print_exc()\n",
        "                    status_html = f\"<div style='padding: 10px; background: #fff3e0; border-radius: 5px;'>‚ö†Ô∏è **L·ªói TTS**: {str(e)}</div>\"\n",
        "                    yield history, \"\", None, status_html\n",
        "            else:\n",
        "                status_html = \"<div style='padding: 10px; background: #e8f5e9; border-radius: 5px;'>‚úÖ **Ho√†n t·∫•t** - S·∫µn s√†ng cho c√¢u h·ªèi ti·∫øp theo</div>\"\n",
        "                yield history, \"\", None, status_html\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"L·ªói trong model processing: {e}\")\n",
        "            response_text = f\"Xin l·ªói, c√≥ l·ªói x·∫£y ra: {str(e)}\"\n",
        "            history[-1] = {\"role\": \"assistant\", \"content\": response_text}\n",
        "            status_html = f\"<div style='padding: 10px; background: #ffebee; border-radius: 5px;'>‚ùå **L·ªói**: {str(e)}</div>\"\n",
        "            yield history, \"\", None, status_html\n",
        "    \n",
        "    # Event handler: X·ª≠ l√Ω khi user nh·∫•n Stop recording\n",
        "    audio_input_realtime.stop_recording(\n",
        "        fn=realtime_voice_chat_streaming,\n",
        "        inputs=[audio_input_realtime, chatbot_realtime],\n",
        "        outputs=[chatbot_realtime, stt_streaming_text, audio_output_realtime, status_display],\n",
        "        show_progress=False\n",
        "    )\n",
        "    \n",
        "    # B·ªè STT streaming - ch·ªâ x·ª≠ l√Ω khi user nh·∫•n Stop\n",
        "    \n",
        "    clear_chat_btn.click(\n",
        "        fn=lambda: ([], \"\", None, \"<div style='padding: 10px; background: #e8f5e9; border-radius: 5px;'>üí° **S·∫µn s√†ng** - Nh·∫•n microphone v√† b·∫Øt ƒë·∫ßu n√≥i</div>\"),\n",
        "        outputs=[chatbot_realtime, stt_streaming_text, audio_output_realtime, status_display]\n",
        "    )\n",
        "    \n",
        "    # Th√™m theme v√†o launch (thay v√¨ Blocks constructor)\n",
        "    # S·∫Ω ƒë∆∞·ª£c set trong cell kh·ªüi ƒë·ªông server\n",
        "    \n",
        "    # Auto-refresh t√†i nguy√™n\n",
        "    demo_realtime.load(\n",
        "        fn=lambda: format_resources_info(),\n",
        "        inputs=None,\n",
        "        outputs=resources_display_realtime\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Realtime Voice Chat Interface ƒë√£ ƒë∆∞·ª£c t·∫°o!\")\n",
        "print(\"üí° S·ª≠ d·ª•ng 'demo_realtime' ƒë·ªÉ kh·ªüi ƒë·ªông server realtime\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Kh·ªüi ƒë·ªông Realtime Voice Chat Server\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kh·ªüi ƒë·ªông Realtime Voice Chat Server\n",
        "\n",
        "# Ki·ªÉm tra xem demo_realtime ƒë√£ ƒë∆∞·ª£c t·∫°o ch∆∞a\n",
        "if 'demo_realtime' not in globals():\n",
        "    print(\"‚ùå L·ªói: demo_realtime ch∆∞a ƒë∆∞·ª£c t·∫°o. Vui l√≤ng ch·∫°y cell t·∫°o Realtime Voice Chat interface tr∆∞·ªõc.\")\n",
        "    raise RuntimeError(\"demo_realtime ch∆∞a ƒë∆∞·ª£c t·∫°o. Ch·∫°y cell t·∫°o interface tr∆∞·ªõc.\")\n",
        "\n",
        "# ƒê√≥ng server c≈© n·∫øu ƒë√£ ch·∫°y\n",
        "try:\n",
        "    if hasattr(demo_realtime, 'close'):\n",
        "        demo_realtime.close()\n",
        "        print(\"‚úÖ ƒê√£ ƒë√≥ng server realtime c≈©\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Kh√¥ng th·ªÉ ƒë√≥ng server c≈© (c√≥ th·ªÉ ch∆∞a ch·∫°y): {e}\")\n",
        "\n",
        "# Kh·ªüi ƒë·ªông server realtime m·ªõi\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ ƒêang kh·ªüi ƒë·ªông Realtime Voice Chat Server...\")\n",
        "print(\"=\"*60)\n",
        "print(\"üìã Th√¥ng tin server:\")\n",
        "print(\"   - Server: 0.0.0.0:7861 (port kh√°c v·ªõi server th√¥ng th∆∞·ªùng)\")\n",
        "print(\"   - Share: True (c√≥ public link)\")\n",
        "print(\"   - Debug: True\")\n",
        "print(\"   - Mode: Realtime Voice Chat 1-1\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    demo_realtime.launch(\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7861,  # Port kh√°c ƒë·ªÉ kh√¥ng conflict v·ªõi server th√¥ng th∆∞·ªùng\n",
        "        share=True,\n",
        "        debug=True,\n",
        "        show_error=True\n",
        "        # Note: theme kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£ trong launch() c·ªßa phi√™n b·∫£n Gradio n√†y\n",
        "    )\n",
        "    print(\"\\n‚úÖ Realtime Voice Chat Server ƒë√£ kh·ªüi ƒë·ªông th√†nh c√¥ng!\")\n",
        "    print(\"üí° B·∫°n c√≥ th·ªÉ:\")\n",
        "    print(\"   - N√≥i chuy·ªán tr·ª±c ti·∫øp v·ªõi AI nh∆∞ cu·ªôc g·ªçi ƒëi·ªán tho·∫°i\")\n",
        "    print(\"   - AI s·∫Ω nh·∫≠n d·∫°ng gi·ªçng n√≥i, tr·∫£ l·ªùi v√† ph√°t √¢m l·∫°i\")\n",
        "    print(\"   - S·ª≠ d·ª•ng local URL ho·∫∑c public URL ƒë·ªÉ truy c·∫≠p\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå L·ªói khi kh·ªüi ƒë·ªông server: {e}\")\n",
        "    print(\"üí° Th·ª≠:\")\n",
        "    print(\"   1. Ki·ªÉm tra xem port 7861 ƒë√£ ƒë∆∞·ª£c s·ª≠ d·ª•ng ch∆∞a\")\n",
        "    print(\"   2. ƒê√≥ng server c≈© n·∫øu c√≥\")\n",
        "    print(\"   3. Ch·∫°y l·∫°i cell n√†y\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Kh·ªüi ƒë·ªông Server\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kh·ªüi ƒë·ªông server\n",
        "# Trong Colab, b·∫°n c√≥ th·ªÉ d√πng share=True ƒë·ªÉ c√≥ public link\n",
        "\n",
        "# ƒê√≥ng server c≈© n·∫øu ƒë√£ ch·∫°y (ƒë·ªÉ c√≥ th·ªÉ kh·ªüi ƒë·ªông l·∫°i)\n",
        "try:\n",
        "    if 'demo' in globals() and hasattr(demo, 'close'):\n",
        "        demo.close()\n",
        "        print(\"ƒê√£ ƒë√≥ng server c≈©\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Kh·ªüi ƒë·ªông server m·ªõi\n",
        "print(\"ƒêang kh·ªüi ƒë·ªông server...\")\n",
        "demo.launch(\n",
        "    server_name=\"0.0.0.0\",\n",
        "    server_port=7860,\n",
        "    share=True,  # T·∫°o public link (t·∫Øt n·∫øu kh√¥ng c·∫ßn)\n",
        "    debug=True\n",
        "    # Note: theme kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£ trong launch() c·ªßa phi√™n b·∫£n n√†y\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Alternative: FastAPI Server (N√¢ng cao)\n",
        "\n",
        "N·∫øu b·∫°n mu·ªën t·∫°o m·ªôt REST API server thay v√¨ Gradio, c√≥ th·ªÉ s·ª≠ d·ª•ng code b√™n d∆∞·ªõi.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment ƒë·ªÉ s·ª≠ d·ª•ng FastAPI thay v√¨ Gradio\n",
        "\"\"\"\n",
        "from fastapi import FastAPI, File, UploadFile, Form\n",
        "from fastapi.responses import FileResponse, JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import uvicorn\n",
        "import tempfile\n",
        "\n",
        "app = FastAPI(title=\"Voice Chat API\")\n",
        "\n",
        "# CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "@app.post(\"/voice-chat\")\n",
        "async def voice_chat_api(\n",
        "    audio: UploadFile = File(...),\n",
        "    image: UploadFile = File(None)\n",
        "):\n",
        "    # L∆∞u audio t·∫°m\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as tmp_audio:\n",
        "        tmp_audio.write(await audio.read())\n",
        "        tmp_audio_path = tmp_audio.name\n",
        "    \n",
        "    # L∆∞u image t·∫°m n·∫øu c√≥\n",
        "    image_path = None\n",
        "    if image:\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".jpg\") as tmp_image:\n",
        "            tmp_image.write(await image.read())\n",
        "            image_path = tmp_image.name\n",
        "    \n",
        "    try:\n",
        "        # Speech-to-Text\n",
        "        user_text = speech_to_text(tmp_audio_path)\n",
        "        \n",
        "        # Process v·ªõi model\n",
        "        response_text = process_with_model(user_text, image_path)\n",
        "        \n",
        "        # Text-to-Speech\n",
        "        audio_output = text_to_speech(response_text)\n",
        "        \n",
        "        return FileResponse(\n",
        "            audio_output,\n",
        "            media_type=\"audio/mpeg\",\n",
        "            headers={\"X-Response-Text\": response_text}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return JSONResponse(\n",
        "            status_code=500,\n",
        "            content={\"error\": str(e)}\n",
        "        )\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        if os.path.exists(tmp_audio_path):\n",
        "            os.remove(tmp_audio_path)\n",
        "        if image_path and os.path.exists(image_path):\n",
        "            os.remove(image_path)\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    return {\"status\": \"ok\"}\n",
        "\n",
        "# Ch·∫°y server\n",
        "# uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## L∆∞u √Ω:\n",
        "\n",
        "1. **Whisper Model**: C√≥ th·ªÉ thay ƒë·ªïi size t·ª´ \"tiny\" (nhanh, √≠t ch√≠nh x√°c) ƒë·∫øn \"large\" (ch·∫≠m, ch√≠nh x√°c cao)\n",
        "2. **gTTS**: Mi·ªÖn ph√≠ nh∆∞ng c·∫ßn internet. C√≥ th·ªÉ thay b·∫±ng c√°c TTS kh√°c nh∆∞:\n",
        "   - Coqui TTS (offline, ch·∫•t l∆∞·ª£ng cao)\n",
        "   - Azure Cognitive Services\n",
        "   - Google Cloud TTS\n",
        "3. **Memory**: Model 2B kh√° nh·∫π, nh∆∞ng n·∫øu h·∫øt memory c√≥ th·ªÉ d√πng quantization\n",
        "4. **Latency**: ƒê·ªÉ gi·∫£m ƒë·ªô tr·ªÖ, c√≥ th·ªÉ:\n",
        "   - D√πng Whisper model nh·ªè h∆°n\n",
        "   - Cache model responses\n",
        "   - S·ª≠ d·ª•ng streaming TTS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## T·ªëi ∆∞u h√≥a ƒë√£ √°p d·ª•ng:\n",
        "\n",
        "### 1. **Whisper Tiny Model**\n",
        "   - Thay \"base\" b·∫±ng \"tiny\" ‚Üí nhanh h∆°n 3-4 l·∫ßn, nh·∫π h∆°n 75%\n",
        "   - T·∫Øt c√°c t√≠nh nƒÉng kh√¥ng c·∫ßn thi·∫øt (verbose, condition_on_previous_text)\n",
        "\n",
        "### 2. **Model Quantization (4-bit)**\n",
        "   - Gi·∫£m memory t·ª´ ~4GB xu·ªëng ~1GB\n",
        "   - TƒÉng t·ªëc ƒë·ªô inference\n",
        "   - V·∫´n gi·ªØ ƒë∆∞·ª£c ch·∫•t l∆∞·ª£ng t·ªët\n",
        "\n",
        "### 3. **Offline TTS (Coqui TTS)**\n",
        "   - Kh√¥ng c·∫ßn internet (thay v√¨ gTTS)\n",
        "   - Nhanh h∆°n gTTS 2-3 l·∫ßn\n",
        "   - H·ªó tr·ª£ nhi·ªÅu ng√¥n ng·ªØ\n",
        "   - C√≥ fallback models n·∫øu c·∫ßn\n",
        "\n",
        "### 4. **T·ªëi ∆∞u Generation**\n",
        "   - `max_new_tokens`: 512 ‚Üí 256 (nhanh h∆°n 2x)\n",
        "   - `torch.no_grad()`: T·∫Øt gradient tracking\n",
        "   - `top_p`: 0.8 ‚Üí 0.9 (√≠t t√≠nh to√°n h∆°n)\n",
        "   - `use_cache=True`: S·ª≠ d·ª•ng KV cache\n",
        "\n",
        "### 5. **T·ªëi ∆∞u Whisper**\n",
        "   - `fp16=True` n·∫øu c√≥ GPU\n",
        "   - T·∫Øt verbose v√† c√°c t√≠nh nƒÉng kh√¥ng c·∫ßn\n",
        "\n",
        "### K·∫øt qu·∫£ so s√°nh:\n",
        "| Metric | Tr∆∞·ªõc | Sau | C·∫£i thi·ªán |\n",
        "|--------|-------|-----|-----------|\n",
        "| **Whisper Speed** | ~2-3s | ~0.5-1s | **3-4x nhanh h∆°n** |\n",
        "| **Model Memory** | ~4GB | ~1GB | **75% ti·∫øt ki·ªám** |\n",
        "| **TTS Speed** | ~2-3s | ~0.5-1s | **2-3x nhanh h∆°n** |\n",
        "| **Total Latency** | ~6-9s | ~2-3s | **3x nhanh h∆°n** |\n",
        "| **Internet** | C·∫ßn (gTTS) | Kh√¥ng c·∫ßn | **100% offline** |\n",
        "\n",
        "### L∆∞u √Ω:\n",
        "- Whisper \"tiny\" c√≥ th·ªÉ k√©m ch√≠nh x√°c h∆°n \"base\" m·ªôt ch√∫t, nh∆∞ng v·∫´n r·∫•t t·ªët\n",
        "- N·∫øu c·∫ßn ch√≠nh x√°c h∆°n, c√≥ th·ªÉ ƒë·ªïi l·∫°i \"base\" ho·∫∑c \"small\"\n",
        "- TTS model s·∫Ω t·ª± ƒë·ªông fallback n·∫øu model ch√≠nh kh√¥ng load ƒë∆∞·ª£c\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
