# Qwen3 Dataset Builder & Training Scripts

HÆ°á»›ng dáº«n sá»­ dá»¥ng cÃ¡c script Python Ä‘á»ƒ táº¡o dataset vÃ  training Qwen3 model.

## ğŸ“‹ YÃªu cáº§u

```bash
pip install -r requirements.txt
```

## ğŸ“ 1. Táº¡o Dataset (qwen3_dataset_builder.py)

Script Ä‘á»ƒ táº¡o vÃ  chuáº©n bá»‹ dataset theo chuáº©n Qwen3.

### CÃ i Ä‘áº·t

```bash
# CÃ i dependencies
pip install transformers datasets pandas tqdm
```

### Sá»­ dá»¥ng

#### Load tá»« CSV vÃ  export ra JSON

```bash
python qwen3_dataset_builder.py \
    --input data.csv \
    --output dataset.json
```

#### Load tá»« JSON vá»›i format Q&A

```bash
python qwen3_dataset_builder.py \
    --input data.json \
    --format qa \
    --output dataset.jsonl
```

#### Load tá»« JSONL vÃ  chia train/test

```bash
python qwen3_dataset_builder.py \
    --input data.jsonl \
    --format jsonl \
    --output dataset.json \
    --split 0.2
```

#### Preview dataset

```bash
python qwen3_dataset_builder.py \
    --input data.json \
    --preview 5
```

### CÃ¡c format há»— trá»£

1. **CSV**: File CSV vá»›i columns `question` vÃ  `answer`
2. **JSON (Q&A)**: `[{"question": "...", "answer": "..."}]`
3. **JSON (Qwen3)**: `[{"messages": [...]}]`
4. **JSONL**: Má»—i dÃ²ng lÃ  má»™t JSON conversation

### Format chuáº©n Qwen3

```json
{
  "messages": [
    {
      "role": "user",
      "content": [{"type": "text", "text": "CÃ¢u há»i"}]
    },
    {
      "role": "assistant",
      "content": [{"type": "text", "text": "CÃ¢u tráº£ lá»i"}]
    }
  ]
}
```

### Options

```
--input, -i          ÄÆ°á»ng dáº«n file input (required)
--output, -o         ÄÆ°á»ng dáº«n file output (optional)
--format, -f         Format input (auto, qwen3, qa, conversation, csv, jsonl)
--question-col       TÃªn cá»™t question (cho CSV, default: question)
--answer-col         TÃªn cá»™t answer (cho CSV, default: answer)
--split              Tá»· lá»‡ test set (0.0-1.0)
--preview, -p        Sá»‘ samples Ä‘á»ƒ preview
--include-image      Há»— trá»£ image (cho Qwen3VL)
--seed               Random seed (default: 42)
```

## ğŸš€ 2. Training Model (train_qwen3_model.py)

Script Ä‘á»ƒ fine-tune Qwen3 model vá»›i LoRA.

### CÃ i Ä‘áº·t

```bash
# CÃ i dependencies
pip install transformers accelerate peft bitsandbytes datasets torch
```

### Sá»­ dá»¥ng

#### Training cÆ¡ báº£n

```bash
python train_qwen3_model.py \
    --model hainguyen306201/bank-model-2b \
    --dataset dataset.json \
    --output ./checkpoints
```

#### Training vá»›i custom parameters

```bash
python train_qwen3_model.py \
    --model hainguyen306201/bank-model-2b \
    --dataset dataset.jsonl \
    --output ./checkpoints \
    --epochs 5 \
    --batch-size 8 \
    --learning-rate 2e-4
```

#### Training vá»›i gradient accumulation

```bash
python train_qwen3_model.py \
    --model hainguyen306201/bank-model-2b \
    --dataset dataset.json \
    --batch-size 4 \
    --gradient-accumulation 2 \
    --epochs 3
```

#### Training khÃ´ng quantization (cáº§n GPU lá»›n)

```bash
python train_qwen3_model.py \
    --model hainguyen306201/bank-model-2b \
    --dataset dataset.json \
    --no-quantization \
    --batch-size 2
```

#### Training vá»›i Flash Attention 2 (tÄƒng tá»‘c)

```bash
# CÃ i flash-attn trÆ°á»›c
pip install flash-attn --no-build-isolation

# Training
python train_qwen3_model.py \
    --model hainguyen306201/bank-model-2b \
    --dataset dataset.json \
    --flash-attention
```

### Options

```
--model, -m              TÃªn model trÃªn Hugging Face (required)
--dataset, -d            ÄÆ°á»ng dáº«n file dataset (required)
--output, -o              ThÆ° má»¥c lÆ°u checkpoints (default: ./checkpoints)
--epochs, -e              Sá»‘ epochs (default: 3)
--batch-size, -b          Batch size (default: 4)
--gradient-accumulation   Gradient accumulation steps (default: 1)
--learning-rate, -lr      Learning rate (default: 2e-4)
--lora-r                  LoRA rank (default: 16)
--lora-alpha              LoRA alpha (default: 32)
--lora-dropout            LoRA dropout (default: 0.05)
--no-quantization         KhÃ´ng dÃ¹ng quantization
--flash-attention         Sá»­ dá»¥ng Flash Attention 2
--save-steps              LÆ°u checkpoint má»—i N steps (default: 500)
--logging-steps           Log má»—i N steps (default: 50)
--max-length              Max sequence length (default: 2048)
--warmup-steps            Warmup steps (default: 100)
```

## ğŸ“Š Workflow hoÃ n chá»‰nh

### BÆ°á»›c 1: Chuáº©n bá»‹ data

```bash
# Táº¡o dataset tá»« CSV
python qwen3_dataset_builder.py \
    --input raw_data.csv \
    --output training_data.jsonl \
    --split 0.2
```

Káº¿t quáº£:
- `training_data.jsonl.train.jsonl` - Train set
- `training_data.jsonl.test.jsonl` - Test set

### BÆ°á»›c 2: Training

```bash
# Training vá»›i train set
python train_qwen3_model.py \
    --model hainguyen306201/bank-model-2b \
    --dataset training_data.jsonl.train.jsonl \
    --output ./checkpoints \
    --epochs 5 \
    --batch-size 4
```

### BÆ°á»›c 3: Evaluate (optional)

Sá»­ dá»¥ng test set Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ model sau khi training.

## ğŸ’¡ Tips

1. **Dataset size**: NÃªn cÃ³ Ã­t nháº¥t 100-500 samples Ä‘á»ƒ training hiá»‡u quáº£
2. **Batch size**: Äiá»u chá»‰nh theo GPU memory (4-bit quantization: batch_size=4-8)
3. **Learning rate**: Báº¯t Ä‘áº§u vá»›i 2e-4, Ä‘iá»u chá»‰nh náº¿u loss khÃ´ng giáº£m
4. **LoRA rank**: TÄƒng `--lora-r` (16â†’32â†’64) náº¿u cáº§n cháº¥t lÆ°á»£ng cao hÆ¡n
5. **Flash Attention**: CÃ i `flash-attn` Ä‘á»ƒ tÄƒng tá»‘c ~20-30%

## ğŸ“š TÃ i liá»‡u tham kháº£o

- [Qwen Documentation](https://qwen.readthedocs.io/en/latest/)
- [PEFT LoRA](https://huggingface.co/docs/peft/task_guides/clm-lora)
- [Transformers Training](https://huggingface.co/docs/transformers/training)

## âš ï¸ LÆ°u Ã½

- Cáº§n GPU vá»›i Ã­t nháº¥t 16GB VRAM (vá»›i 4-bit quantization)
- Training cÃ³ thá»ƒ máº¥t vÃ i giá» tÃ¹y dataset size
- Backup checkpoints thÆ°á»ng xuyÃªn
- Monitor GPU memory vÃ  temperature

